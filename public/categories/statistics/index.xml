<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics | KHstats</title>
    <link>/categories/statistics/</link>
      <atom:link href="/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 13 Oct 2020 21:13:14 -0500</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>statistics</title>
      <link>/categories/statistics/</link>
    </image>
    
    <item>
      <title>Whatcha measuring?? Collider Bias Examples in R</title>
      <link>/blog/collider-bias/race/</link>
      <pubDate>Tue, 13 Oct 2020 21:13:14 -0500</pubDate>
      <guid>/blog/collider-bias/race/</guid>
      <description>


&lt;p&gt;Anyways, I wholeheartedly agree with this. I find many of the tools in causal inference (specifically Directed Acyclic Graphs) to be incredibly useful in recognizing what I &lt;em&gt;cannot&lt;/em&gt; unbiasedly estimate.&lt;/p&gt;
&lt;p&gt;I am an expert lurker on the statistics twitter (war) threads is a favorite pasttime of mine, so I was admittedly a bit sad it didn’t gain more steam.&lt;/p&gt;
&lt;p&gt;So now your investigator wants to know the attributable mortality of race once someone is hospitalized. “We don’t want your causal inference, we just want the &lt;em&gt;independent association&lt;/em&gt;!” they tell you emphatically. “We &lt;em&gt;PROMISE&lt;/em&gt; we won’t talk about anything causally!” You think about Miguel Hernan’s &lt;em&gt;The C-Word&lt;/em&gt; paper. You don’t believe them at all, but you let it go, for a moment.&lt;/p&gt;
&lt;p&gt;Let’s start with simulating race. Let’s say our population of interest is approximately 50% of one race and 50% of another. We’ll simulate that with a binomial distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;race &amp;lt;- rbinom(n, 1, 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll simulate COVID-19 disease severity. Although there is some evidence suggesting COVID-19 disease severity is not random (i.e. it could be related to viral load, blood type, etc.), let’s assume for the sake of example that it is completely random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disease_severity &amp;lt;- rbinom(n, 1, 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s think about a common adjustment scientists use to reduce confounding of race: socioeconomic status (SES). SES is unarguably affected by race. Again, for simplicity, let’s pretend SES is a binary outcome. It could represent if a person with COVID-19 lives “above” or “below” the poverty line. We’ll simulate SES as a binomial distribution where the probability someone will be above the poverty line is affected by their race.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ses &amp;lt;- rbinom(n, 1, plogis(-race))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve made our population, so now let’s figure out whether we’re actually going to get to see each person’s data. We only have access to COVID-19 patients who were hospitalized. Whether or not someone is hospitalized is affected by their disease severity, of course.&lt;/p&gt;
&lt;p&gt;However, let’s say for our example that hospitalization status is also affected by a person’s SES (whether or not they have insurance may affect their decision to go to the hospital, for example). It also may be affected by some part of the social construct that we use to define race/ethnicity. If individuals in one race/ethnicity are more or less likely to go to the hospital than individuals of another race/ethnicity, then that will affect hospitalization. This is very reasonable in COVID-19 (and in other diseases) where Black and Hispanic persons are disproportionately dying. If a person knows a family member or friend who is on a …&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hospitalized &amp;lt;- rbinom(n, 1, plogis(-3 + 2 * disease_severity + 2 * ses + 3 * race))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we simulate our outcome. Let’s keep it simple and say whether or not someone dies is directly affected by disease severity and whether or not the person is cared for in a hospital:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;death &amp;lt;- rbinom(n, 1, plogis(-(2 * disease_severity + hospitalized)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s put this simulated data into a data frame. I’m going to relabel the data with interpretable to make it clearer how harmful doing these analyses can be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_df &amp;lt;-
  data.frame(
  race = factor(race, levels=c(1,0), labels=c(&amp;quot;White&amp;quot;,&amp;quot;Black&amp;quot;)),
  disease_severity = factor(disease_severity, levels=c(1,0), labels=c(&amp;quot;Mild&amp;quot;,&amp;quot;Severe&amp;quot;)),
  ses = factor(ses, levels=c(0,1), labels=c(&amp;quot;Above poverty line&amp;quot;, &amp;quot;Below poverty line&amp;quot;)),
  hospitalized = factor(hospitalized, levels=0:1, labels=c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;)),
  death = factor(death, levels=0:1, labels=c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I’ll make a data frame that contains only the patients who were put in the hospital.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hospitalized_df &amp;lt;-
  covid_df %&amp;gt;%
  filter(hospitalized == &amp;quot;Yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The association between race and mortality for people who are hospitalized. Can you get the true association?&lt;/p&gt;
&lt;p&gt;Let’s pretend more a moment that you think you can. You’re a pro at &lt;code&gt;ggplot2&lt;/code&gt; so you whip out a nice bar chart. Look, race and mortality in hospitalized patients! It’s Figure 1 of a paper if I’ve ever seen one!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;race_death_count &amp;lt;-
  hospitalized_df %&amp;gt;%
  group_by(race) %&amp;gt;%
  count(death) %&amp;gt;%
  mutate(sum_n = sum(n)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(prop = n/sum_n)

race_death_count %&amp;gt;%
  filter(death == &amp;quot;Yes&amp;quot;) %&amp;gt;%
  ggplot(aes(x=race, y=prop)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  theme_classic() +
  labs(x=&amp;quot;Race&amp;quot;,y=&amp;quot;In-hospital Mortality&amp;quot;,title=&amp;quot;Race and Mortality in COVID-19&amp;quot;,subtitle=&amp;quot;Among Hospitalized Patients&amp;quot;) +
  scale_y_continuous(labels = scales::percent_format(), expand=c(0,0), limits=c(0,.25)) +
  geom_text(aes(label=paste(n, &amp;quot;/&amp;quot;, sum_n), y=prop+.01))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/collider-bias/race_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pretty_logistic_table &amp;lt;- function(model_fit) {
  model_fit %&amp;gt;%
    broom::tidy(exponentiate=T, conf.int=T) %&amp;gt;%
    filter(term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;%
     mutate( term = case_when(term == &amp;quot;raceBlack&amp;quot; ~ &amp;quot;Race: Black&amp;quot;,
                             term == &amp;quot;disease_severitySevere&amp;quot; ~ &amp;quot;Disease severity: Severe&amp;quot;,
                             term == &amp;quot;sesBelow poverty line&amp;quot; ~ &amp;quot;SES: Below poverty level&amp;quot;,
                             TRUE ~ term),
           odds_ratio = paste0(round(estimate,1),&amp;quot; (&amp;quot;,round(conf.low,1),&amp;quot;, &amp;quot;,round(conf.high,1),&amp;quot;)&amp;quot;),
           p_value = case_when(p.value &amp;lt; .001 ~ &amp;quot;&amp;lt;.001&amp;quot;,
                               TRUE ~ as.character(round(p.value, 3)))) %&amp;gt;%
             select(term, odds_ratio, p_value) %&amp;gt;%
    gt::gt() %&amp;gt;%
    gt::cols_label(
       term = &amp;quot;Coefficient&amp;quot;,
               odds_ratio = &amp;quot;Odds Ratio (95% CI)&amp;quot;,
               p_value = &amp;quot;P-value&amp;quot;
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;start by just looking at the unadjusted Odds Ratio of &lt;strong&gt;race&lt;/strong&gt; on &lt;strong&gt;mortality&lt;/strong&gt;. Simple enough&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race, data = hospitalized_df, family = binomial()) %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#fefxepmids .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#fefxepmids .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#fefxepmids .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#fefxepmids .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fefxepmids .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#fefxepmids .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#fefxepmids .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#fefxepmids .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#fefxepmids .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#fefxepmids .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#fefxepmids .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#fefxepmids .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#fefxepmids .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#fefxepmids .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#fefxepmids .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#fefxepmids .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#fefxepmids .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#fefxepmids .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#fefxepmids .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#fefxepmids .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#fefxepmids .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fefxepmids .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#fefxepmids .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#fefxepmids .gt_left {
  text-align: left;
}

#fefxepmids .gt_center {
  text-align: center;
}

#fefxepmids .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#fefxepmids .gt_font_normal {
  font-weight: normal;
}

#fefxepmids .gt_font_bold {
  font-weight: bold;
}

#fefxepmids .gt_font_italic {
  font-style: italic;
}

#fefxepmids .gt_super {
  font-size: 65%;
}

#fefxepmids .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;fefxepmids&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.7 (0.5, 0.9)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.002&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;“we’re not interested in this,”
but what are you interested in, then? what meaning does it hold to conclude that
perhaps you’re going to say that providers don’t have any additional effect on patients; there’s no inherent racism. but is that actually what you can conclude? all your estimates are now biased&lt;/p&gt;
&lt;p&gt;statistician drawing dag
I. don’t. want. causation.
“okay, great!”
I just want to show that within my hospitalized patients
&lt;em&gt;whiteboard, draws H&lt;/em&gt;
there is an effect of race
&lt;em&gt;draws race&lt;/em&gt;
on mortality
&lt;em&gt;draws mortality&lt;/em&gt;
independent of confounders
&lt;em&gt;draws confounders&lt;/em&gt;
WITHIN A HOSPITALIZED POPULATION
&lt;em&gt;draws box&lt;/em&gt;
Yep, still can’t answer that. could be correlated, could not be
THAT’SWHAT I WANT TO WRITE ABOUT
but what are you going to SAY about that?!?!
&#34;This estimate of correlation between race and mortality in a hospitalized population – which could be completely incorrect, either in the null or wrong direction – is evidence of… What??? The point of research is to ask a question you can set up an experiment to answer, and then to answer it as best you can, not ask a question you &lt;em&gt;know&lt;/em&gt; you cannot answer and then conjecture about the number you obtain.&lt;/p&gt;
&lt;p&gt;but everyone knows Black people are set up for failure in the healthcare system, this will only be accumulating evidence
but what if it’s not right, what if you find being black is protective, or being Black has no effect on mortality. Any of these are possible!&lt;/p&gt;
&lt;p&gt;There’s no way there won’t be an effect of race on mortality among hospitalized patients. Race …&lt;/p&gt;
&lt;p&gt;So why are you doing this study?&lt;/p&gt;
&lt;p&gt;So what do you suggest I do?&lt;/p&gt;
&lt;p&gt;I suggest you leave this research question to the people who actually have the population level data on race, hospitalization rates, and mortality, to answer the effect of race on mortality. Or, I suggest you go find the people who have that data and try to team up with them. I’m not saying your question is not important, I’m saying you don’t have the data to correctly answer it, and whatever estimate you come up with will be incorrect, and could do more harm than good.&lt;/p&gt;
&lt;p&gt;Headlines: RACE IS A RISK FACTOR FOR DYING FROM COVID-19 IN HOSPITALIZED PATIENTS&lt;/p&gt;
&lt;p&gt;Fitting a model among the entire population and hospitalized patients only:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race + disease_severity + ses, data=covid_df, family = binomial()) %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#eexirarxtj .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#eexirarxtj .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#eexirarxtj .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#eexirarxtj .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#eexirarxtj .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#eexirarxtj .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#eexirarxtj .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#eexirarxtj .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#eexirarxtj .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#eexirarxtj .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#eexirarxtj .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#eexirarxtj .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#eexirarxtj .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#eexirarxtj .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#eexirarxtj .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#eexirarxtj .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#eexirarxtj .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#eexirarxtj .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#eexirarxtj .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#eexirarxtj .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#eexirarxtj .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#eexirarxtj .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#eexirarxtj .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#eexirarxtj .gt_left {
  text-align: left;
}

#eexirarxtj .gt_center {
  text-align: center;
}

#eexirarxtj .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#eexirarxtj .gt_font_normal {
  font-weight: normal;
}

#eexirarxtj .gt_font_bold {
  font-weight: bold;
}

#eexirarxtj .gt_font_italic {
  font-style: italic;
}

#eexirarxtj .gt_super {
  font-size: 65%;
}

#eexirarxtj .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;eexirarxtj&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;2 (1.7, 2.3)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Disease severity: Severe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;11.7 (9.8, 14)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;SES: Below poverty level&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.7 (0.6, 0.9)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race + disease_severity + ses, data=hospitalized_df, family = binomial())  %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#wfcbehcipe .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#wfcbehcipe .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#wfcbehcipe .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#wfcbehcipe .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wfcbehcipe .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#wfcbehcipe .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#wfcbehcipe .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#wfcbehcipe .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#wfcbehcipe .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#wfcbehcipe .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#wfcbehcipe .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#wfcbehcipe .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#wfcbehcipe .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#wfcbehcipe .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#wfcbehcipe .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#wfcbehcipe .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#wfcbehcipe .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#wfcbehcipe .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#wfcbehcipe .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#wfcbehcipe .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#wfcbehcipe .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wfcbehcipe .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#wfcbehcipe .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#wfcbehcipe .gt_left {
  text-align: left;
}

#wfcbehcipe .gt_center {
  text-align: center;
}

#wfcbehcipe .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#wfcbehcipe .gt_font_normal {
  font-weight: normal;
}

#wfcbehcipe .gt_font_bold {
  font-weight: bold;
}

#wfcbehcipe .gt_font_italic {
  font-style: italic;
}

#wfcbehcipe .gt_super {
  font-size: 65%;
}

#wfcbehcipe .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;wfcbehcipe&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1 (0.7, 1.3)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.856&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Disease severity: Severe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;9.2 (7, 12.2)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;SES: Below poverty level&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1.1 (0.8, 1.4)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.695&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;We miss the effect of race (through hospitalization) on death.&lt;/p&gt;
&lt;p&gt;There is no effect of race on mortality in hospitalized patients… but there is definitely an effect of race on mortality in the full population, because it affects whether someone gets hospitalized.&lt;/p&gt;
&lt;p&gt;So what are you estimating? What are you going to do with that information?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Illustrated Guide to Targeted Minimum Loss-based Estimation</title>
      <link>/blog/tmle/tmle-tutorial-3/</link>
      <pubDate>Sat, 10 Oct 2020 21:13:14 -0500</pubDate>
      <guid>/blog/tmle/tmle-tutorial-3/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kableExtra)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hello! I’m excited that you’re excited to learn about Targeted Maximum Likelihood Estimation (TMLE). I’ve tried to lay out TMLE in the simplest terms possible for an applied analyst who wants to understand the motivation and/or algorithm behind TMLE.&lt;/p&gt;
&lt;p&gt;For better or for worse, I interpret statistical algorithms very visually, and the way I learned TMLE was by tracking every single step with a little colored data frame and pseudo-&lt;code&gt;R&lt;/code&gt; code. I eventually formalized that thought process into a one page “cheat sheet” in case it can help anyone else:&lt;/p&gt;
&lt;p&gt;It is available as an 8.5x11&#34; pdf on my &lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;Github&lt;/a&gt; in case you’d like to print it out for reference.&lt;/p&gt;
&lt;p&gt;I’ll now discuss the motivation behind TMLE, the algorithm (step by step, with real &lt;code&gt;R&lt;/code&gt; code), and a tiny bit on &lt;em&gt;why&lt;/em&gt; it works and where you can learn more.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;tmle-in-two-sentences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TMLE in two sentences&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The general form of Targeted Maximum Likelihood Estimation (TMLE) allows you to &lt;strong&gt;estimate a statistical estimand of interest using&lt;/strong&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;the expected outcome of an individual, given the treatment they received and their baseline confounders&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the probability an individual received the treatment of interest, given their baseline confounders (propensity score)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;These estimates can come from flexible statistical models we commonly categorize as machine learning, but the &lt;strong&gt;overall estimate of the treatment effect will still have valid confidence intervals&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;div id=&#34;wait-what-about-causal-inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wait, what about causal inference??&lt;/h3&gt;
&lt;p&gt;Now you might be thinking, “Wait a second! I only ever hear about TMLE in the context of causal inference.” It is true that TMLE is often talked about in the context of causal inference; however, by itself, TMLE is not causal inference. TMLE is a statistical estimation method which can be used for estimating parameters that do (or don’t!) have a causal interpretation.&lt;/p&gt;
&lt;p&gt;TMLE was developed for causal inference because those who apply causal inference ideology to their work spend &lt;em&gt;so much time&lt;/em&gt; trying to get the identification part (1) and (2) of causal inference correct, that they don’t want to “waste” that time by proceeding to use estimation algorithms which are known to place unreasonable assumptions on the distribution of the data.&lt;/p&gt;
&lt;p&gt;This brings me to my last point of motivation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-would-i-pick-tmle-over-other-statistical-estimation-algorithms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why would I pick TMLE over other statistical estimation algorithms?&lt;/h3&gt;
&lt;p&gt;Because TMLE allows you to use machine learning algorithms on the backend of its estimation, that means you’re placing &lt;em&gt;minimal assumptions&lt;/em&gt; on the underlying distribution of your data. However, TMLE still has a formula for obtaining valid confidence intervals. That’s so important!&lt;/p&gt;
&lt;p&gt;Flexible machine learning models (EX: random forest, LASSO, neural nets) used on their own generally only allow us to predict, not to make statistical inference. They often have the wrong bias variance tradeoff for any particular exposure, or predictor, because their goal is to optimize prediction of an outcome.&lt;/p&gt;
&lt;p&gt;TMLE uses some clever math, and something called an &lt;strong&gt;efficient influence function&lt;/strong&gt; to solve this. More on that later; let’s talk about the algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-tmle-algorithm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The TMLE Algorithm&lt;/h1&gt;
&lt;p&gt;Initial set up: Load libraries, set seed, and simulate data&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/1_data_structure.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse, quietly=T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ───────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ──────────────────── tidyverse_conflicts() ──
## x dplyr::filter()     masks stats::filter()
## x dplyr::group_rows() masks kableExtra::group_rows()
## x dplyr::lag()        masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(7)

# Superlearner functions from Ivan
#source(&amp;quot;sl.r&amp;quot;)

# using data generating code from Miguel Angel Luque Fernandez&amp;#39; tutorial
generate_data &amp;lt;- function(n){
    w1 &amp;lt;- rbinom(n, size=1, prob=0.5)
    w2 &amp;lt;- rbinom(n, size=1, prob=0.65)
    w3 &amp;lt;- round(runif(n, min=0, max=4), digits=3)
    w4 &amp;lt;- round(runif(n, min=0, max=5), digits=3)
    A  &amp;lt;- rbinom(n, size=1, prob= plogis(-0.4 + 0.2*w2 + 0.15*w3 + 0.2*w4 + 0.15*w2*w4))
    # counterfactual
    Y_1 &amp;lt;- rbinom(n, size=1, prob= plogis(-1 + 1 -0.1*w1 + 0.3*w2 + 0.25*w3 + 0.2*w4 + 0.15*w2*w4))
    Y_0 &amp;lt;- rbinom(n, size=1, prob= plogis(-1 + 0 -0.1*w1 + 0.3*w2 + 0.25*w3 + 0.2*w4 + 0.15*w2*w4))
    # Observed outcome
    Y &amp;lt;- Y_1*A + Y_0*(1 - A)
    # return data.frame
    tibble(w1, w2, w3, w4, A, Y, Y_1, Y_0)
}

# observations N
n &amp;lt;- 10000

# full data set, including Y0 and Y0
dat_full &amp;lt;- generate_data(n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our estimate of interest here is going to be the mean difference in outcomes if everyone had recevied the treatment compared to if everyone had not received the treatment. If we were to go through the whole causal inference identification process (AKA the very careful thinking half of causal inference), this might be identifiable as the Average Treatment Effect, or ATE.&lt;/p&gt;
&lt;p&gt;Since we have simulated data, we can generate what the outcome would actually look like if everyone were to receive treatment and then if everyone were to not receive treatment, and we can take the mean difference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate the true psi if we saw both outcomes
true_psi &amp;lt;- mean(dat_full$Y_1 - dat_full$Y_0)
round(true_psi,3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.192&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool, so the true ATE for our is 0.192. Since our outcome is binary, that translates to a `19.2% difference in outcomes if everyone were to receive the treatment of interest. That means if A huge effect!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make a data set with observed data only
dat_obs &amp;lt;- dat_full %&amp;gt;%
  select(-Y_1,-Y_0)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 1:&lt;/h1&gt;
&lt;p&gt;Estimate the expected value of the outcome using treatment and confounders as predictors.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(A,W) = \mathrm{E}[Y|A,W]\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q &amp;lt;- glm(Y ~ w1 + w2 + w3 + w4 + A, data = dat_obs, family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate SEs -----------------------------------------------------------

ic &amp;lt;- (dat_obs$Y - Q_A_update) * H + Q_1_update + Q_0_update
tmle_se &amp;lt;- sqrt(var(ic)/nrow(dat_obs))

ci_lo &amp;lt;- tmle_psi - 1.96*tmle_se
ci_hi &amp;lt;- tmle_psi + 1.96*tmle_se

pval &amp;lt;- 2 * (1 - pnorm(abs(tmle_psi / tmle_se)))


# Compare with TMLE package -----------------------------------------------------------

tmle_fit &amp;lt;- tmle::tmle(Y, A, X_A,
                       gbound = .000000001, # trying 
                       Q.SL.library = lib, g.SL.library = lib)
tmle_fit$epsilon # mine are different :(
tmle_fit$estimates$ATE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/2_outcome_fit.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then predict the outcome for every observation under three scenarios:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. If every observation received the treatment they &lt;em&gt;actually&lt;/em&gt; received.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(A,W) = \mathrm{E}[Y|A,W]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/3_QA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_A &amp;lt;- predict(Q, type=&amp;quot;response&amp;quot;)
kable(head(Q_A))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7989757
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7948479
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8348842
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7992942
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7315107
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5590003
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Remember we’re talking about a binary treatment for this tutorial.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. If every observation received the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(1,W) = \mathrm{E}[Y|1,W]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/4_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_A1 &amp;lt;- dat_obs %&amp;gt;% mutate(A = 1)  # data set where everyone received treatment
Q_1 &amp;lt;- predict(Q, newdata = X_A1, type=&amp;quot;response&amp;quot;)
kable(head(Q_1))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7989757
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7948479
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8348842
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7992942
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8766217
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5590003
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;3. If every observation received the control.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(0,W) = \mathrm{E}[Y|0,W]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/5_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_A0 &amp;lt;- dat_obs %&amp;gt;% mutate(A = 0) # data set where no one received treatment
Q_0 &amp;lt;- predict(Q, newdata = X_A0, type=&amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimate g --------------------------------------------------------------

g &amp;lt;- glm(A ~ w1 + w2 + w3 + w4, data = dat_obs, family=binomial)

# predictions for Q -------------------------------------------------------

H_1 &amp;lt;- 1/predict(g, type=&amp;quot;response&amp;quot;)
H_0 &amp;lt;- -1/(1-predict(g, type=&amp;quot;response&amp;quot;))

# prep data to fit the clever covariate
dat_cc &amp;lt;-
  dat_obs %&amp;gt;%
  mutate(Q_A = Q_A,
         H = case_when(A == 1 ~ H_1,
                       A == 0 ~ H_0)) %&amp;gt;%
  select(Y, A, Q_A, H)

# fit parametric working model - this will ALWAYS be the working model
glm_fit &amp;lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H, data=dat_cc, family=binomial)

# get epsilon
eps &amp;lt;- coef(glm_fit)
# also get H as a vector (for Q_A_update)
H &amp;lt;- dat_cc$H

# update expected outcome estimates (Q star) ------------------------------

# update
Q_1_update &amp;lt;- plogis(qlogis(Q_1) + eps*H_1)
Q_0_update &amp;lt;- plogis(qlogis(Q_0) + eps*H_0)
Q_A_update &amp;lt;- plogis(qlogis(Q_A) + eps*H)

# Estimate ATE ------------------------------------------------------------

# get the ATE TMLE
tmle_psi &amp;lt;- mean(Q_1_update - Q_0_update)

# compare to true parameter
tmle_psi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1887145&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;true_psi &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1918&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Step 2:&lt;/p&gt;
&lt;p&gt;Estimate the probability of treatment, given confounders. Note that this quantity is often called a &lt;strong&gt;propensity score&lt;/strong&gt;, as in it gives the &lt;em&gt;propensity&lt;/em&gt; that an observation will receive a treatment of interest.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(g(W) = Pr(A=1|W)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/6_treatment_fit.png&#34; style=&#34;width:60.0%&#34; /&gt;
&lt;img src=&#34;/img/tmle/7_H1.png&#34; style=&#34;width:60.0%&#34; /&gt;
&lt;img src=&#34;/img/tmle/8_H0.png&#34; style=&#34;width:70.0%&#34; /&gt;
&lt;img src=&#34;/img/tmle/9_HA.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;img src=&#34;/img/tmle/10_logistic_regression.png&#34; /&gt;
&lt;img src=&#34;/img/tmle/11_epsilon.png&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;img src=&#34;/img/tmle/12_update_Q1.png&#34; style=&#34;width:70.0%&#34; /&gt;
&lt;img src=&#34;/img/tmle/13_update_Q0.png&#34; style=&#34;width:70.0%&#34; /&gt;
&lt;img src=&#34;/img/tmle/14_compute_ATE.png&#34; style=&#34;width:60.0%&#34; /&gt;
&lt;img src=&#34;/img/tmle/15_ses.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set SL libraries
lib &amp;lt;- c(&amp;#39;SL.speedglm&amp;#39;, # faster glm
         &amp;#39;SL.glmnet&amp;#39;, # lasso
         &amp;#39;SL.ranger&amp;#39;, # random forest
         &amp;#39;SL.earth&amp;#39;)  #


# estimate Q --------------------------------------------------------------

Y &amp;lt;- dat_obs$Y
X_Y &amp;lt;- dat_obs %&amp;gt;% select(-Y)

Q &amp;lt;- SuperLearner(Y = Y, X = X_Y,
                    family=binomial(),
                    SL.library=lib,
                    # metalearner = NNloglik for binary outcomes, NNLS for continuous
                    method = method.NNloglik,
                    # 5-fold cross validation
                    cvControl = list(V = 5))

# estimate g --------------------------------------------------------------

A &amp;lt;- dat_obs$A
X_A &amp;lt;- dat_obs %&amp;gt;% select(-Y, -A)

g &amp;lt;- SuperLearner(Y = A, X = X_A,
                    family=binomial(),
                    SL.library=lib,
                    # metalearner = NNloglik for binary outcomes, NNLS for continuous
                    method = method.NNloglik,
                    # 5-fold cross validation
                    cvControl = list(V = 5))


# predictions for Q -------------------------------------------------------

Q_A &amp;lt;- predict(Q)$pred

X_Y_A1 &amp;lt;- X_Y %&amp;gt;% mutate(A = 1) 
Q_1 &amp;lt;- predict(Q, newdata = as.data.frame(X_Y_A1))$pred

X_Y_A0 &amp;lt;- X_Y %&amp;gt;% mutate(A = 0)
Q_0 &amp;lt;- predict(Q, newdata = X_Y_A0)$pred


# create clever covariate -------------------------------------------------

H_1 &amp;lt;- 1/predict(g)$pred
H_0 &amp;lt;- -1/(1-predict(g)$pred)

# prep data to fit the clever covariate
dat_cc &amp;lt;-
  dat_obs %&amp;gt;%
  mutate(Q_A = as.vector(Q_A),
         H = case_when(A == 1 ~ H_1,
                       A == 0 ~ H_0)) %&amp;gt;%
  select(Y, A, Q_A, H)

# fit parametric working model
glm_fit &amp;lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H, data=dat_cc, family=binomial)
# get epsilon
eps &amp;lt;- coef(glm_fit)
# also get H as a vector (for Q_A_update)
H &amp;lt;- dat_cc$H

# update expected outcome estimates (Q star) ------------------------------

# update
Q_1_update &amp;lt;- plogis(qlogis(Q_1) + eps*H_1)
Q_0_update &amp;lt;- plogis(qlogis(Q_0) + eps*H_0)
Q_A_update &amp;lt;- plogis(qlogis(Q_A) + eps*H)

# Estimate ATE ------------------------------------------------------------

# get the ATE TMLE
tmle_psi &amp;lt;- mean(Q_1_update - Q_0_update)

# compare to true parameter
tmle_psi
true_psi 


# Calculate SEs -----------------------------------------------------------

ic &amp;lt;- (dat_obs$Y - Q_A_update) * H + Q_1_update + Q_0_update
tmle_se &amp;lt;- sqrt(var(ic)/nrow(dat_obs))

ci_lo &amp;lt;- tmle_psi - 1.96*tmle_se
ci_hi &amp;lt;- tmle_psi + 1.96*tmle_se

pval &amp;lt;- 2 * (1 - pnorm(abs(tmle_psi / tmle_se)))


# Compare with TMLE package -----------------------------------------------------------

tmle_fit &amp;lt;- tmle::tmle(Y, A, X_A,
                       gbound = .000000001, # trying 
                       Q.SL.library = lib, g.SL.library = lib)
tmle_fit$epsilon # mine are different :(
tmle_fit$estimates$ATE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Essentially, you start with the estimate of an individual’s outcome given their treatment and baseline covariates, and you update that estimate using the probability an individual received the treatment given their baseline covariates. Every time you update that estimate, you remove more of the bias that naturally exists in your observational data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-should-i-bother-with-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why should I bother with TMLE?&lt;/h1&gt;
&lt;p&gt;Before I show an applied example of TMLE, I want to explain why I prefer TMLE to other propensity score estimation methods. In short, it is because because you don’t need to fit any certain type of regression to get your final estimates. You can use any estimator you want – a generalized linear model, splines, random forests, gradient boosting, neural nets, honestly anything – to get the initial probability estimates. You can actually use &lt;em&gt;all&lt;/em&gt; of those estimators to get the two probability estimates, if you want! But more on that later.&lt;/p&gt;
&lt;p&gt;The benefit of expanding your toolbox of potential estimators is that most estimators are built with prediction in mind, and thus yield very good probability estimates to initiate our TMLE algorithm. When our goal as statisticians is to calculate effects on an outcome attributable to a treatment, it’s easy to shy away from these prediction-focused types of estimators, because most of them do not have any statistical theory to allow us to calculate valid confidence intervals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A huge selling point of TMLE is that it allows you to utilize prediction-focused (often non-parametric) estimators, but still obtain valid confidence intervals on your final estimate of the treatment’s effect.&lt;/strong&gt; TMLE utilizes concepts from semi-parametric influence function theory to determine valid standard errors, and therefore valid confidence intervals, on estimates of treatment effects. This is not important unless you plan to tackle the math of TMLE, but if you do decide to venture into technical explanations, know that you’ll see references to influence functions &lt;em&gt;a lot&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;In other propensity score methods, like IPTW, we can only obtain valid confidence intervals if we obtain our propensity score using pre-specified parametric models. For example, we fit a logistic regression for our treatment predicted by five pre-specified baseline covariates. Using a logistic model like this to calculate the propensity score is not only placing a strong distributional assumption on our data, but it is limited in its ability to take in high-, or even medium-dimensional data. When we use estimators that are well equipped for very “wide” data and perform variable selection, we obtain better overall estimates for our treatment effect of interest.&lt;/p&gt;
&lt;p&gt;There are a few other major benefits to TMLE. For one, it has very good bias-variance properties, and those properties are “robust,” or “resistant to” model mispecification, i.e. having the wrong type of estimator or the wrong variables in your estimation of either the treatment or outcome. Another benefit is that if you are able to go through a causal identification process of the research question and available data (a concept I won’t discuss further, since it’s a separate realm of the problem), you’ll have a causal interpretation of your average treatment effect estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-real-world-application-of-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Real World Application of TMLE&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr, warn.conflicts=F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I’ll be using a data set from the ???. EXPLAIN DATA aND goal&lt;/p&gt;
&lt;p&gt;I chose this data set because it is the same data used in a really good resource if you want to do a deeper dive in Targeted Learning:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;washb_data &amp;lt;- data.table::fread(&amp;quot;https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv&amp;quot;,
                    stringsAsFactors = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we can run a TMLE analysis, we need to clean the data a bit. The &lt;code&gt;tmle&lt;/code&gt; function requires a binary treatment, so I’ll need to turn the treatment data from character strings to binary 0/1 variables. I’m only interested right now in women who recieved the treatment of Nutrition and Hand washing, and comparing that to the control women, so I’ll filter out the appropriate subjects and make my data binary.&lt;/p&gt;
&lt;p&gt;To use &lt;code&gt;tmle&lt;/code&gt;, your data structure should be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Wide (each row is one observation)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;No factors (all categorical variables should be transformed to dummy/indicator columns using a function like &lt;code&gt;model.matrix()&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No missing data (I usually make a new column indicating whether the value was missing, and then impute at the mean or median for continuous variables)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_clean &amp;lt;- 
  washb_data %&amp;gt;%
  filter(tr %in% c(&amp;quot;Control&amp;quot;, &amp;quot;Nutrition + WSH&amp;quot;)) %&amp;gt;%
  mutate(tr = case_when(tr == &amp;quot;Control&amp;quot; ~ 0,
                        TRUE ~ 1)) %&amp;gt;%
  mutate_at(vars(one_of(c(&amp;quot;momage&amp;quot;,&amp;quot;momheight&amp;quot;))), list(miss =~ ifelse(is.na(.), 1, 0))) %&amp;gt;%
  mutate_at(vars(one_of(&amp;quot;momage&amp;quot;,&amp;quot;momheight&amp;quot;)), list( ~ ifelse(is.na(.), mean(., na.rm=T), .))) %&amp;gt;%
  model.matrix(~ . + 0, data = .) %&amp;gt;%
  as.data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once I’ve cleaned my data, I make vectors specifying my treatment, outcome, and baseline covariates. In the TMLE literature, and in this package, the notations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Outcome: &lt;code&gt;Y&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Treatment: &lt;code&gt;A&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Confounders: &lt;code&gt;W&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this data set, our outcome is the variable &lt;code&gt;whz&lt;/code&gt;, our treatment is the variable &lt;code&gt;tr&lt;/code&gt;, and our baseline confounders are all the other variables in our data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Y &amp;lt;- dat_clean$whz
A &amp;lt;- dat_clean$tr
W &amp;lt;- dat_clean %&amp;gt;% dplyr::select(-whz, -tr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we’ve done that, we can get our TMLE estimate of the average treatment effect! Here, I’m only inputting the&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmlefit_default &amp;lt;- tmle::tmle(Y, A, W)
tmlefit_default&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, I’m going to specify the kinds of estimators I want to use to get my two important estimates: the expected outcome and the probability of the exposure.&lt;/p&gt;
&lt;p&gt;I’ve chosen to call the functions &lt;code&gt;glm&lt;/code&gt; for a generalized linear model, &lt;code&gt;glmnet&lt;/code&gt; for penalized regression, &lt;code&gt;ranger&lt;/code&gt; for random forests, and &lt;code&gt;xgboost&lt;/code&gt; for extreme gradient boosting.&lt;/p&gt;
&lt;p&gt;I’m going to use &lt;em&gt;all&lt;/em&gt; of these estimators to estimate the&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SL_library &amp;lt;- c(&amp;quot;SL.glm&amp;quot;,
                &amp;quot;SL.glmnet&amp;quot;,
                &amp;quot;SL.ranger&amp;quot;,
                &amp;quot;SL.xgboost&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re going to specify those libraries in the arguments &lt;code&gt;Q.SL.library&lt;/code&gt; and &lt;code&gt;g.SL.library&lt;/code&gt;. These arguments sound a bit scary, but they are just what the notation in TMLE literature is – Q refers to the estimation for the outcome given treatment and covariates, and g refers to the estimation of the probability of the treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmlefit &amp;lt;- tmle::tmle(Y, A, W,
                Q.SL.library = SL_library, g.SL.library = SL_library)
tmlefit&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To understand how the various machine learning models are combining, you should look into a type of ensemble learning called stacking, or “superlearning.” I have a slideshow that you could look at &lt;a href=&#34;https://github.com/hoffmakl/sl3-demo/blob/master/superlearning_slides_no_animation.pdf&#34;&gt;here&lt;/a&gt;, but there are plenty of other resources online.&lt;/p&gt;
&lt;p&gt;This is obviously not a super technical post, and it was not intended to be, but I hope that it may catch your interest to learn more about the complexities of TMLE and try it out in your next analysis. I plan to write a few similar posts on what to do if your outcomes are survival or longitudinal data. In the meantime, you may find the more technical tutorial on TMLE helpful:&lt;/p&gt;
&lt;p&gt;or, “The Hitchiker’s Guide to Targeted Learning” is an excellent, still-in-progress, resource for learning TMLE and many other targeted learning.&lt;/p&gt;
&lt;p&gt;Learn about causal inference,&lt;/p&gt;
&lt;p&gt;we should be estimating answers for questions of actual interest, rather than debiasing our results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt;: The difference between the true value of a parameter and the estimated value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: The bias of an estimator decreases to 0 as sample size approaches infinity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: The variance of an estimator is as small as possible as sample size approaches infinity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Double robust&lt;/strong&gt;: If either of&lt;/p&gt;
&lt;p&gt;Causal inference, at a bird’s eye view, involves a process of:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Defining a question and the causal parameter, or causal estimand, of interest (for example, we might want to know the difference in COVID-19 infection rates if everyone did wear a mask compared to if everyone did not wear a mask).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using causality tools (for example, Directed Acyclic Graphs or Non Parametric Structrual Equations) and assumptions (exchangeability, consistency, and positivity) to determine whether we can write our causal estimand of interest in terms of the data (using expectations and random variables). If we can do this, we say our causal estimand is &lt;strong&gt;identifiable&lt;/strong&gt;, and we can make the leap that a &lt;em&gt;statistical estimand&lt;/em&gt; (i.e. expectations and random variables) is a true representation of what we would see in a world that allows us to actually observe all the parallel, or counterfactual, outcomes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once we have a statistical estimand, we can consider the statistical estimation algorithms available to estimate it (typically we weigh them in terms of bias and variance properties, and perhaps computational requirements), and then go on to estimate it.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There’s thousands of books and papers that elaborate on those three points above, but the main takeaway is this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TMLE focuses on Step 3 only.&lt;/strong&gt; It is a &lt;strong&gt;statistical estimation algorithm&lt;/strong&gt; that &lt;em&gt;allows&lt;/em&gt;, but &lt;em&gt;does not require&lt;/em&gt; statistical estimands to have a causal interpretation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;a-comparison-with-g-computation-and-iptw&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A comparison with G-Computation and IPTW&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Become a Superlearner! An Illustrated Guide to Superlearning</title>
      <link>/blog/sl/superlearning/</link>
      <pubDate>Sat, 10 Oct 2020 21:13:14 -0500</pubDate>
      <guid>/blog/sl/superlearning/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;blockquote&gt;
&lt;p&gt;Why use &lt;em&gt;one&lt;/em&gt; machine learning algorithm when you could use all of them?! This post contains a step-by-step walkthrough of how to build a superlearner prediction algorithm in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;/img/Superlearning.jpg&#34;  
         width=100%&#34;&gt;
&lt;figcaption&gt;
&lt;strong&gt;&lt;em&gt;A Visual Guide…&lt;/em&gt;&lt;/strong&gt; Over the winter, I read &lt;a href=&#34;https://www.springer.com/gp/book/9781441997814&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt; by Mark van der Laan and Sherri Rose. This “visual guide” I made for &lt;em&gt;Chapter 3: Superlearning&lt;/em&gt; by Rose, van der Laan, and Eric Polley is a condensed version of the following tutorial. It is available as an &lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/Superlearner.pdf&#34;&gt;8.5x11&#34; pdf on Github&lt;/a&gt;, should you wish to print it out for reference (or desk decor).
&lt;/figcaption&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;div id=&#34;supercuts-of-superlearning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Supercuts of superlearning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Superlearning&lt;/strong&gt; is a technique for prediction that involves &lt;strong&gt;combining many individual statistical algorithms&lt;/strong&gt; (commonly called “data-adaptive” or “machine learning” algorithms) to &lt;strong&gt;create a new, single prediction algorithm&lt;/strong&gt; that is expected to &lt;strong&gt;perform at least as well as any of the individual algorithms&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The superlearner algorithm “decides” how to combine, or weight, the individual algorithms based upon how well each one &lt;strong&gt;minimizes a specified loss function&lt;/strong&gt;, for example, the mean squared error (MSE). This is done using cross-validation to avoid overfitting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The motivation for this type of “ensembling” is that &lt;strong&gt;a mix of multiple algorithms may be more optimal for a given data set than any single algorithm&lt;/strong&gt;. For example, a tree based model averaged with a linear model (e.g. random forests and LASSO) could smooth some of the model’s edges to improve predictive performance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Superlearning is also called stacking, stacked generalizations, and weighted ensembling by different specializations within the realms of statistics and data science.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/spiderman_meme.jpg&#34; style=&#34;width:42.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;superlearning-step-by-step&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Superlearning, step by step&lt;/h1&gt;
&lt;p&gt;First I’ll go through the algorithm one step at a time using a simulated data set.&lt;/p&gt;
&lt;div id=&#34;initial-set-up-load-libraries-set-seed-simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initial set-up: Load libraries, set seed, simulate data&lt;/h2&gt;
&lt;p&gt;For simplicity I’ll show the concept of superlearning using only four variables (AKA features or predictors) to predict a continuous outcome. Let’s first simulate a continuous outcome, &lt;code&gt;y&lt;/code&gt;, and four potential predictors, &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, &lt;code&gt;x3&lt;/code&gt;, and &lt;code&gt;x4&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(kableExtra)
set.seed(7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 5000
obs &amp;lt;- tibble(
  id = 1:n,
  x1 = rnorm(n),
  x2 = rbinom(n, 1, plogis(10*x1)),
  x3 = rbinom(n, 1, plogis(x1*x2 + .5*x2)),
  x4 = rnorm(n, mean=x1*x2, sd=.5*x3),
  y = x1 + x2 + x2*x3 + sin(x4)
)
kable(head(obs), digits=3, caption = &amp;quot;Simulated data set&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Simulated data set
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x3
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x4
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
y
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.287
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.385
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.270
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.197
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.197
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.694
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.694
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.541
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.928
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.971
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.971
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.947
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.160
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.107
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34; &gt;
&lt;strong&gt;Step 1: Split data into K folds
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step1.png&#34; style=&#34;width:50.0%&#34; /&gt;
The superlearner algorithm relies on K-fold cross-validation (CV) to avoid overfitting. We will start this process by splitting the data into 10 folds. The easiest way to do this is by creating indices for each CV fold.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- 10 # 10 fold cv
cv_index &amp;lt;- sample(rep(1:k, each = n/k)) # create indices for each CV fold. We need each fold K to contain n (all the rows of our data set) divided by k rows. in our example this is 5000/10 = 500 rows in each fold&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 2: Fit base learners for first CV-fold
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step2.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recall that in K-fold CV, each fold serves as the validation set one time. In this first round of CV, we will train all of our base learners on all the CV folds (k = 1,2,…,9) &lt;em&gt;except&lt;/em&gt; for the very last one: &lt;code&gt;cv_index == 10&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The individual algorithms or &lt;strong&gt;base learners&lt;/strong&gt; that we’ll use here are three linear regressions with differently specified parameters:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Learner A&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y=\beta_0 + \beta_1 X_2 + \beta_2 X_4 + \epsilon\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Learner B&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_3 + \beta_4 sin(X_4) + \epsilon\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Learner C&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_1 X_2 + \beta_5 X_1 X_3 + \beta_6 X_2 X_3 + \beta_7 X_1 X_2 X_3 + \epsilon\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_train_1 &amp;lt;- obs[-which(cv_index == 10),] # make a data set that contains all observations except those in k=1
fit_1a &amp;lt;- glm(y ~ x2 + x4, data=cv_train_1) # fit the first linear regression on that training data
fit_1b &amp;lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train_1) # second LR fit on the training data
fit_1c &amp;lt;- glm(y ~ x1*x2*x3, data=cv_train_1) # and the third LR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am &lt;em&gt;only&lt;/em&gt; using the linear regressions so that code for running more complicated regressions does not take away from understanding the general superlearning algorithm.&lt;/p&gt;
&lt;p&gt;Superlearning actually works best if you use a diverse set, or &lt;strong&gt;superlearner library&lt;/strong&gt;, of base learners. For example, instead of three linear regressions, we could use a least absolute shrinkage estimator (LASSO), random forest, and multivariate adaptive splines (MARS). Any parametric or non-parametric supervised machine learning algorithm can be included as a base learner.&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 3: Obtain predictions for first CV-fold
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step3.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can then get use our validation data, &lt;code&gt;cv_index == 10&lt;/code&gt;, to obtain our first set of cross-validated predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_valid_1 &amp;lt;- obs[which(cv_index == 10),] # make a data set that only contains observations except in k=10
pred_1a &amp;lt;- predict(fit_1a, newdata = cv_valid_1) # use that data set as the validation for all the models in the SL library
pred_1b &amp;lt;- predict(fit_1b, newdata = cv_valid_1) 
pred_1c &amp;lt;- predict(fit_1c, newdata = cv_valid_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we have 5000 &lt;code&gt;obs&lt;/code&gt;ervations, that gives us three vectors of length 500: a set of predictions for each of our Learners A, B, and C.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(pred_1a) # double check we only have n/k predictions ...we do :-)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 500&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(cbind(pred_1a, pred_1b, pred_1c)), digits= 2, caption = &amp;quot;First CV round of predictions&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-6&#34;&gt;Table 2: &lt;/span&gt;First CV round of predictions
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pred_1a
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pred_1b
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pred_1c
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.98
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.78
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.42
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.83
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 4: Obtain CV predictions for entire data set
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step4.png&#34; style=&#34;width:32.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’ll want to get those predictions for &lt;em&gt;every&lt;/em&gt; fold. So, using your favorite &lt;code&gt;for&lt;/code&gt; loop, &lt;code&gt;apply&lt;/code&gt; statement, or &lt;code&gt;map&lt;/code&gt;ping function, fit the base learners and obtain predictions for each of them, so that there are 1000 predictions – one for every point in &lt;code&gt;obs&lt;/code&gt;ervations.&lt;/p&gt;
&lt;p&gt;The way I chose to code this was to make a generic function that combines Step 2 (base learners fit to the training data) and Step 3 (predictions on the validation data), then use &lt;code&gt;map_dfr()&lt;/code&gt; from the &lt;code&gt;purrr&lt;/code&gt; package to repeat over all 10 CV folds. I saved the results in a new data frame called &lt;code&gt;cv_preds&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_folds &amp;lt;- as.list(1:k)
names(cv_folds) &amp;lt;- paste0(&amp;quot;fold&amp;quot;,1:k)

get_preds &amp;lt;- function(fold){   # function that does the same procedure as step 2 and 3 for any CV fold
  cv_train &amp;lt;- obs[-which(cv_index == fold),]  # make a training data set that contains all data except fold k
  fit_a &amp;lt;- glm(y ~ x2 + x4, data=cv_train)  # fit all the base learners to that data
  fit_b &amp;lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train)
  fit_c &amp;lt;- glm(y ~ x1*x2*x3, data=cv_train)
  cv_valid &amp;lt;- obs[which(cv_index == fold),]  # make a validation data set that only contains data from fold k
  pred_a &amp;lt;- predict(fit_a, newdata = cv_valid)  # obtain predictions from all the base learners for that validation data
  pred_b &amp;lt;- predict(fit_b, newdata = cv_valid)
  pred_c &amp;lt;- predict(fit_c, newdata = cv_valid)
  return(data.frame(&amp;quot;obs_id&amp;quot; = cv_valid$id, &amp;quot;cv_fold&amp;quot; = fold, pred_a, pred_b, pred_c))  # save the predictions and the ids of the observations in a data frame
}

cv_preds &amp;lt;- purrr::map_dfr(cv_folds, ~get_preds(fold = .x)) # map_dfr loops through every fold (1:k) and binds the rows of the listed results together

cv_preds %&amp;gt;% arrange(obs_id) %&amp;gt;% head() %&amp;gt;% kable(digits=2, caption = &amp;quot;All CV predictions for all three base learners&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-7&#34;&gt;Table 3: &lt;/span&gt;All CV predictions for all three base learners
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
obs_id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
cv_fold
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pred_a
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pred_b
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pred_c
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1…1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.42
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.28
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1…2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1…3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.69
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1…4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1…5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1…6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.04
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.94
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 5: Choose and compute loss function of interest via metalearner
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step5.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is the key step of the superlearner algorithm: we will use a new learner, a &lt;strong&gt;metalearner&lt;/strong&gt;, to take information from all of the base learners and create that new algorithm.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have cross-validated predictions for every observation in the data set, we want to merge those CV predictions back into our main data set…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obs_preds &amp;lt;- 
  full_join(obs, cv_preds, by=c(&amp;quot;id&amp;quot; = &amp;quot;obs_id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…so that we can minimize a final loss function of interest between the true outcome and each CV prediction. This is how we’re going to optimize our overall prediction algorithm: we want to make sure we’re “losing the least” in the way we combine our base learners’ predictions to ultimately make final predictions. We can do this efficiently by choosing a new learner, a metalearner, which reflects the final loss function of interest.&lt;/p&gt;
&lt;p&gt;For simplicity, we’ll use another linear regression as our metalearner. Using a linear regression as a metalearner will minimize the Cross-Validated Mean Squared Error (CV-MSE) when combining the base learner predictions. Note that we could use a variety of parametric or non-parametric regressions to minimize the CV-MSE.&lt;/p&gt;
&lt;p&gt;No matter what metalearner we choose, the predictors will always be the cross-validated predictions from each base learner, and the outcome will always be the true outcome, &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_fit &amp;lt;- glm(y ~ pred_a + pred_b + pred_c, data = obs_preds)
kable(broom::tidy(sl_fit), digits=3, caption = &amp;quot;Metalearner regression coefficients&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-9&#34;&gt;Table 4: &lt;/span&gt;Metalearner regression coefficients
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
term
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
estimate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
std.error
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
statistic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
p.value
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.003
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.002
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.447
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.148
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pred_a
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.017
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.004
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.739
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pred_b
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.854
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.007
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
128.241
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pred_c
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.165
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.005
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30.103
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This metalearner provides us with the coefficients, or weights, to apply to each of the base learners. In other words, if we have a set of predictions from Learner A, B, and C, we can obtain our best possible predictions by starting with an intercept of -0.003, then adding -0.017 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; predictions from Learner A, 0.854 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; predictions from Learner B, and 0.165 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; predictions from Learner C.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For more information on the metalearning step, check out the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 6: Fit base learners on entire data set
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step6.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After we fit the metalearner, we officially have our superlearner algorithm, so it’s time to input data and obtain predictions! To implement the algorithm and obtain final predictions, we first need to fit the base learners on the full data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_a &amp;lt;- glm(y ~ x2 + x4, data=obs)
fit_b &amp;lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=obs)
fit_c &amp;lt;- glm(y ~ x1*x2*x3, data=obs)&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 7: Obtain predictions from each base learner on entire data set
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step7.png&#34; style=&#34;width:40.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’ll use &lt;em&gt;those&lt;/em&gt; base learner fits to get predictions from each of the base learners for the entire data set, and then we will plug those predictions into the metalearner fit. Remember, we were previously using cross-validated predictions, rather than fitting the base learners on the whole data set. This was to avoid overfitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_a &amp;lt;- predict(fit_a)
pred_b &amp;lt;- predict(fit_b)
pred_c &amp;lt;- predict(fit_c)
full_data_preds &amp;lt;- tibble(pred_a, pred_b, pred_c)&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 8: Use metalearner fit to weight base learners
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step8.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once we have the predictions from the full data set, we can input them to the metalearner, where the output will be a final prediction for &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_predictions &amp;lt;- predict(sl_fit, newdata = full_data_preds)
kable(head(sl_predictions), col.names = &amp;quot;sl_predictions&amp;quot;, digits= 2, caption = &amp;quot;Final SL predictions (manual)&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-12&#34;&gt;Table 5: &lt;/span&gt;Final SL predictions (manual)
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sl_predictions
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.79
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.71
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.02
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.03
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And… that’s it! Those are our superlearner predictions for the full data set.&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 9: Obtain predictions on new data
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;We can now modify Step 7 and Step 8 to accommodate any new observation(s):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;To predict on new data:&lt;/strong&gt;&lt;br&gt; 1. Use the fits from each base learner to obtain base learner predictions for the new observation(s).&lt;br&gt; 2. Plug those base learner predictions into the metalearner fit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can generate a single &lt;code&gt;new_obs&lt;/code&gt;ervation to see how this would work in practice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_obs &amp;lt;- tibble(x1 = .5, x2 = 0, x3 = 0, x4 = -3)
new_pred_a &amp;lt;- predict(fit_a, new_obs)
new_pred_b &amp;lt;- predict(fit_b, new_obs)
new_pred_c &amp;lt;- predict(fit_c, new_obs)
new_pred_df &amp;lt;- tibble(&amp;quot;pred_a&amp;quot; = new_pred_a, &amp;quot;pred_b&amp;quot; = new_pred_b, &amp;quot;pred_c&amp;quot; = new_pred_c)
predict(sl_fit, newdata = new_pred_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1 
## 0.1181052&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our superlearner model predicts that an observation with predictors &lt;span class=&#34;math inline&#34;&gt;\(x1=.5\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x2=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x3=0\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(x4=-3\)&lt;/span&gt; will have an outcome of 0.118. Good to know!&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 10 and beyond…
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;We could compute the MSE of the ensemble superlearner predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_mse &amp;lt;- mean((obs$y - sl_predictions)^2)
sl_mse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01927392&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also add more algorithms to our base learner library (we definitely should, since we only used linear regressions!), and we could write functions to tune these algorithms’ hyperparameters over various grids. For example, if we were to include random forest in our library, we may want to tune over a number of trees and maximum bucket sizes.&lt;/p&gt;
&lt;p&gt;We can then cross-validate this entire process to evaluate the predictive performance of our superlearner algorithm. Alternatively, we could leave a hold-out training data set to evaluate the performance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-superlearner-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the &lt;code&gt;SuperLearner&lt;/code&gt; package&lt;/h1&gt;
&lt;p&gt;Or… we could use a package and avoid all the hand-coding. Here is how you would build an ensemble superlearner for our data with the base learner libraries of &lt;code&gt;ranger&lt;/code&gt; (random forests), &lt;code&gt;glmnet&lt;/code&gt; (LASSO, by default), and &lt;code&gt;earth&lt;/code&gt; (MARS) using the &lt;code&gt;SuperLearner&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(SuperLearner)
x_df &amp;lt;- obs %&amp;gt;% select(x1:x4) %&amp;gt;% as.data.frame()
sl_fit &amp;lt;- SuperLearner(Y = obs$y, X = x_df, family = gaussian(),
                     SL.library = c(&amp;quot;SL.ranger&amp;quot;, &amp;quot;SL.glmnet&amp;quot;, &amp;quot;SL.earth&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can specify the metalearner with the &lt;code&gt;method&lt;/code&gt; argument. The default is &lt;a href=&#34;##non-negative-least-squares&#34;&gt;Non-Negative Least Squares&lt;/a&gt; (NNLS).&lt;/p&gt;
&lt;div id=&#34;cv-risk-and-coefficient-weights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CV-Risk and Coefficient Weights&lt;/h2&gt;
&lt;p&gt;We can examine the cross-validated &lt;code&gt;Risk&lt;/code&gt; (loss function), and the &lt;code&gt;Coef&lt;/code&gt;ficient (weight) given to each of the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  
## SuperLearner(Y = obs$y, X = x_df, family = gaussian(), SL.library = c(&amp;quot;SL.ranger&amp;quot;,  
##     &amp;quot;SL.glmnet&amp;quot;, &amp;quot;SL.earth&amp;quot;)) 
## 
## 
##                      Risk      Coef
## SL.ranger_All 0.013278476 0.1619231
## SL.glmnet_All 0.097149642 0.0000000
## SL.earth_All  0.003168299 0.8380769&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this summary we can see that the CV-risk (the default risk is MSE) in this library of base learners is smallest for &lt;code&gt;SL.Earth&lt;/code&gt;. This translates to the largest coefficient, or weight, given to the predictions from &lt;code&gt;earth&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The LASSO model implemented by &lt;code&gt;glmnet&lt;/code&gt; has the largest CV-risk, and after the metalearning step, those predictions receive a coefficient, or weight, of 0. This means that the predictions from LASSO will not be incorporated into the final predictions at all.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;obtaining-the-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Obtaining the predictions&lt;/h2&gt;
&lt;p&gt;We can extract the predictions easily via the &lt;code&gt;SL.predict&lt;/code&gt; element of the &lt;code&gt;SuperLearner&lt;/code&gt; fit object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(head(sl_fit$SL.predict), digits=2, col.names = &amp;quot;sl_predictions&amp;quot;, caption = &amp;quot;Final SL predictions (package)&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-17&#34;&gt;Table 6: &lt;/span&gt;Final SL predictions (package)
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sl_predictions
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.68
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.87
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.08
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validated-superlearner&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross-validated Superlearner&lt;/h2&gt;
&lt;p&gt;Recall that we can cross-validate the entire model fitting process to evaluate the predictive performance of our superlearner algorithm. This is easy with the function &lt;code&gt;CV.SuperLearner()&lt;/code&gt;. Beware, this gets computationally burdensome very quickly!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_sl_fit &amp;lt;- CV.SuperLearner(Y = obs$y, X = x_df, family = gaussian(),
                     SL.library = c(&amp;quot;SL.ranger&amp;quot;, &amp;quot;SL.glmnet&amp;quot;, &amp;quot;SL.earth&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more information on the &lt;code&gt;SuperLearner&lt;/code&gt; package, take a look at this &lt;a href=&#34;https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html&#34;&gt;vignette&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-packages-to-superlearn&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alternative packages to superlearn&lt;/h2&gt;
&lt;p&gt;Other packages freely available in &lt;code&gt;R&lt;/code&gt; that can be used to implement the superlearner algorithm include &lt;a href=&#34;https://tlverse.org/tlverse-handbook/sl3.html&#34;&gt;&lt;code&gt;sl3&lt;/code&gt;&lt;/a&gt; (an update to the original &lt;code&gt;Superlearner&lt;/code&gt; package), &lt;a href=&#34;https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html&#34;&gt;&lt;code&gt;h2o&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://rdrr.io/cran/caretEnsemble/f/vignettes/caretEnsemble-intro.Rmd&#34;&gt;&lt;code&gt;caretEnsemble&lt;/code&gt;&lt;/a&gt;. I previously wrote a &lt;a href=&#34;https://www.khstats.com/blog/sl3_demo/sl/&#34;&gt;brief demo&lt;/a&gt; on using &lt;code&gt;sl3&lt;/code&gt; for an NYC R-Ladies demo. At this time &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;&lt;code&gt;tidymodels&lt;/code&gt;&lt;/a&gt; does not yet support superlearning directly, but Alex Hayes previously wrote a blog post on &lt;a href=&#34;https://www.alexpghayes.com/blog/implementing-the-super-learner-with-tidymodels/&#34;&gt;using &lt;code&gt;tidymodels&lt;/code&gt; infrastructure to implement superlearning&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coming-soon-when-prediction-is-not-the-end-goal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Coming soon… when prediction is not the end goal&lt;/h1&gt;
&lt;p&gt;When prediction is not the end goal, superlearning combines well with semi-parametric estimation methods for statistical inference. This is the reason I was reading &lt;em&gt;Targeted Learning&lt;/em&gt; in the first place; I am a statistician with collaborators who typically want estimates of treatment effects with confidence intervals, not predictions!&lt;/p&gt;
&lt;p&gt;I’m working on a similar visual guide for one such semiparametric estimation method: &lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;Targeted Maximum Likelihood Estimation&lt;/a&gt; (TMLE)). TMLE allows the use of flexible statistical models like the superlearner algorithm when estimating treatment effects. If you found this superlearning tutorial helpful, check back here later for another one on TMLE. If you’re curious about TMLE in the meantime, I really like &lt;a href=&#34;https://migariane.github.io/TMLE.nb.html&#34;&gt;this tutorial&lt;/a&gt; by Miguel Angel Luque Fernandez.&lt;/p&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;/img/TMLE.jpg&#34;
         width=100%&#34;&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;These sections contain a bit of extra information on the superlearning algorithm, such as: intuition on manually computing the loss function of interest, explanation of the discrete superlearner, brief advice on choosing a metalearner, and a different summary visual provided in the &lt;em&gt;Targeted Learning&lt;/em&gt; book.&lt;/p&gt;
&lt;div id=&#34;manually-computing-the-mse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Manually computing the MSE&lt;/h3&gt;
&lt;p&gt;Let’s say we have chosen our loss function of interest to be the Mean Squared Error (MSE). We could first compute the squared error &lt;span class=&#34;math inline&#34;&gt;\((y - \hat{y})^2\)&lt;/span&gt; for each CV prediction A, B, and C.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_sq_error &amp;lt;-
  obs_preds %&amp;gt;%
  mutate(cv_sqrd_error_a = (y-pred_a)^2,   # compute squared error for each observation
         cv_sqrd_error_b = (y-pred_b)^2,
         cv_sqrd_error_c = (y-pred_c)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_sq_error %&amp;gt;% 
  pivot_longer(c(cv_sqrd_error_a, cv_sqrd_error_b, cv_sqrd_error_c), # make the CV squared errors long form for plotting
               names_to = &amp;quot;base_learner&amp;quot;,
               values_to = &amp;quot;squared_error&amp;quot;) %&amp;gt;%
  mutate(base_learner = toupper(str_remove(base_learner, &amp;quot;cv_sqrd_error_&amp;quot;))) %&amp;gt;%
  ggplot(aes(base_learner, squared_error, col=base_learner)) + # make box plots
  geom_boxplot() +
  theme_bw() +
  guides(col=F) +
  labs(x = &amp;quot;Base Learner&amp;quot;, y=&amp;quot;Squared Error&amp;quot;, title=&amp;quot;Squared Errors of Learner A, B, and C&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/sl/superlearning_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And then take the mean of those three cross-validated squared error columns, grouped by &lt;code&gt;cv_fold&lt;/code&gt;, to get the CV-MSE for each fold.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_risks &amp;lt;-
  cv_sq_error %&amp;gt;%
  group_by(cv_fold) %&amp;gt;%
  summarise(cv_mse_a = mean(cv_sqrd_error_a),
            cv_mse_b = mean(cv_sqrd_error_b),
            cv_mse_c = mean(cv_sqrd_error_c)
            )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_risks %&amp;gt;%
  pivot_longer(cv_mse_a:cv_mse_c,
               names_to = &amp;quot;base_learner&amp;quot;,
               values_to = &amp;quot;mse&amp;quot;) %&amp;gt;%
  mutate(base_learner = toupper(str_remove(base_learner,&amp;quot;cv_mse_&amp;quot;)))  %&amp;gt;%
  ggplot(aes(cv_fold, mse, col=base_learner)) +
  geom_point() +
  theme_bw()  +
    scale_x_continuous(breaks = 1:10) +
  labs(x = &amp;quot;Cross-Validation (CV) Fold&amp;quot;, y=&amp;quot;Mean Squared Error (MSE)&amp;quot;, col = &amp;quot;Base Learner&amp;quot;, title=&amp;quot;CV-MSEs for Base Learners A, B, and C&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/sl/superlearning_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that across each fold, Learner B consistently has an MSE around 0.02, while Learner C hovers around 0.1, and Learner A varies between 0.35 and .45. We can take another mean to get the overall CV-MSE for each learner.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_risks %&amp;gt;%
  select(-cv_fold) %&amp;gt;%
  summarise_all(mean) %&amp;gt;%
  kable(digits=2, caption = &amp;quot;CV-MSE for each base learner&amp;quot;) %&amp;gt;%
  kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-22&#34;&gt;Table 7: &lt;/span&gt;CV-MSE for each base learner
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
cv_mse_a
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
cv_mse_b
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
cv_mse_c
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.38
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.11
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The base learner that performs the best using our chosen loss function of interest is clearly Learner B. We can see from our data simulation code why this is true – Learner B is almost exactly the mimicking the data generating mechanism of &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Our results align with the linear regression fit from our metalearning step; Learner B predictions received a much larger coefficient relative to Learners A and C.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discrete-superlearner&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Discrete Superlearner&lt;/h3&gt;
&lt;p&gt;We &lt;em&gt;could&lt;/em&gt; stop after minimizing our loss function (MSE) and fit Learner B to our full data set, and that would be called using the &lt;strong&gt;discrete superlearner&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;discrete_sl_predictions &amp;lt;- predict(glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=obs))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we can almost always create an even better prediction algorithm if we use information from &lt;em&gt;all&lt;/em&gt; of the algorithms’ CV predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-metalearner&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Choosing a metalearner&lt;/h3&gt;
&lt;p&gt;In the &lt;a href=&#34;#references&#34;&gt;Reference&lt;/a&gt; papers on superlearning, the metalearner which yields the best results theoretically and in practice is a &lt;strong&gt;convex combination optimization&lt;/strong&gt; of learners. This means fitting the following regression, where &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_3\)&lt;/span&gt; are all non-negative and sum to 1.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{E}[Y|\hat{Y}_{LrnrA},\hat{Y}_{LrnrB},\hat{Y}_{LrnrC}] = \alpha_1\hat{Y}_{LrnrA} + \alpha_2\hat{Y}_{LrnrB} + \alpha_3\hat{Y}_{LrnrC}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The default in the &lt;code&gt;Superlearner&lt;/code&gt; package is to fit a non-negative least squares (NNLS) regression. NNLS fits the above equation where the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;’s must be greater than or equal to 1 but do not necessarily sum to 1. The package then reweights the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;’s to force them to sum to 1. This makes the weights a convex combination, but may not yield the same optimal results as an initial convex combination optimization.&lt;/p&gt;
&lt;p&gt;The metalearner should change with the goals of the prediction algorithm and the loss function of interest. In these examples it is the MSE, but if the goal is to build a prediction algorithm that is best for binary classification, the loss function of interest may be the rank loss, or &lt;span class=&#34;math inline&#34;&gt;\(1-AUC\)&lt;/span&gt;. It is outside the scope of this post, but for more information, I recommend this &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4912128/&#34;&gt;paper&lt;/a&gt; by Erin Ledell on maximizing the Area Under the Curve (AUC) with superlearner algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-visual-guide-for-superlearning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Another visual guide for superlearning&lt;/h3&gt;
&lt;p&gt;The steps of the superlearner algorithm are summarized nicely in this graphic in Chapter 3 of the &lt;em&gt;Targeted Learning&lt;/em&gt; book:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_diagram.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Acknowledgments&lt;/h1&gt;
&lt;p&gt;Thank you to Eric Polley, Iván Díaz, Nick Williams, Anjile An, and Adam Peterson for very helpful content (and design!) suggestions for this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;MJ Van der Laan, EC Polley, AE Hubbard, Super Learner, Statistical applications in genetics and molecular, 2007&lt;/p&gt;
&lt;p&gt;Polley, Eric. “Chapter 3: Superlearning.” Targeted Learning: Causal Inference for Observational and Experimental Data, by M. J. van der. Laan and Sherri Rose, Springer, 2011.&lt;/p&gt;
&lt;p&gt;Polley E, LeDell E, Kennedy C, van der Laan M. Super Learner: Super Learner Prediction. 2016 URL &lt;a href=&#34;https://CRAN.R-project.org/package=SuperLearner&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=SuperLearner&lt;/a&gt;. R package version 2.0-22.&lt;/p&gt;
&lt;p&gt;Naimi AI, Balzer LB. Stacked generalization: an introduction to super learning. &lt;em&gt;Eur J Epidemiol.&lt;/em&gt; 2018;33(5):459-464. &lt;a href=&#34;doi:10.1007/s10654-018-0390-z&#34; class=&#34;uri&#34;&gt;doi:10.1007/s10654-018-0390-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LeDell, E. (2015). Scalable Ensemble Learning and Computationally Efficient Variance Estimation. UC Berkeley. ProQuest ID: LeDell_berkeley_0028E_15235. Merritt ID: ark:/13030/m5wt1xp7. Retrieved from &lt;a href=&#34;https://escholarship.org/uc/item/3kb142r2&#34; class=&#34;uri&#34;&gt;https://escholarship.org/uc/item/3kb142r2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;M. Petersen and L. Balzer. Introduction to Causal Inference. UC Berkeley, August 2014. &lt;a href=&#34;www.ucbbiostat.com&#34;&gt;www.ucbbiostat.com&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] SuperLearner_2.0-26 nnls_1.4            kableExtra_1.1.0   
##  [4] forcats_0.5.0       stringr_1.4.0       dplyr_1.0.2        
##  [7] purrr_0.3.4         readr_1.3.1         tidyr_1.1.2        
## [10] tibble_3.0.3        ggplot2_3.3.2       tidyverse_1.3.0    
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.4.1         jsonlite_1.6.1     viridisLite_0.3.0  foreach_1.5.0     
##  [5] modelr_0.1.6       Formula_1.2-3      assertthat_0.2.1   highr_0.8         
##  [9] cellranger_1.1.0   yaml_2.2.1         pillar_1.4.6       backports_1.1.8   
## [13] lattice_0.20-38    glue_1.4.2         digest_0.6.25      rvest_0.3.5       
## [17] colorspace_1.4-1   htmltools_0.4.0    Matrix_1.2-18      pkgconfig_2.0.3   
## [21] broom_0.7.0        earth_5.1.2        haven_2.2.0        bookdown_0.19     
## [25] scales_1.1.1       webshot_0.5.2      ranger_0.12.1      TeachingDemos_2.12
## [29] generics_0.0.2     farver_2.0.3       ellipsis_0.3.1     withr_2.2.0       
## [33] cli_2.0.2          magrittr_1.5       crayon_1.3.4       readxl_1.3.1      
## [37] evaluate_0.14      fs_1.4.1           fansi_0.4.1        xml2_1.3.0        
## [41] cabinets_0.5.0     blogdown_0.19      tools_3.6.3        hms_0.5.3         
## [45] lifecycle_0.2.0    munsell_0.5.0      reprex_0.3.0       glmnet_3.0-2      
## [49] plotrix_3.7-7      compiler_3.6.3     rlang_0.4.7        plotmo_3.5.7      
## [53] grid_3.6.3         iterators_1.0.12   rstudioapi_0.11    labeling_0.3      
## [57] rmarkdown_2.1      gtable_0.3.0       codetools_0.2-16   DBI_1.1.0         
## [61] R6_2.4.1           lubridate_1.7.9    knitr_1.28         shape_1.4.4       
## [65] stringi_1.4.6      Rcpp_1.0.5         vctrs_0.3.4        dbplyr_1.4.3      
## [69] tidyselect_1.1.0   xfun_0.14&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Key to Learning Causal Inference</title>
      <link>/blog/collider-bias/hardest_part_of_causal/</link>
      <pubDate>Sat, 12 Sep 2020 21:13:14 -0500</pubDate>
      <guid>/blog/collider-bias/hardest_part_of_causal/</guid>
      <description>


&lt;p&gt;I’ve been trying to learn causal inference for the past year and a half. Not in the way people say, “I’ve been trying to learn guitar” or “I’ve been trying to learn Japanese”, either. Trying as in, spent too many pre-pandemic Sundays at crowded coffee shops with a backpack causal inference books, and even dragged a binder of notes to a vacation in Costa Rica…&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/causal_at_beach.jpg&#34; /&gt;
I’m no expert in causal inference, but at this point I at least have an opinion on the best way to &lt;em&gt;learn&lt;/em&gt; causal inference. It’s actually a pretty simple concept, but I was really spinning my wheels until I learned it. So, in case you’re early into your causal inference journey and haven’t discovered it yet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I believe the key to learning (and applying) causal inference is to always keep &lt;strong&gt;identification&lt;/strong&gt; separate from &lt;strong&gt;estimation&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now what does &lt;em&gt;that&lt;/em&gt; mean!? Broadly, &lt;strong&gt;identification means figuring out whether you can write a causal estimand of interest in terms of data&lt;/strong&gt; (i.e., in forms of expectations of random variables).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;causal estimand&lt;/strong&gt; is just some summary quantity that would be interesting to evaluate in the system you’re studying. A few common causal estimands are the Average Treatment Effect and the Conditional Average Treatment Effect. There are many, many more though, such as the Dose Response Curve, Causal Odds Ratio, Causal Risk Ratio, and Marginal Structural Model.&lt;/p&gt;
&lt;p&gt;To determine whether you can identify your causal estimand of interest in your system of study and with the data you have, careful thought is required. It’s the stage of a problem when you, as a statistician, sit down and talk with subject matter experts about the precise question they’d like to answer. You may draw &lt;em&gt;Directed Acyclic Graphs&lt;/em&gt; (DAGs) or &lt;em&gt;Single World Intervention Graphs&lt;/em&gt; (SWIGs), or write out the &lt;em&gt;Non Parametric Structural Equations&lt;/em&gt; of the system.&lt;/p&gt;
&lt;p&gt;These tools will help you evaluate some of the main requirements of causal inference: for the causal estimand of interest to be identified, you must satisfy causal assumptions of &lt;em&gt;positivity&lt;/em&gt;, &lt;em&gt;exchangeability&lt;/em&gt;, and &lt;em&gt;ignorability&lt;/em&gt;, while still avoiding the usual pitfalls in statistics like &lt;em&gt;selection bias&lt;/em&gt; and &lt;em&gt;unmeasured confounding&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You’ll eventually determine whether the causal estimand is possible to write out in terms of your data. For example, if your goal is to identify the Average Treatment Effect (ATE), i.e. the mean difference in outcomes between the treated and untreated in a hypothetical world where everyone &lt;em&gt;could&lt;/em&gt; get each treatment &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, your identification of the ATE, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, may look like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta = E_W[E[Y|A=1,W]-E[Y|A=0,W]]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is a vector of confounding variables that affect both &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Once&lt;/p&gt;
&lt;p&gt;Everything I’ve mentioned so far is part of the &lt;strong&gt;identification&lt;/strong&gt; side of causal inference. I think of it as the non-data side, because we are not yet dealing with data when we think about the study design, causal estimand, and whether the causal conditions are reasonable to assume. You should never be thinking about &lt;strong&gt;estimation methods&lt;/strong&gt; until you’ve determined whether your causal estimand can be identified.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/causal_comic.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once you’ve &lt;em&gt;identified&lt;/em&gt; your WHAT, or your estimand of interest, then you can start thinking about HOW you’re going to &lt;em&gt;estimate&lt;/em&gt; that estimand. There are many statistical estimation methods to estimate the same causal effect of interest.&lt;/p&gt;
&lt;p&gt;My favorite way to think about this is through a little graph like this. On the left, there are three causal estimands, or estimands of interest. On the top, we can see six different ways you might try to estimate any of these causal estimands of interest. Each of the estimation methods have pros and cons.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Odds ratio = E[Y]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Relative risk = E[Y]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{ATE} = E[Y]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A common example is figuring out whether the Average Treatment Effect (ATE), or the difference in outcomes if everyone were to receive the exposure compared to if everyone had not received the exposure&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta^* = E[E[Y|A=1,W]-E[Y|A=0,W]]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Sure, propensity scores are part of causal inference, but they’re part of the &lt;em&gt;estimation&lt;/em&gt; side of causal inference. A propensity score can’t fix a research question with a causal estimand that does not meet causal assumptions.&lt;/p&gt;
&lt;p&gt;Causal&lt;/p&gt;
&lt;p&gt;I think causal inference is the most interesting topic in all of statistics– I think it’s so cool that sometimes I make really dweeby comics about it:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Conditional and Iterated Expectations with a Linear Regression Model</title>
      <link>/blog/iterated-expectations/iterated-expectations/</link>
      <pubDate>Sat, 14 Mar 2020 21:13:14 -0500</pubDate>
      <guid>/blog/iterated-expectations/iterated-expectations/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;hr /&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TL;DR&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can a regress an outcome on a grouping variable &lt;em&gt;plus any other variable(s)&lt;/em&gt; and the unadjusted and adjusted group means will be identical.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can see this in a simple example using the &lt;a href=&#34;https://github.com/allisonhorst/palmerpenguins&#34;&gt;&lt;code&gt;palmerpenguins&lt;/code&gt;&lt;/a&gt; data:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remotes::install_github(&amp;quot;allisonhorst/palmerpenguins&amp;quot;)
library(palmerpenguins)
library(tidyverse)
library(gt)

# use complete cases for simplicity
penguins &amp;lt;- drop_na(penguins)

penguins %&amp;gt;%
  # fit a linear regression for bill length given bill depth and species
  # make a new column containing the fitted values for bill length
  mutate(preds = predict(lm(bill_length_mm ~ bill_depth_mm + species, data = .))) %&amp;gt;%
  # compute unadjusted and adjusted group means
  group_by(species) %&amp;gt;%
  summarise(mean_bill_length = mean(bill_length_mm),
            mean_predicted_bill_length = mean(preds)) %&amp;gt;%
  gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#mcysheiceb .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#mcysheiceb .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#mcysheiceb .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#mcysheiceb .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#mcysheiceb .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#mcysheiceb .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#mcysheiceb .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#mcysheiceb .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#mcysheiceb .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#mcysheiceb .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#mcysheiceb .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#mcysheiceb .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#mcysheiceb .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#mcysheiceb .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#mcysheiceb .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#mcysheiceb .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#mcysheiceb .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#mcysheiceb .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#mcysheiceb .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#mcysheiceb .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#mcysheiceb .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#mcysheiceb .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#mcysheiceb .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#mcysheiceb .gt_left {
  text-align: left;
}

#mcysheiceb .gt_center {
  text-align: center;
}

#mcysheiceb .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#mcysheiceb .gt_font_normal {
  font-weight: normal;
}

#mcysheiceb .gt_font_bold {
  font-weight: bold;
}

#mcysheiceb .gt_font_italic {
  font-style: italic;
}

#mcysheiceb .gt_super {
  font-size: 65%;
}

#mcysheiceb .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;mcysheiceb&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;species&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_bill_length&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_predicted_bill_length&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Adelie&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Chinstrap&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Gentoo&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is because &lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z]=E[Y|Z=z]\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can view a fitted value from the regression, &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z]\)&lt;/span&gt;, as a random variable to help us see this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-by-step-proof&#34;&gt;Skip to the end&lt;/a&gt; to see the proof.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src=&#34;/img/expectations.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ll admit I spent many weeks of my first probability theory course struggling to understand when and why my professor was writing &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. When I finally learned all the rules for expectations of random variables, I still had zero appreciation for their implications in my future work as an applied statistician.&lt;/p&gt;
&lt;p&gt;I recently found myself in a rabbit hole of expectation properties while trying to write a seemingly simple function in &lt;code&gt;R&lt;/code&gt;. Now that I have the output of my function all sorted out, I have a newfound appreciation for how I can use regressions – a framework I’m very comfortable with – to rethink some of the properties I learned in my probability theory courses.&lt;/p&gt;
&lt;p&gt;In the function, I was regressing an outcome on a few variables plus a grouping variable, and then returning the group means of the fitted values. My function kept outputting adjusted group means that were &lt;em&gt;identical&lt;/em&gt; to the unadjusted group means.&lt;/p&gt;
&lt;p&gt;I soon realized that for what I needed to do, my grouping variable should not be in the regression model. However, I was still perplexed as to how the adjusted and unadjusted group means could be the same.&lt;/p&gt;
&lt;p&gt;I created a very basic example to test this unexpected result. I regressed a variable from the new &lt;code&gt;penguins&lt;/code&gt; data set, &lt;code&gt;bill_length_mm&lt;/code&gt;, on another variable called &lt;code&gt;bill_depth_mm&lt;/code&gt; and a grouping variable &lt;code&gt;species&lt;/code&gt;. I then looked at the mean within each category of &lt;code&gt;species&lt;/code&gt; for both the unadjusted &lt;code&gt;bill_depth_mm&lt;/code&gt; and fitted values from my linear regression model for &lt;code&gt;bill_depth_mm&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins %&amp;gt;%
  # fit a linear regression for bill length given bill depth and species
  # make a new column containing the fitted values for bill length
  mutate(preds = predict(lm(bill_length_mm ~ bill_depth_mm + species, data = .))) %&amp;gt;%
  # compute unadjusted and adjusted group means
  group_by(species) %&amp;gt;%
  summarise(mean_bill_length = mean(bill_length_mm),
            mean_predicted_bill_length = mean(preds)) %&amp;gt;%
  gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#ecegfguyhs .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ecegfguyhs .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ecegfguyhs .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ecegfguyhs .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ecegfguyhs .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ecegfguyhs .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ecegfguyhs .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ecegfguyhs .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ecegfguyhs .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ecegfguyhs .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#ecegfguyhs .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ecegfguyhs .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ecegfguyhs .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#ecegfguyhs .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#ecegfguyhs .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ecegfguyhs .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#ecegfguyhs .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ecegfguyhs .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#ecegfguyhs .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ecegfguyhs .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ecegfguyhs .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ecegfguyhs .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#ecegfguyhs .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#ecegfguyhs .gt_left {
  text-align: left;
}

#ecegfguyhs .gt_center {
  text-align: center;
}

#ecegfguyhs .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ecegfguyhs .gt_font_normal {
  font-weight: normal;
}

#ecegfguyhs .gt_font_bold {
  font-weight: bold;
}

#ecegfguyhs .gt_font_italic {
  font-style: italic;
}

#ecegfguyhs .gt_super {
  font-size: 65%;
}

#ecegfguyhs .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;ecegfguyhs&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;species&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_bill_length&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_predicted_bill_length&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Adelie&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Chinstrap&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Gentoo&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;I saw the same strange output, even in my simple example. I realized this must be some statistics property I’d learned about and since forgotten, so I decided to write out what I was doing in expectations.&lt;/p&gt;
&lt;p&gt;First, I wrote down the unadjusted group means in the form of an expectation. I wrote down a conditional expectation, since we are looking at the mean of &lt;code&gt;bill_length_mm&lt;/code&gt; when &lt;code&gt;species&lt;/code&gt; is restricted to a certain category. We can explicitly show this by taking the expectation of a random variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Bill Length}\)&lt;/span&gt;, while setting another random variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;, equal to only one category at a time.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Adelie]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Chinstrap]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Gentoo]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;More generally, we could write out the unadjusted group mean using a group indicator variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;, which can take on all possible values &lt;span class=&#34;math inline&#34;&gt;\(species\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=species]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So that’s our unadjusted group means. What about the adjusted group mean? We can start by writing out the linear regression model, which is the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{BillLength}\)&lt;/span&gt;, conditional on the random variables &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{BillDepth}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When I used the &lt;code&gt;predict&lt;/code&gt; function on the fit of that linear regression model, I obtained the fitted values from that expectation, before I separated the fitted values by group to get the grouped means. We can see those fitted values as random variables themselves, and write out another conditional mean using a group indicator variable, just as we did for the unadjusted group means earlier.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]|\mathrm{Species}=species]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;My table of unadjusted and adjusted Bill Length means thus showed me that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]|\mathrm{Species}=species] \\ = E[\mathrm{BillLength}|\mathrm{Species}=species]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Or, in more general notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[Y|X,Z]|Z=z] = E[Y|Z=z]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Is it true?! Spoiler alert – yes. Let’s work through the steps of the proof one by one.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;proof-set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Proof set-up&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Let’s pretend for the proof that both our &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (outcome), &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (adjustment variable), and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; (grouping variable) are categorical (discrete) variables. This is just to make the math a bit cleaner, since the expectation of a discrete variable (a weighted summation) is a little easier to show than the expectation of a continuous variable (the integral of a probability density function times the realization of the random variable).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A few fundamental expectation results we’ll need:&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;conditional-probability&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Conditional probability&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A|B) = \frac{P(A ∩ B)}{P(B)}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partition-theorem&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Partition theorem&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[A|B] = \sum_Ba \cdot P(A=a|B=b)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-distribution-from-a-joint-distribution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Marginal distribution from a joint distribution&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_A\sum_Ba\cdot P(A=a,B=b) = \sum_Aa\sum_B\cdot P(A=a,B=b) = \sum_Aa\cdot P(A=a)=E[A]\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-by-step-proof&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step-by-step Proof&lt;/h1&gt;
&lt;p&gt;Click on the superscript number after each step for more information.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=E[E[Y|X,Z=z]|Z=z]\)&lt;/span&gt; &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}E[Y|X=x,Z=z]\cdot P(X=x|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y P(Y=y|X=x,Z=z)\cdot P(X=x|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y \frac{P(Y=y,X=x,Z=z)}{P(X=x,Z=z)}\cdot \frac{P(X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y \frac{P(Y=y,X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y\sum_{X}\frac{P(Y=y,X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y\frac{P(Y=y,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y P(Y=y|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=E[Y|Z=z]\)&lt;/span&gt; &lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, we’ve proved that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z] = E[Y|Z=z]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which, thankfully, means I have an answer to my function output confusion. It was a lightbulb moment for me to realize I should think of an inner expectation as a random variable, and all the rules I learned about conditional and iterated expectations can be revisited in the regressions I fit on a daily basis.&lt;/p&gt;
&lt;p&gt;Here’s hoping you too feel inspired to revisit probability theory from time to time, even if your work is very applied. It is, after all, a perfect activity for social distancing! 😷&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0090081&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0090081&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.math.arizona.edu/~tgk/464_07/cond_exp.pdf&#34;&gt;A Conditional Expectation - Arizona Math&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Because we’re making our outer expectation conditional on &lt;span class=&#34;math inline&#34;&gt;\(Z=z\)&lt;/span&gt;, we can also move &lt;span class=&#34;math inline&#34;&gt;\(Z=z\)&lt;/span&gt; into our inner expectation. This becomes obvious in the &lt;code&gt;penguins&lt;/code&gt; example, since we only use the fitted values from one category of &lt;code&gt;species&lt;/code&gt; to get the adjusted group mean for that category.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;We can rewrite &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z=z]\)&lt;/span&gt; as the weighted summation of all possible values &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can take. &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z=z]\)&lt;/span&gt; will only ever be able to take values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; that vary over the range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; since our value &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is already fixed. We can weight each of these possible &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; values by &lt;span class=&#34;math inline&#34;&gt;\(P(X=x|Z=z)\)&lt;/span&gt;, since that’s the probabilty &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; will take value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; at our already-fixed &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. Thus, we can start to find &lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z=z]|Z=z]\)&lt;/span&gt; by weighting each &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(P(X=x|Z=z)\)&lt;/span&gt; and adding them all up (see Partition Theorem).&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;We can get the expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at each of those possible values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by a similar process as step 2 (weighting each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(P(Y=y|X=x, Z=z)\)&lt;/span&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;By the Law of Conditional Probability, we can rewrite our conditional probabilities as joint distributions.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The denominator of the first fraction cancels out with the numerator of the second fraction.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;We can switch the summations around so that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is outside the summation over all values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. This lets us get the joint distribution of only &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;This is a conditional expectation, written in the form of a joint distribution.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;By the Partition Theorem.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Rewriting the previous equation as an expectation.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
