<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics | KHstats</title>
    <link>/categories/statistics/</link>
      <atom:link href="/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 09 Nov 2020 21:13:14 -0500</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>statistics</title>
      <link>/categories/statistics/</link>
    </image>
    
    <item>
      <title>An Illustrated Introduction to Targeted Maximum Likelihood Estimation (TMLE)</title>
      <link>/blog/tmle/tutorial/</link>
      <pubDate>Mon, 09 Nov 2020 21:13:14 -0500</pubDate>
      <guid>/blog/tmle/tutorial/</guid>
      <description>


&lt;blockquote&gt;
&lt;p&gt;A beginner‚Äôs guide to understanding Targeted Maximum Likelihood Estimation (TMLE) via a step-by-step tutorial for estimating the Average Treatment Effect (ATE).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;/img/TMLE.jpg&#34;
         width=100%&#34;&gt;
&lt;figcaption&gt;
&lt;em&gt;&lt;a href=&#34;&#34;&gt;T&lt;/a&gt;here is a condensed version of this tutorial as an 8.5x11&#34; pdf on my Github in case you would like to print it out for reference as you read more formal TMLE explanations.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;hr /&gt;
&lt;div id=&#34;tmle-in-three-sentences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. TMLE in three sentences üéØ&lt;/h1&gt;
&lt;p&gt;Targeted Maximum Likelihood Estimation (TMLE) is a general semiparametric estimation framework to &lt;strong&gt;estimate a statistical quantity of interest&lt;/strong&gt;. TMLE allows the use of &lt;strong&gt;machine learning&lt;/strong&gt; (ML) models which place &lt;strong&gt;minimal assumptions on the distribution of the data&lt;/strong&gt;. Unlike estimates normally obtained from ML, the &lt;strong&gt;final TMLE estimate will still have valid standard errors for statistical inference&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-analysts-motivation-for-learning-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. An Analyst‚Äôs Motivation for Learning TMLE üë©üèº‚Äçüíª&lt;/h1&gt;
&lt;p&gt;When I graduated with my MS in Biostatistics two years ago, I had a mental framework of statistics and data science that I think is pretty common among new graduates. It went like this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;If the goal is &lt;span style=&#34;color: #3366ff;&#34;&gt;inference&lt;/span&gt;, use an &lt;span style=&#34;color: #3366ff;&#34;&gt;interpretable (usually parametric) model&lt;/span&gt; and explain the meaning behind the coefficients and their standard errors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the goal is &lt;span style=&#34;color: #cc0000;&#34;&gt;prediction&lt;/span&gt;, use a &lt;span style=&#34;color: #cc0000;&#34;&gt;flexible machine learning model&lt;/span&gt; and then look at performance metrics and variable importance.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This mentality changed drastically when I started learning about semiparametric estimation methods like TMLE in the context of causal inference. I quickly realized two flaws in this mental framework.&lt;/p&gt;
&lt;p&gt;First, I was thinking about inference backwards: I was choosing a model based on my outcome type (binary, continuous, time-to-event, repeated measures) and then interpreting specific coefficients as my estimates of interest. Yet it makes way more sense to &lt;em&gt;first&lt;/em&gt; determine the statistical quantity, or &lt;strong&gt;estimand&lt;/strong&gt;, that best answers a scientific question, and &lt;em&gt;then&lt;/em&gt; use the method, or &lt;strong&gt;estimator&lt;/strong&gt;, best suited for estimating it. This is the paradigm TMLE is based upon: we want to build an algorithm &lt;strong&gt;targeted to an estimand of interest&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;/img/tmle/estimator.png&#34;
    width= 70%
         alt=&#34;Estimator and Estimand&#34;&gt;
&lt;figcaption&gt;
An estimand is a quantity that answers a scientific question of interest. Once we figure out the estimand, we can build an algorithm, or estimator, to estimate it. Image courtesy of Dr.¬†Laura Hatfield &lt;a href=&#34;https://diff.healthpolicydatascience.org/&#34; class=&#34;uri&#34;&gt;https://diff.healthpolicydatascience.org/&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Second, I thought flexible, data-adaptive models we commonly classify as &lt;em&gt;machine learning&lt;/em&gt; could only be used for prediction, since they don‚Äôt have asymptotic properties for inference (i.e.¬†standard errors). However, certain semiparametric estimation methods like TMLE can actually use these models to obtain a final estimate that is closer to the target quantity than would be obtained using standard parametric models (e.g.¬†linear and logistic regression). This is because machine learning models can often accommodate large numbers of confounders with complex relationships better than parametric models.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The way we use the machine learning estimates in TMLE, surprisingly enough, yields &lt;strong&gt;known asymptotic properties of bias and variance&lt;/strong&gt; ‚Äì just like we see in parametric maximum likelihood estimation ‚Äì for our target estimand.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;why-the-visual-guide&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Why the Visual Guide? üé®&lt;/h1&gt;
&lt;p&gt;TMLE was developed in 2007 by Mark van der Laan at UC Berkeley, and it is slowly but surely starting to see more widespread use. Since learning about TMLE, I‚Äôve believed many more analysts with a skill set similar to mine &lt;em&gt;could&lt;/em&gt; be using TMLE, but perhaps find even the most introductory resources to be a bit daunting.&lt;/p&gt;
&lt;p&gt;I am a very applied thinker, and I‚Äôve tried to write this tutorial in the way I would have found most useful and accessible when I began learning about TMLE. Each step is accompanied by a non-rigorous explanation, an equation using the simplest notation possible, in-line &lt;code&gt;R&lt;/code&gt; code, and a small graphic representing the computation on a data frame or vector. This last piece is super important for me, because I remember best when I associate an image with what I‚Äôve learned.&lt;/p&gt;
&lt;p&gt;This tutorial is not meant to replace the resources I used to learn TMLE, but rather to supplement them. I use the same mathematical notation as the TMLE literature to make it easier to move back and forth. I hope you find the way I think about the algorithm useful, but if not, consider checking out the &lt;a href=&#34;#references&#34;&gt;references&lt;/a&gt; I‚Äôve listed at the end. TMLE is a very powerful method and will undoubtedly only grow in popularity in statistics and data science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-tmle-causal-inference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Is TMLE Causal Inference? ü§î&lt;/h1&gt;
&lt;p&gt;Although TMLE was developed for causal inference due to its many attractive statistical properties (&lt;a href=&#34;#properties-of-tmle-üìà&#34;&gt;discussed later&lt;/a&gt;), it cannot be considered causal inference by itself. Causal inference is a process that first requires &lt;strong&gt;causal assumptions&lt;/strong&gt; (not discussed here) before a statistical estimand can be interpreted causally.&lt;/p&gt;
&lt;p&gt;TMLE can be used to estimate various statistical estimands (odds ratio, risk ratio, mean outcome difference, etc.) even when causal assumptions are not met. TMLE is, as its name implies, simply a tool for estimation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this tutorial I‚Äôll show a basic version of TMLE: estimating the mean difference in outcomes, adjusted for baseline confounders, for a binary outcome and binary treatment. Under causal assumptions, this is the Average Treatment Effect (ATE).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;tmle-step-by-step-Ô∏è&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. TMLE, Step-by-Step üö∂üèΩ‚Äç‚ôÇÔ∏è&lt;/h1&gt;
&lt;p&gt;Let‚Äôs look at the algorithm step-by-step. As you‚Äôre reading, keep in mind that there are &lt;code&gt;R&lt;/code&gt; packages that will do this for you as easily as running a &lt;code&gt;glm()&lt;/code&gt; or &lt;code&gt;coxph()&lt;/code&gt; function. This tutorial is to help understand what‚Äôs going on behind-the-scenes.&lt;/p&gt;
&lt;p&gt;One quick note before we dive into the algorithm:&lt;/p&gt;
&lt;div id=&#34;superlearning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Superlearning&lt;/h3&gt;
&lt;p&gt;I use the ensemble learning method &lt;strong&gt;superlearning&lt;/strong&gt; (also known as ‚Äústacking‚Äù) to demonstrate TMLE. This is because superlearning is theoretically and empirically proven to yield the best results in TMLE.&lt;/p&gt;
&lt;p&gt;For a tutorial on superlearning, you can check out my &lt;a href=&#34;www.khstats.com/sl/superlearning&#34;&gt;previous blog post&lt;/a&gt;. If you‚Äôre new to superlearning/stacking, the necessary knowledge for this post is that it allows you to combine many machine learning algorithms for prediction. When I use &lt;code&gt;SuperLearner()&lt;/code&gt; in the following example code, I could have used &lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;randomForest()&lt;/code&gt;, or any other parametric or non-parametric supervised learning algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;initial-set-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Initial set up&lt;/h3&gt;
&lt;p&gt;Let‚Äôs first load the necessary libraries and set a seed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # for data manipulation
library(kableExtra) # for table printing
library(SuperLearner) # for ensemble learning

set.seed(7) # for reproducible results&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let‚Äôs simulate a data set for demonstration of the algorithm. This data will have a very simple structure: a binary treatment, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, binary outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and four confounders: &lt;span class=&#34;math inline&#34;&gt;\(W_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W_3\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(W_4\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/1_data_structure.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;generate_data &amp;lt;- function(n){ 
    W1 &amp;lt;- rbinom(n, size=1, prob=0.2) # binary confounder
    W2 &amp;lt;- rbinom(n, size=1, prob=0.5) # binary confounder
    W3 &amp;lt;- round(runif(n, min=0, max=5)) # continuous confounder
    W4 &amp;lt;- round(runif(n, min=0, max=4)) # continuous confounder
    A  &amp;lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + 0.1*W3 + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders
    Y &amp;lt;- rbinom(n, size=1, prob= plogis(-1 + A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + 0.1*W2*W4)) # binary outcome depends on confounders
    return(tibble(W1, W2, W3, W4, A, Y))
}

n &amp;lt;- 2000
dat_obs &amp;lt;- generate_data(n) # generate a data set with n observations

kable(head(dat_obs), digits=2, caption = &amp;quot;Simulated data set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Simulated data set.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W3
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W4
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As mentioned earlier, TMLE can estimate many different statistical estimands of interest. In this example, the statistical estimand is the mean difference in outcomes between those who received the treatment and those who did not, adjusting for confounders.&lt;/p&gt;
&lt;p&gt;Under causal assumptions, this could be identifiable as the Average Treatment Effect (ATE). Let‚Äôs pretend for this example that we previously met causal assumptions and call our statistical estimand, &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, the ATE.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ATE = \Psi = E_W[\mathrm{E}[Y|A=1,\mathbf{W}] - \mathrm{E}[Y|A=0,\mathbf{W}]]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;At this point in set-up, we should also pick our statistical learning algorithms to combine when we use the superlearner to estimate the expected outcome and probability of treatment. Let‚Äôs use LASSO (&lt;code&gt;glmnet&lt;/code&gt;), random forests (&lt;code&gt;ranger&lt;/code&gt;), and Multivariate Adaptive Regression Splines (MARS) (&lt;code&gt;earth&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_libs &amp;lt;- c(&amp;#39;SL.glmnet&amp;#39;, &amp;#39;SL.ranger&amp;#39;, &amp;#39;SL.earth&amp;#39;) # a library of machine learning algorithms (penalized regression, random forests, and multivariate adaptive regression splines)&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 1: Estimate the Outcome
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;The very first step of TMLE is to estimate the expected value of the outcome using treatment and confounders as predictors.&lt;/p&gt;
&lt;p&gt;This is what that looks like in mathematical notation. There is some function &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; which takes &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; as inputs and yields the conditional expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(A,\mathbf{W}) = \mathrm{E}[Y|A,\mathbf{W}]\]&lt;/span&gt;
We can use any regression to estimate this conditional expectation, but it is best to use flexible machine learning models so that we don‚Äôt have unnecessary assumptions on the underlying distribution of the data.&lt;/p&gt;
&lt;p&gt;We can think about the above equation as some generic regression function in &lt;code&gt;R&lt;/code&gt; called &lt;code&gt;fit()&lt;/code&gt; with inputs in formula form: &lt;code&gt;outcome ~ predictors&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/2_outcome_fit.png&#34; style=&#34;width:70.0%&#34; /&gt;
In real &lt;code&gt;R&lt;/code&gt; code, we‚Äôll use the &lt;code&gt;SuperLearner()&lt;/code&gt; function to fit a weighted combination of multiple machine learning models (defined earlier in &lt;code&gt;sl_libs&lt;/code&gt;). This function takes the outcome &lt;code&gt;Y&lt;/code&gt; as a vector and a data frame &lt;code&gt;X&lt;/code&gt; as predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Y &amp;lt;- dat_obs$Y
W_A &amp;lt;- dat_obs %&amp;gt;% select(-Y) # remove the outcome to make a matrix of predictors (A, W1, W2, W3, W4) for SuperLearner
Q &amp;lt;- SuperLearner(Y = Y, # Y is the outcome vector
                  X = W_A, # W_A is the matrix of W1, W2, W3, W4, and A
                  family=binomial(), # specify we have a binary outcome
                  SL.library = sl_libs) # specify our superlearner library of LASSO, RF, and MARS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: earth&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: ranger&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we should estimate the outcome for every observation under three different scenarios:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. If every observation received the treatment they &lt;em&gt;actually&lt;/em&gt; received.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can get this expected outcome estimate by simply calling &lt;code&gt;predict()&lt;/code&gt; on the model fit without specifying any new data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(A,\mathbf{W}) = \mathrm{\hat{E}}[Y|A,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will save that vector of estimates as a new object in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/3_QA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_A &amp;lt;- as.vector(predict(Q)$pred) # obtain predictions for everyone using the treatment they actually received&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. If every observation received the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To do this, we‚Äôll first need to create a data set where every observation received the treatment of interest, whether they actually did or not. Then we can call the &lt;code&gt;predict()&lt;/code&gt; function on that data set.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(1,\mathbf{W}) = \mathrm{\hat{E}}[Y|A=1,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/4_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;W_A1 &amp;lt;- W_A %&amp;gt;% mutate(A = 1)  # data set where everyone received treatment
Q_1 &amp;lt;- as.vector(predict(Q, newdata = W_A1)$pred) # predict on that everyone-exposed data set&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. If every observation received the control.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Similarly, we create a data set where every observation did &lt;em&gt;not&lt;/em&gt; receive the treatment of interest, whether they actually did or not, and call the &lt;code&gt;predict()&lt;/code&gt; function again.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(0,\mathbf{W}) = \mathrm{\hat{E}}[Y|A=0,\mathbf{W}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/5_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;W_A0 &amp;lt;- W_A %&amp;gt;% mutate(A = 0) # data set where no one received treatment
Q_0 &amp;lt;- as.vector(predict(Q, newdata = W_A0)$pred)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let‚Äôs create a new data frame, &lt;code&gt;dat_tmle&lt;/code&gt;, to hold the three vectors we‚Äôve created so far, along with the treatment status &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and observed outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Notice that when &lt;span class=&#34;math inline&#34;&gt;\(A=1\)&lt;/span&gt;, the expected outcome &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,\mathbf{W}]\)&lt;/span&gt; equals the expected outcome under treatment &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(A=0\)&lt;/span&gt;, the expected outcome &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,\mathbf{W}]\)&lt;/span&gt; equals the expected outcome under no treatment &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=0,\mathbf{W}]\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_tmle &amp;lt;- tibble(Y = dat_obs$Y, A = dat_obs$A, Q_A, Q_0, Q_1)
kable(head(dat_tmle), digits=2, caption = &amp;quot;TMLE Algorithm after Step 1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-8&#34;&gt;Table 2: &lt;/span&gt;TMLE Algorithm after Step 1
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.67
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.85
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.73
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.46
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.62
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- *Also note that our expected outcomes are on the original outcome scale (i.e. probability, rather than the $logit$ probability).* --&gt;
&lt;p&gt;We could stop here and get our estimate of the ATE by computing the average difference between &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=0,\mathbf{W}]\)&lt;/span&gt;, which &lt;em&gt;would&lt;/em&gt; be the mean difference in the expected outcomes, conditional on confounders. This estimation method is often called &lt;strong&gt;standardization&lt;/strong&gt;, &lt;strong&gt;simple substitution estimation&lt;/strong&gt;, or &lt;strong&gt;G-computation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{ATE}_{G-comp}= \hat{\Psi}_{G-comp} = \sum_{i=1}^{n}(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]-\mathrm{\hat{E}}[Y|A=0,\mathbf{W}])\]&lt;/span&gt;
&lt;img src=&#34;/img/tmle/ate_gcomp.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ate_gcomp &amp;lt;- mean(dat_tmle$Q_1 - dat_tmle$Q_0)
ate_gcomp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2341362&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, because we used machine learning, those expected outcome estimates have the optimal bias-variance trade-off for estimating the outcome, not the ATE. The ATE estimate may be biased, and we cannot compute the standard error. We need to incorporate information about the treatment mechanism to fix this.&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 2: Estimate the Probability of Treatment
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;The next step is to estimate the probability of treatment, given confounders. This quantity is often called the &lt;strong&gt;propensity score&lt;/strong&gt;, as in it gives the &lt;em&gt;propensity&lt;/em&gt; that an observation will receive a treatment of interest.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[g(\mathbf{W}) = \mathrm{Pr}(A=1|\mathbf{W})\]&lt;/span&gt;
&lt;img src=&#34;/img/tmle/6_treatment_fit.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Pr}(A=1|\mathbf{W})\)&lt;/span&gt; in the same way as we estimated &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{E}[Y|A,\mathbf{W}]\)&lt;/span&gt;: using the superlearner algorithm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A &amp;lt;- dat_obs$A
W &amp;lt;- dat_obs %&amp;gt;% select(-Y, -A) # matrix of predictors that only contains the confounders W1, W2, W3, and W4

g &amp;lt;- SuperLearner(Y = A,
                  X = W,
                  family=binomial(),
                  SL.library=sl_libs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we need to compute three different quantities from this model fit:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. The inverse probability of receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(1,\mathbf{W}) = \frac{1}{g(\mathbf{W})} = \frac{1}{\mathrm{Pr}(A=1|\mathbf{W})}\]&lt;/span&gt;
We can estimate the probability of receiving treatment for every observation by using the &lt;code&gt;predict()&lt;/code&gt; funcion without specifying any new data, and then take the inverse of that.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/7_H1.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_w &amp;lt;- as.vector(predict(g)$pred)
H_1 &amp;lt;- 1/g_w&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. The negative inverse probability of not receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(0,\mathbf{W}) = -\frac{1}{1-g(\mathbf{W})}= -\frac{1}{\mathrm{Pr}(A=0|\mathbf{W})}\]&lt;/span&gt;
The probability of not receiving treatment for a binary treatment is simply 1 minus the probability of treatment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/8_H0.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;H_0 &amp;lt;- -1/(1-g_w)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(A,\mathbf{W}) = \frac{\mathrm{I}(A=1)}{\mathrm{Pr}(A=1|\mathbf{W})}-\frac{\mathrm{I}(A=0)}{\mathrm{Pr}(A=0|\mathbf{W})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/9_HA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To calculate this, we‚Äôll first add the &lt;span class=&#34;math inline&#34;&gt;\(H(1,\mathbf{W})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H(0,\mathbf{W})\)&lt;/span&gt; vectors to our &lt;code&gt;dat_tmle&lt;/code&gt; data frame, and then we can use &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to assign &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_tmle &amp;lt;- # add clever covariate data to dat_tmle
  dat_tmle %&amp;gt;%
  bind_cols(
         H_1 = H_1,
         H_0 = H_0) %&amp;gt;%
  mutate(H_A = case_when(A == 1 ~ H_1, # if A is 1 (treated), assign H_1
                       A == 0 ~ H_0))  # if A is 0 (not treated), assign H_0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our initial estimates of the outcome, and the estimates of the probability of treatment:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(head(dat_tmle), digits=2, caption=&amp;quot;TMLE Algorithm after Step 2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-14&#34;&gt;Table 3: &lt;/span&gt;TMLE Algorithm after Step 2
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_A
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.67
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-9.15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-6.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.55
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.55
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.58
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.52
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.28
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Next we will use those vectors containing information about the treatment mechanism ‚Äì &lt;span class=&#34;math inline&#34;&gt;\(H(1,\mathbf{W})\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(H(0,\mathbf{W})\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt; ‚Äì to fix the wrong bias-variance trade-off for the expected outcome fits from Step 1.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Another way to estimate the ATE would be to down-weight outcomes from observations more likely to receive the treatment and up-weight outcomes from observations less likely to receive treatment. This is called &lt;strong&gt;inverse probability weighting&lt;/strong&gt; (IPW).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{ATE}_{IPW} = \hat{\Psi}_{IPW} =\sum_{i=1}^{n}(\frac{AY}{\mathrm{Pr}(A=1|\mathbf{W})}-\frac{(1-A)Y}{\mathrm{Pr}(A=0|\mathbf{W})})\]&lt;/span&gt;
&lt;img src=&#34;/img/tmle/ate_ipw.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ate_ipw &amp;lt;- mean(dat_tmle$Y * dat_tmle$H_A)
ate_ipw&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2474638&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, once we use machine learning to estimate the probability of treatment, we generally cannot get standard errors for our IPW estimate. We could bootstrap, but the standard errors will only be correct if our estimand is asymptotically normally distributed and they are unlikely to be efficient (more on that later).&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 3: Estimate the Fluctuation Parameter
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;To reiterate, in Step 1 we estimated the expected outcome, conditional on treatment and confounders. We know those machine learning fits have an optimal bias-variance trade-off for estimating the outcome (conditional on treatment and confounders), rather than the ATE. We will now use information about the treatment mechanism (from Step 2) to optimize the bias-variance trade-off for the ATE so we can obtain valid inference.&lt;/p&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;/img/bear_with_me.jpg&#34;
               style=&#34;float:right; padding-left:65px;&#34; width=50%&#34;&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;üö® Warning: this step is easy to code, but difficult to understand unless you have a background in semiparametric theory. Since this post is focused on learning the TMLE algorithm, I‚Äôll give only a very brief high-level explanation, and then you can look at references if you‚Äôre curious to learn more about the theory. Importantly, the explanation I offer for this step is more than sufficient to appropriately implement TMLE as an analyst.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The point of this step is to set up an estimating equation for the &lt;em&gt;efficient influence function&lt;/em&gt; of our estimand of interest.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An &lt;strong&gt;influence function&lt;/strong&gt; is a function that quantifies how much influence each observation has on the estimator. For this reason, it is very useful to characterize the variance of the estimator. Some estimands have an &lt;strong&gt;efficient influence function&lt;/strong&gt; (EIF), which means there exists an influence function that achieves the Cramer Rao Lower Bound and can be used to create efficient estimators. (Informally, an efficient estimator means it achieves the smallest possible variance for a fixed number of observations.)&lt;/p&gt;
&lt;p&gt;If we were to work through a mathematical proof, we could show the ATE has an EIF, and that there is an equation we can set up to &lt;em&gt;target&lt;/em&gt; it. &lt;strong&gt;Setting up this estimating equation for the EIF will allow us to do two things:&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Update our initial outcome estimates so that our estimate of the ATE is asymptotically unbiased (under certain conditions, see the &lt;a href=&#34;#7-properties-of-tmle&#34;&gt;statistical properties section&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculate the variance, and thus the standard error, confidence interval, and p-value for our estimate of the ATE.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;This is the reason TMLE allows us to use machine learning models ‚Äúunder the hood‚Äù while still obtaining asymptotic properties for inference: our estimand of interest has an EIF, and we‚Äôve built an algorithm, or estimator, to utilize its beneficial properties.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let‚Äôs take a look at that estimating equation that will help us get to the EIF:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[logit(\mathrm{E}[Y|A,\mathbf{W}]) = logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}]) + \epsilon H(A,\mathbf{W})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we look at the left side, we can see it contains the true outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, just &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; transformed. Luckily for us, there‚Äôs a well-known model that &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; transforms the left side of an equation: logistic regression. Our estimating equation looks &lt;em&gt;a lot&lt;/em&gt; like a simple logistic regression, actually: &lt;span class=&#34;math inline&#34;&gt;\(logit(\mathrm{E}[Y|X]) = \beta_0 + \beta_1 X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Do you see how our estimating equation &lt;em&gt;also&lt;/em&gt; has a vector on the right-hand side, &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt;, with a corresponding coefficient, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;? The only difference is that the ‚Äúintercept‚Äù in our estimating equation, &lt;span class=&#34;math inline&#34;&gt;\(logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}])\)&lt;/span&gt; is not a constant value like &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;; it is a vector of values. We can see it as an &lt;strong&gt;offset&lt;/strong&gt; or a &lt;strong&gt;fixed intercept&lt;/strong&gt; in a logistic regression, rather than a constant-value intercept.&lt;/p&gt;
&lt;p&gt;Therefore, to solve our estimating equation we can leverage standard statistical software and fit a logistic regression with one covariate, &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt;, and the initial outcome estimate, &lt;span class=&#34;math inline&#34;&gt;\(logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}])\)&lt;/span&gt;, as a fixed intercept. The outcome of the logistic regression is the observed outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Two technical points for application: we use &lt;code&gt;qlogis&lt;/code&gt; to transform the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,\mathbf{W}]\)&lt;/span&gt; to the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; scale. Also, the &lt;code&gt;R&lt;/code&gt; code for a fixed intercept is &lt;code&gt;-1 + offset(fixed_intercept)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/10_logistic_regression.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm_fit &amp;lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle, family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note that we are only using a logistic regression because it happens to have the correct form for solving the estimating equation for the EIF for the ATE estimand. It has nothing to do with having a binary outcome, and it isn‚Äôt putting any parametric restraints on our data.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Next we need to save the coefficient from that logistic regression, which we will call &lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/11_epsilon.png&#34; style=&#34;width:40.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eps &amp;lt;- coef(glm_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the TMLE literature, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is called the &lt;strong&gt;fluctuation parameter&lt;/strong&gt;, because it provides information about how much to change, or fluctuate, our initial outcome estimates. Similarly, &lt;span class=&#34;math inline&#34;&gt;\(H(A,\mathbf{W})\)&lt;/span&gt; is called the &lt;strong&gt;clever covariate&lt;/strong&gt; because it ‚Äúcleverly‚Äù helps us solve for the EIF and then update our estimates.&lt;/p&gt;
&lt;p&gt;We will use both the fluctuation parameter and clever covariate in the next step to update our initial estimates of the expected outcome, conditional on confounders and treatment.&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 4: Update the Initial Estimates of the Expected Outcome
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;To update our expected outcome estimates, we first need to put the initial expected outcome estimates on the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; scale using &lt;code&gt;qlogis()&lt;/code&gt; because that‚Äôs the scale we used to solve the fluctuation parameter in Step 3. Then we can add our update using the fluctuation parameter and clever covariate: &lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon} \times H(A,\mathbf{W})\)&lt;/span&gt;. Finally, we can put the updated estimates back on the true outcome scale using &lt;code&gt;plogis()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note we can use &lt;span class=&#34;math inline&#34;&gt;\(expit\)&lt;/span&gt; to show the inverse of the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; function, and we will denote updates to the outcome regressions as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathrm{E}}^*\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathrm{E}}\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A,\mathbf{W}]) + \hat{\epsilon}H(A,\mathbf{W}))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/update_qAW.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;H_A &amp;lt;- dat_tmle$H_A # for cleaner code in Q_A_update
Q_A_update &amp;lt;- plogis(qlogis(Q_A) + eps*H_A)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A=1,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A=1,\mathbf{W}]) + \hat{\epsilon}H(A,1))\]&lt;/span&gt;
&lt;img src=&#34;/img/tmle/12_update_Q1.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_1_update &amp;lt;- plogis(qlogis(Q_1) + eps*H_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A=0,\mathbf{W}] = expit(logit(\mathrm{\hat{E}}[Y|A=0,\mathbf{W}]) + \hat{\epsilon}H(A,0))\]&lt;/span&gt;
&lt;img src=&#34;/img/tmle/13_update_Q0.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_0_update &amp;lt;- plogis(qlogis(Q_0) + eps*H_0)&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 5: Compute the Statistical Estimand of Interest
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;We now have updated expected outcomes estimates, so we can compute the ATE as the mean difference in the updated outcome estimates under treatment and no treatment:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{ATE}_{TMLE} = \hat{\Psi}_{TMLE} = \sum_{i=1}^{n}[\hat{E^*}[Y|A=1,\mathbf{W}] - \hat{E^*}[Y|A=0,\mathbf{W}]]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/14_compute_ATE.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmle_ate &amp;lt;- mean(Q_1_update - Q_0_update)
tmle_ate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2403501&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then say, ‚Äúthe average treatment effect was estimated to be 24%.‚Äù&lt;/p&gt;
&lt;p&gt;If causal assumptions were not met, we would say, ‚Äúthe proportion of observations who experienced the outcome, after adjusting for baseline confounders, was estimated to be 24% higher for those who received treatment compared to those who did not.‚Äù&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 6: Calculate the Standard Errors, Confidence Intervals, and P-values
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;This point estimate is great, but usually standard errors are needed to compute confidence intervals and test statistics. To obtain the standard errors, we first need to compute the &lt;strong&gt;Influence Curve&lt;/strong&gt; (IC), which is the empirical version of what we used our estimating equation to figure out in step 3. The IC tells us how much each observation influences the final estimate.&lt;/p&gt;
&lt;p&gt;The equation for the IC looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{IC} = (Y-\hat{E^*}[Y|A,\mathbf{W}])H(A,\mathbf{W}) + \hat{E^*}[Y|A=1,\mathbf{W}] - \hat{E^*}[Y|A=0,\mathbf{W}] - \hat{ATE}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ic &amp;lt;- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the IC, we can rely on the delta method by taking the square-root of its variance divided by the number of observations to get the standard error of our estimate.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: This is another step which relies on semiparametric theory so I won‚Äôt get into the details; however, I‚Äôve linked resources in &lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{SE} = \sqrt{\frac{var(\hat{IC})}{N}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/15_ses.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmle_se &amp;lt;- sqrt(var(ic)/nrow(dat_obs))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have that standard error, we can easily get the 95% confidence interval and p-value of our estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_low &amp;lt;- tmle_ate - 1.96*tmle_se
conf_high &amp;lt;- tmle_ate + 1.96*tmle_se
pval &amp;lt;- 2 * (1 - pnorm(abs(tmle_ate / tmle_se)))

kable(tibble(tmle_ate, conf_low, conf_high), digits=3, caption = &amp;quot;TMLE Estimate of the ATE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-24&#34;&gt;Table 4: &lt;/span&gt;TMLE Estimate of the ATE
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
tmle_ate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
conf_low
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
conf_high
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.194
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.287
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Then we can successfully report our ATE as 0.24 (95% CI: 0.194, 0.287).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: A TMLE estimate is asymptotically normal, so we could bootstrap the entire algorithm to get our standard errors instead.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-tmle-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6 Using the &lt;code&gt;tmle&lt;/code&gt; package&lt;/h1&gt;
&lt;p&gt;Luckily there are &lt;code&gt;R&lt;/code&gt; packages so that you don‚Äôt have to hand code TMLE yourself. &lt;code&gt;R&lt;/code&gt; packages to implement the TMLE algorithm include &lt;a href=&#34;https://www.jstatsoft.org/article/view/v051i13&#34;&gt;&lt;code&gt;tmle&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://tlverse.org/tlverse-handbook/tmle3.html&#34;&gt;&lt;code&gt;tmle3&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.jstatsoft.org/article/view/v081i01&#34;&gt;&lt;code&gt;ltmle&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://htmlpreview.github.io/?https://gist.githubusercontent.com/nt-williams/ddd44c48390b8d976fad71750e48d8bf/raw/45db700a02bf92e2a55790e60ed48266a97ca4e7/intro-lmtp.html&#34;&gt;&lt;code&gt;lmtp&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code using the original &lt;code&gt;tmle&lt;/code&gt; package‚Äôs &lt;code&gt;tmle()&lt;/code&gt; function is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmle_fit &amp;lt;-
  tmle::tmle(Y = Y, # outcome vector
           A = A, # treatment vector
           W = W, # matrix of confounders W1, W2, W3, W4
           Q.SL.library = sl_libs, # superlearning libraries from earlier for outcome regression Q(A,W)
           g.SL.library = sl_libs) # superlearning libraries from earlier for treatment regression g(W)

tmle_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Additive Effect
##    Parameter Estimate:  0.23733
##    Estimated Variance:  0.00053649
##               p-value:  &amp;lt;2e-16
##     95% Conf Interval: (0.19194, 0.28273) 
## 
##  Additive Effect among the Treated
##    Parameter Estimate:  0.23705
##    Estimated Variance:  0.0005385
##               p-value:  &amp;lt;2e-16
##     95% Conf Interval: (0.19157, 0.28253) 
## 
##  Additive Effect among the Controls
##    Parameter Estimate:  0.23867
##    Estimated Variance:  0.00053584
##               p-value:  &amp;lt;2e-16
##     95% Conf Interval: (0.1933, 0.28404)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the same result (variation due to randomness in the machine learning models) in just a few lines of code: the estimate using the original &lt;code&gt;tmle&lt;/code&gt; package is 0.237 (95% CI: 0.192, 0.283).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;properties-of-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;7 Properties of TMLE üìà&lt;/h1&gt;
&lt;p&gt;TMLE is a &lt;strong&gt;doubly robust&lt;/strong&gt; estimator, which means that if either the regression to estimate the expected outcome, or the regression to estimate the probability of treatment, are correctly specified (formally, their bias goes to zero as sample size grows large, meaning they are &lt;strong&gt;consistent&lt;/strong&gt;), the final TMLE estimate will be consistent.&lt;/p&gt;
&lt;p&gt;If both regressions are consistent, the final estimate will reach the smallest possible variance in a fixed number of observations (formally: it will be &lt;strong&gt;efficient&lt;/strong&gt;). The reason we use superlearning for estimating the outcome and treatment regression fits is to give us the best possible chance of having correctly specified models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle_props.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Besides allowing for the use of machine learning models which do not assume an underlying distribution of the data, TMLE is appealing because its estimates will always stay in the bounds of the original outcome. This is because it is part of a class of &lt;strong&gt;substitution estimators&lt;/strong&gt;. There is another class of semiparametric estimation methods frequently used in causal inference that are referred to as &lt;strong&gt;one-step estimators&lt;/strong&gt;, but they can sometimes yield final estimates that are outside the original outcome scale.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;If you‚Äôd like to learn more, I recommend the following resources:&lt;/p&gt;
&lt;div id=&#34;tmle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TMLE&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The paper I referred to most often while learning TMLE was &lt;a href=&#34;https://academic.oup.com/aje/article/185/1/65/2662306&#34;&gt;&lt;em&gt;Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies&lt;/em&gt;&lt;/a&gt; by Megan S. Schuler and Sherri Rose. It has a nice step-by-step written explanation and details the statistical advantages of TMLE for an applied thinker.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I also really like the written explanations in the &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4419-9782-1&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt; book by Mark van der Laan and Sherri Rose. The notation was too difficult for me to follow, but the words themselves make a lot of sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Miguel Luque wrote an &lt;a href=&#34;https://migariane.github.io/TMLE.nb.html&#34;&gt;excellent bookdown tutorial on TMLE&lt;/a&gt;, also with step-by-step &lt;code&gt;R&lt;/code&gt; code. It is more technical and thorough than my post, but still aimed at an applied audience. He also has a tutorial on the &lt;a href=&#34;https://migariane.github.io/DeltaMethodEpiTutorial.nb.html&#34;&gt;functional delta method&lt;/a&gt; which is part of the theory behind Step 6.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;semiparametric-theory&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Semiparametric Theory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Edward Kennedy has several well-written pieces on semiparametric estimation in causal inference. I recommend starting with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;His introductory paper on &lt;a href=&#34;https://arxiv.org/pdf/1709.06418.pdf&#34;&gt;Semiparametric Theory&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;His &lt;a href=&#34;http://www.ehkennedy.com/uploads/5/8/4/5/58450265/unc_2019_cirg.pdf&#34;&gt;slideshow tutorial&lt;/a&gt; &lt;em&gt;Nonparametric efficiency theory and machine learning in causal inference&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;influence-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Influence Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;My favorite resource so far for learning specifically about influence functions has been &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/00031305.2020.1717620&#34;&gt;Visually Communicating Influence Functions&lt;/a&gt; by Aaron Fisher and Edward Kennedy. However, this paper didn‚Äôt make sense to me until I worked through this &lt;a href=&#34;https://observablehq.com/@herbps10/one-step-estimators-and-pathwise-derivatives&#34;&gt;interactive tutorial&lt;/a&gt; by Herb Susmann. I suggest playing around with the interactive examples first, and then trying to work through the paper.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The derivation of the Efficient Influence Function (EIF) in TMLE is in the Appendix of &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4419-9782-1&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Causal Inference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you want to learn more about the foundations of causal inference, I suggest &lt;a href=&#34;http://bayes.cs.ucla.edu/PRIMER/&#34;&gt;&lt;em&gt;Causal Inference in Statistics: A Primer&lt;/em&gt;&lt;/a&gt; by Judea Pearl and &lt;a href=&#34;https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/&#34;&gt;&lt;em&gt;What If&lt;/em&gt;&lt;/a&gt; (Part I) by Miguel Hernan and James Robins. These are both good starters for learning about the &lt;em&gt;identification&lt;/em&gt; side of causal inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I also think the introductory chapters of the previously mentioned &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4419-9782-1&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt; book do an excellent job of setting up the ‚Äúroadmap‚Äù of causal inference.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I‚Äôll continue to update this page with resources as I discover them.&lt;/p&gt;
&lt;p&gt;Feedback or clarifications on this post is welcome, either from the new learners of TMLE or experts in causal inference. The best way to reach me is through &lt;a href=&#34;mailto:kathoffman.stats@gmail.com&#34;&gt;email&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Acknowledgements&lt;/h1&gt;
&lt;p&gt;A huge thank you to Iv√°n D√≠az for patiently answering many, many questions on TMLE. I am also very appreciative of Miguel Luque, Nick Williams, Axel Martin, and Anjile An for their very helpful feedback on this tutorial.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tutorial-code-and-session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tutorial code and session info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # for data manipulation
library(kableExtra) # for table printing
library(SuperLearner) # for ensemble learning

set.seed(7) # for reproducible results

sl_libs &amp;lt;- c(&amp;#39;SL.glmnet&amp;#39;, &amp;#39;SL.ranger&amp;#39;, &amp;#39;SL.earth&amp;#39;) # a library of machine learning algorithms (penalized regression, random forests, and multivariate adaptive regression splines)

generate_data &amp;lt;- function(n){ 
    W1 &amp;lt;- rbinom(n, size=1, prob=0.2) # binary confounder
    W2 &amp;lt;- rbinom(n, size=1, prob=0.5) # binary confounder
    W3 &amp;lt;- round(runif(n, min=0, max=5)) # continuous confounder
    W4 &amp;lt;- round(runif(n, min=0, max=4)) # continuous confounder
    A  &amp;lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + 0.1*W3 + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders
    Y &amp;lt;- rbinom(n, size=1, prob= plogis(-1 + A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + 0.1*W2*W4)) # binary outcome depends on confounders
    return(tibble(W1, W2, W3, W4, A, Y))
}

n &amp;lt;- 2000
dat_obs &amp;lt;- generate_data(n) # generate a data set with n observations

Y &amp;lt;- dat_obs$Y
W_A &amp;lt;- dat_obs %&amp;gt;% select(-Y) # remove the outcome to make a matrix of predictors (A, W1, W2, W3, W4) for SuperLearner

### Step 1: Estimate Q
Q &amp;lt;- SuperLearner(Y = Y, # Y is the outcome vector
                  X = W_A, # W_A is the matrix of W1, W2, W3, W4, and A
                  family=binomial(), # specify we have a binary outcome
                  SL.library = sl_libs) # specify our superlearner library of LASSO, RF, and MARS
Q_A &amp;lt;- as.vector(predict(Q)$pred) # obtain predictions for everyone using the treatment they actually received
W_A1 &amp;lt;- W_A %&amp;gt;% mutate(A = 1)  # data set where everyone received treatment
Q_1 &amp;lt;- as.vector(predict(Q, newdata = W_A1)$pred) # predict on that everyone-exposed data set
W_A0 &amp;lt;- W_A %&amp;gt;% mutate(A = 0) # data set where no one received treatment
Q_0 &amp;lt;- as.vector(predict(Q, newdata = W_A0)$pred)
dat_tmle &amp;lt;- tibble(Y = dat_obs$Y, A = dat_obs$A, Q_A, Q_0, Q_1)

### Step 2: Estimate g and compute H(A,W)
A &amp;lt;- dat_obs$A
W &amp;lt;- dat_obs %&amp;gt;% select(-Y, -A) # matrix of predictors that only contains the confounders W1, W2, W3, and W4
g &amp;lt;- SuperLearner(Y = A,
                  X = W,
                  family=binomial(),
                  SL.library=sl_libs)
g_w &amp;lt;- as.vector(predict(g)$pred)
H_1 &amp;lt;- 1/g_w
H_0 &amp;lt;- -1/(1-g_w)
dat_tmle &amp;lt;- # add clever covariate data to dat_tmle
  dat_tmle %&amp;gt;%
  bind_cols(
         H_1 = H_1,
         H_0 = H_0) %&amp;gt;%
  mutate(H_A = case_when(A == 1 ~ H_1, # if A is 1 (treated), assign H_1
                       A == 0 ~ H_0))  # if A is 0 (not treated), assign H_0

### Step 3: Estimate fluctuation parameter
glm_fit &amp;lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle, family=binomial)
eps &amp;lt;- coef(glm_fit)

### Step 4: Update Q&amp;#39;s
H_A &amp;lt;- dat_tmle$H_A # for cleaner code in Q_A_update
Q_A_update &amp;lt;- plogis(qlogis(Q_A) + eps*H_A)
Q_1_update &amp;lt;- plogis(qlogis(Q_1) + eps*H_1)
Q_0_update &amp;lt;- plogis(qlogis(Q_0) + eps*H_0)

### Step 5: Compute ATE
tmle_ate &amp;lt;- mean(Q_1_update - Q_0_update)

### Step 6: compute standard error, CIs and pvals
ic &amp;lt;- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate
tmle_se &amp;lt;- sqrt(var(ic)/nrow(dat_obs))
conf_low &amp;lt;- tmle_ate - 1.96*tmle_se
conf_high &amp;lt;- tmle_ate + 1.96*tmle_se
pval &amp;lt;- 2 * (1 - pnorm(abs(tmle_ate / tmle_se)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] SuperLearner_2.0-26 nnls_1.4            kableExtra_1.1.0   
##  [4] forcats_0.5.0       stringr_1.4.0       dplyr_1.0.2        
##  [7] purrr_0.3.4         readr_1.3.1         tidyr_1.1.2        
## [10] tibble_3.0.4        ggplot2_3.3.2       tidyverse_1.3.0    
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.5         lubridate_1.7.9    lattice_0.20-38    plotmo_3.5.7      
##  [5] earth_5.1.2        assertthat_0.2.1   glmnet_3.0-2       digest_0.6.27     
##  [9] foreach_1.5.0      ranger_0.12.1      R6_2.5.0           cellranger_1.1.0  
## [13] backports_1.1.8    reprex_0.3.0       evaluate_0.14      httr_1.4.2        
## [17] highr_0.8          blogdown_0.19      pillar_1.4.6       TeachingDemos_2.12
## [21] rlang_0.4.8        readxl_1.3.1       rstudioapi_0.11    Matrix_1.2-18     
## [25] rmarkdown_2.1      webshot_0.5.2      munsell_0.5.0      broom_0.7.0       
## [29] compiler_3.6.3     modelr_0.1.6       xfun_0.19          pkgconfig_2.0.3   
## [33] shape_1.4.4        htmltools_0.4.0    tidyselect_1.1.0   bookdown_0.19     
## [37] codetools_0.2-16   tmle_1.4.0.1       fansi_0.4.1        viridisLite_0.3.0 
## [41] crayon_1.3.4       dbplyr_1.4.3       withr_2.2.0        cabinets_0.6.0    
## [45] grid_3.6.3         jsonlite_1.7.1     gtable_0.3.0       lifecycle_0.2.0   
## [49] DBI_1.1.0          magrittr_1.5       scales_1.1.1       cli_2.1.0         
## [53] stringi_1.5.3      fs_1.4.1           xml2_1.3.0         ellipsis_0.3.1    
## [57] generics_0.1.0     vctrs_0.3.4        Formula_1.2-3      iterators_1.0.12  
## [61] tools_3.6.3        glue_1.4.2         hms_0.5.3          plotrix_3.7-7     
## [65] yaml_2.2.1         colorspace_1.4-1   rvest_0.3.5        knitr_1.30        
## [69] haven_2.2.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Whatcha measuring?? Collider Bias Examples in R</title>
      <link>/blog/collider-bias/race/</link>
      <pubDate>Tue, 13 Oct 2020 21:13:14 -0500</pubDate>
      <guid>/blog/collider-bias/race/</guid>
      <description>


&lt;p&gt;Anyways, I wholeheartedly agree with this. I find many of the tools in causal inference (specifically Directed Acyclic Graphs) to be incredibly useful in recognizing what I &lt;em&gt;cannot&lt;/em&gt; unbiasedly estimate.&lt;/p&gt;
&lt;p&gt;I am an expert lurker on the statistics twitter (war) threads is a favorite pasttime of mine, so I was admittedly a bit sad it didn‚Äôt gain more steam.&lt;/p&gt;
&lt;p&gt;So now your investigator wants to know the attributable mortality of race once someone is hospitalized. ‚ÄúWe don‚Äôt want your causal inference, we just want the &lt;em&gt;independent association&lt;/em&gt;!‚Äù they tell you emphatically. ‚ÄúWe &lt;em&gt;PROMISE&lt;/em&gt; we won‚Äôt talk about anything causally!‚Äù You think about Miguel Hernan‚Äôs &lt;em&gt;The C-Word&lt;/em&gt; paper. You don‚Äôt believe them at all, but you let it go, for a moment.&lt;/p&gt;
&lt;p&gt;Let‚Äôs start with simulating race. Let‚Äôs say our population of interest is approximately 50% of one race and 50% of another. We‚Äôll simulate that with a binomial distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;race &amp;lt;- rbinom(n, 1, 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we‚Äôll simulate COVID-19 disease severity. Although there is some evidence suggesting COVID-19 disease severity is not random (i.e.¬†it could be related to viral load, blood type, etc.), let‚Äôs assume for the sake of example that it is completely random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disease_severity &amp;lt;- rbinom(n, 1, 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let‚Äôs think about a common adjustment scientists use to reduce confounding of race: socioeconomic status (SES). SES is unarguably affected by race. Again, for simplicity, let‚Äôs pretend SES is a binary outcome. It could represent if a person with COVID-19 lives ‚Äúabove‚Äù or ‚Äúbelow‚Äù the poverty line. We‚Äôll simulate SES as a binomial distribution where the probability someone will be above the poverty line is affected by their race.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ses &amp;lt;- rbinom(n, 1, plogis(-race))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We‚Äôve made our population, so now let‚Äôs figure out whether we‚Äôre actually going to get to see each person‚Äôs data. We only have access to COVID-19 patients who were hospitalized. Whether or not someone is hospitalized is affected by their disease severity, of course.&lt;/p&gt;
&lt;p&gt;However, let‚Äôs say for our example that hospitalization status is also affected by a person‚Äôs SES (whether or not they have insurance may affect their decision to go to the hospital, for example). It also may be affected by some part of the social construct that we use to define race/ethnicity. If individuals in one race/ethnicity are more or less likely to go to the hospital than individuals of another race/ethnicity, then that will affect hospitalization. This is very reasonable in COVID-19 (and in other diseases) where Black and Hispanic persons are disproportionately dying. If a person knows a family member or friend who is on a ‚Ä¶&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hospitalized &amp;lt;- rbinom(n, 1, plogis(-3 + 2 * disease_severity + 2 * ses + 3 * race))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we simulate our outcome. Let‚Äôs keep it simple and say whether or not someone dies is directly affected by disease severity and whether or not the person is cared for in a hospital:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;death &amp;lt;- rbinom(n, 1, plogis(-(2 * disease_severity + hospitalized)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let‚Äôs put this simulated data into a data frame. I‚Äôm going to relabel the data with interpretable to make it clearer how harmful doing these analyses can be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_df &amp;lt;-
  data.frame(
  race = factor(race, levels=c(1,0), labels=c(&amp;quot;White&amp;quot;,&amp;quot;Black&amp;quot;)),
  disease_severity = factor(disease_severity, levels=c(1,0), labels=c(&amp;quot;Mild&amp;quot;,&amp;quot;Severe&amp;quot;)),
  ses = factor(ses, levels=c(0,1), labels=c(&amp;quot;Above poverty line&amp;quot;, &amp;quot;Below poverty line&amp;quot;)),
  hospitalized = factor(hospitalized, levels=0:1, labels=c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;)),
  death = factor(death, levels=0:1, labels=c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I‚Äôll make a data frame that contains only the patients who were put in the hospital.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hospitalized_df &amp;lt;-
  covid_df %&amp;gt;%
  filter(hospitalized == &amp;quot;Yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The association between race and mortality for people who are hospitalized. Can you get the true association?&lt;/p&gt;
&lt;p&gt;Let‚Äôs pretend more a moment that you think you can. You‚Äôre a pro at &lt;code&gt;ggplot2&lt;/code&gt; so you whip out a nice bar chart. Look, race and mortality in hospitalized patients! It‚Äôs Figure 1 of a paper if I‚Äôve ever seen one!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;race_death_count &amp;lt;-
  hospitalized_df %&amp;gt;%
  group_by(race) %&amp;gt;%
  count(death) %&amp;gt;%
  mutate(sum_n = sum(n)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(prop = n/sum_n)

race_death_count %&amp;gt;%
  filter(death == &amp;quot;Yes&amp;quot;) %&amp;gt;%
  ggplot(aes(x=race, y=prop)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  theme_classic() +
  labs(x=&amp;quot;Race&amp;quot;,y=&amp;quot;In-hospital Mortality&amp;quot;,title=&amp;quot;Race and Mortality in COVID-19&amp;quot;,subtitle=&amp;quot;Among Hospitalized Patients&amp;quot;) +
  scale_y_continuous(labels = scales::percent_format(), expand=c(0,0), limits=c(0,.25)) +
  geom_text(aes(label=paste(n, &amp;quot;/&amp;quot;, sum_n), y=prop+.01))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/collider-bias/race_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pretty_logistic_table &amp;lt;- function(model_fit) {
  model_fit %&amp;gt;%
    broom::tidy(exponentiate=T, conf.int=T) %&amp;gt;%
    filter(term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;%
     mutate( term = case_when(term == &amp;quot;raceBlack&amp;quot; ~ &amp;quot;Race: Black&amp;quot;,
                             term == &amp;quot;disease_severitySevere&amp;quot; ~ &amp;quot;Disease severity: Severe&amp;quot;,
                             term == &amp;quot;sesBelow poverty line&amp;quot; ~ &amp;quot;SES: Below poverty level&amp;quot;,
                             TRUE ~ term),
           odds_ratio = paste0(round(estimate,1),&amp;quot; (&amp;quot;,round(conf.low,1),&amp;quot;, &amp;quot;,round(conf.high,1),&amp;quot;)&amp;quot;),
           p_value = case_when(p.value &amp;lt; .001 ~ &amp;quot;&amp;lt;.001&amp;quot;,
                               TRUE ~ as.character(round(p.value, 3)))) %&amp;gt;%
             select(term, odds_ratio, p_value) %&amp;gt;%
    gt::gt() %&amp;gt;%
    gt::cols_label(
       term = &amp;quot;Coefficient&amp;quot;,
               odds_ratio = &amp;quot;Odds Ratio (95% CI)&amp;quot;,
               p_value = &amp;quot;P-value&amp;quot;
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;start by just looking at the unadjusted Odds Ratio of &lt;strong&gt;race&lt;/strong&gt; on &lt;strong&gt;mortality&lt;/strong&gt;. Simple enough&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race, data = hospitalized_df, family = binomial()) %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#fefxepmids .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#fefxepmids .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#fefxepmids .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#fefxepmids .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fefxepmids .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#fefxepmids .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#fefxepmids .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#fefxepmids .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#fefxepmids .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#fefxepmids .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#fefxepmids .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#fefxepmids .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#fefxepmids .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#fefxepmids .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#fefxepmids .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#fefxepmids .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#fefxepmids .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#fefxepmids .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#fefxepmids .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#fefxepmids .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#fefxepmids .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fefxepmids .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#fefxepmids .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#fefxepmids .gt_left {
  text-align: left;
}

#fefxepmids .gt_center {
  text-align: center;
}

#fefxepmids .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#fefxepmids .gt_font_normal {
  font-weight: normal;
}

#fefxepmids .gt_font_bold {
  font-weight: bold;
}

#fefxepmids .gt_font_italic {
  font-style: italic;
}

#fefxepmids .gt_super {
  font-size: 65%;
}

#fefxepmids .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;fefxepmids&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.7 (0.5, 0.9)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.002&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;‚Äúwe‚Äôre not interested in this,‚Äù
but what are you interested in, then? what meaning does it hold to conclude that
perhaps you‚Äôre going to say that providers don‚Äôt have any additional effect on patients; there‚Äôs no inherent racism. but is that actually what you can conclude? all your estimates are now biased&lt;/p&gt;
&lt;p&gt;statistician drawing dag
I. don‚Äôt. want. causation.
‚Äúokay, great!‚Äù
I just want to show that within my hospitalized patients
&lt;em&gt;whiteboard, draws H&lt;/em&gt;
there is an effect of race
&lt;em&gt;draws race&lt;/em&gt;
on mortality
&lt;em&gt;draws mortality&lt;/em&gt;
independent of confounders
&lt;em&gt;draws confounders&lt;/em&gt;
WITHIN A HOSPITALIZED POPULATION
&lt;em&gt;draws box&lt;/em&gt;
Yep, still can‚Äôt answer that. could be correlated, could not be
THAT‚ÄôSWHAT I WANT TO WRITE ABOUT
but what are you going to SAY about that?!?!
&#34;This estimate of correlation between race and mortality in a hospitalized population ‚Äì which could be completely incorrect, either in the null or wrong direction ‚Äì is evidence of‚Ä¶ What??? The point of research is to ask a question you can set up an experiment to answer, and then to answer it as best you can, not ask a question you &lt;em&gt;know&lt;/em&gt; you cannot answer and then conjecture about the number you obtain.&lt;/p&gt;
&lt;p&gt;but everyone knows Black people are set up for failure in the healthcare system, this will only be accumulating evidence
but what if it‚Äôs not right, what if you find being black is protective, or being Black has no effect on mortality. Any of these are possible!&lt;/p&gt;
&lt;p&gt;There‚Äôs no way there won‚Äôt be an effect of race on mortality among hospitalized patients. Race ‚Ä¶&lt;/p&gt;
&lt;p&gt;So why are you doing this study?&lt;/p&gt;
&lt;p&gt;So what do you suggest I do?&lt;/p&gt;
&lt;p&gt;I suggest you leave this research question to the people who actually have the population level data on race, hospitalization rates, and mortality, to answer the effect of race on mortality. Or, I suggest you go find the people who have that data and try to team up with them. I‚Äôm not saying your question is not important, I‚Äôm saying you don‚Äôt have the data to correctly answer it, and whatever estimate you come up with will be incorrect, and could do more harm than good.&lt;/p&gt;
&lt;p&gt;Headlines: RACE IS A RISK FACTOR FOR DYING FROM COVID-19 IN HOSPITALIZED PATIENTS&lt;/p&gt;
&lt;p&gt;Fitting a model among the entire population and hospitalized patients only:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race + disease_severity + ses, data=covid_df, family = binomial()) %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#eexirarxtj .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#eexirarxtj .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#eexirarxtj .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#eexirarxtj .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#eexirarxtj .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#eexirarxtj .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#eexirarxtj .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#eexirarxtj .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#eexirarxtj .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#eexirarxtj .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#eexirarxtj .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#eexirarxtj .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#eexirarxtj .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#eexirarxtj .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#eexirarxtj .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#eexirarxtj .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#eexirarxtj .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#eexirarxtj .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#eexirarxtj .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#eexirarxtj .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#eexirarxtj .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#eexirarxtj .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#eexirarxtj .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#eexirarxtj .gt_left {
  text-align: left;
}

#eexirarxtj .gt_center {
  text-align: center;
}

#eexirarxtj .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#eexirarxtj .gt_font_normal {
  font-weight: normal;
}

#eexirarxtj .gt_font_bold {
  font-weight: bold;
}

#eexirarxtj .gt_font_italic {
  font-style: italic;
}

#eexirarxtj .gt_super {
  font-size: 65%;
}

#eexirarxtj .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;eexirarxtj&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;2 (1.7, 2.3)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Disease severity: Severe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;11.7 (9.8, 14)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;SES: Below poverty level&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.7 (0.6, 0.9)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race + disease_severity + ses, data=hospitalized_df, family = binomial())  %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#wfcbehcipe .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#wfcbehcipe .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#wfcbehcipe .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#wfcbehcipe .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wfcbehcipe .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#wfcbehcipe .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#wfcbehcipe .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#wfcbehcipe .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#wfcbehcipe .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#wfcbehcipe .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#wfcbehcipe .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#wfcbehcipe .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#wfcbehcipe .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#wfcbehcipe .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#wfcbehcipe .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#wfcbehcipe .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#wfcbehcipe .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#wfcbehcipe .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#wfcbehcipe .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#wfcbehcipe .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#wfcbehcipe .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wfcbehcipe .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#wfcbehcipe .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#wfcbehcipe .gt_left {
  text-align: left;
}

#wfcbehcipe .gt_center {
  text-align: center;
}

#wfcbehcipe .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#wfcbehcipe .gt_font_normal {
  font-weight: normal;
}

#wfcbehcipe .gt_font_bold {
  font-weight: bold;
}

#wfcbehcipe .gt_font_italic {
  font-style: italic;
}

#wfcbehcipe .gt_super {
  font-size: 65%;
}

#wfcbehcipe .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;wfcbehcipe&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1 (0.7, 1.3)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.856&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Disease severity: Severe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;9.2 (7, 12.2)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;SES: Below poverty level&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1.1 (0.8, 1.4)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.695&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;We miss the effect of race (through hospitalization) on death.&lt;/p&gt;
&lt;p&gt;There is no effect of race on mortality in hospitalized patients‚Ä¶ but there is definitely an effect of race on mortality in the full population, because it affects whether someone gets hospitalized.&lt;/p&gt;
&lt;p&gt;So what are you estimating? What are you going to do with that information?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Become a Superlearner! An Illustrated Guide to Superlearning</title>
      <link>/blog/sl/superlearning/</link>
      <pubDate>Sat, 10 Oct 2020 21:13:14 -0500</pubDate>
      <guid>/blog/sl/superlearning/</guid>
      <description>


&lt;blockquote&gt;
&lt;p&gt;Why use &lt;em&gt;one&lt;/em&gt; machine learning algorithm when you could use all of them?! This post contains a step-by-step walkthrough of how to build a superlearner prediction algorithm in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;/img/Superlearning.jpg&#34;  
         width=100%&#34;&gt;
&lt;figcaption&gt;
&lt;strong&gt;&lt;em&gt;A Visual Guide‚Ä¶&lt;/em&gt;&lt;/strong&gt; Over the winter, I read &lt;a href=&#34;https://www.springer.com/gp/book/9781441997814&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt; by Mark van der Laan and Sherri Rose. This ‚Äúvisual guide‚Äù I made for &lt;em&gt;Chapter 3: Superlearning&lt;/em&gt; by Rose, van der Laan, and Eric Polley is a condensed version of the following tutorial. It is available as an &lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/Superlearner.pdf&#34;&gt;8.5x11&#34; pdf on Github&lt;/a&gt;, should you wish to print it out for reference (or desk decor).
&lt;/figcaption&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;div id=&#34;supercuts-of-superlearning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Supercuts of superlearning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Superlearning&lt;/strong&gt; is a technique for prediction that involves &lt;strong&gt;combining many individual statistical algorithms&lt;/strong&gt; (commonly called ‚Äúdata-adaptive‚Äù or ‚Äúmachine learning‚Äù algorithms) to &lt;strong&gt;create a new, single prediction algorithm&lt;/strong&gt; that is expected to &lt;strong&gt;perform at least as well as any of the individual algorithms&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The superlearner algorithm ‚Äúdecides‚Äù how to combine, or weight, the individual algorithms based upon how well each one &lt;strong&gt;minimizes a specified loss function&lt;/strong&gt;, for example, the mean squared error (MSE). This is done using cross-validation to avoid overfitting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The motivation for this type of ‚Äúensembling‚Äù is that &lt;strong&gt;a mix of multiple algorithms may be more optimal for a given data set than any single algorithm&lt;/strong&gt;. For example, a tree based model averaged with a linear model (e.g.¬†random forests and LASSO) could smooth some of the model‚Äôs edges to improve predictive performance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Superlearning is also called stacking, stacked generalizations, and weighted ensembling by different specializations within the realms of statistics and data science.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/spiderman_meme.jpg&#34; style=&#34;width:42.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;superlearning-step-by-step&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Superlearning, step by step&lt;/h1&gt;
&lt;p&gt;First I‚Äôll go through the algorithm one step at a time using a simulated data set.&lt;/p&gt;
&lt;div id=&#34;initial-set-up-load-libraries-set-seed-simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initial set-up: Load libraries, set seed, simulate data&lt;/h2&gt;
&lt;p&gt;For simplicity I‚Äôll show the concept of superlearning using only four variables (AKA features or predictors) to predict a continuous outcome. Let‚Äôs first simulate a continuous outcome, &lt;code&gt;y&lt;/code&gt;, and four potential predictors, &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, &lt;code&gt;x3&lt;/code&gt;, and &lt;code&gt;x4&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(knitr)
set.seed(7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 5000
obs &amp;lt;- tibble(
  id = 1:n,
  x1 = rnorm(n),
  x2 = rbinom(n, 1, plogis(10*x1)),
  x3 = rbinom(n, 1, plogis(x1*x2 + .5*x2)),
  x4 = rnorm(n, mean=x1*x2, sd=.5*x3),
  y = x1 + x2 + x2*x3 + sin(x4)
)
kable(head(obs), digits=3, caption = &amp;quot;Simulated data set&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Simulated data set&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;y&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.287&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.385&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.197&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.197&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.694&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.694&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.412&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.541&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.928&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.971&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.971&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.947&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.160&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.107&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34; &gt;
&lt;strong&gt;Step 1: Split data into K folds
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step1.png&#34; style=&#34;width:50.0%&#34; /&gt;
The superlearner algorithm relies on K-fold cross-validation (CV) to avoid overfitting. We will start this process by splitting the data into 10 folds. The easiest way to do this is by creating indices for each CV fold.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- 10 # 10 fold cv
cv_index &amp;lt;- sample(rep(1:k, each = n/k)) # create indices for each CV fold. We need each fold K to contain n (all the rows of our data set) divided by k rows. in our example this is 5000/10 = 500 rows in each fold&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 2: Fit base learners for first CV-fold
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step2.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recall that in K-fold CV, each fold serves as the validation set one time. In this first round of CV, we will train all of our base learners on all the CV folds (k = 1,2,‚Ä¶,9) &lt;em&gt;except&lt;/em&gt; for the very last one: &lt;code&gt;cv_index == 10&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The individual algorithms or &lt;strong&gt;base learners&lt;/strong&gt; that we‚Äôll use here are three linear regressions with differently specified parameters:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Learner A&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y=\beta_0 + \beta_1 X_2 + \beta_2 X_4 + \epsilon\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Learner B&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_3 + \beta_4 sin(X_4) + \epsilon\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Learner C&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_1 X_2 + \beta_5 X_1 X_3 + \beta_6 X_2 X_3 + \beta_7 X_1 X_2 X_3 + \epsilon\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_train_1 &amp;lt;- obs[-which(cv_index == 10),] # make a data set that contains all observations except those in k=1
fit_1a &amp;lt;- glm(y ~ x2 + x4, data=cv_train_1) # fit the first linear regression on that training data
fit_1b &amp;lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train_1) # second LR fit on the training data
fit_1c &amp;lt;- glm(y ~ x1*x2*x3, data=cv_train_1) # and the third LR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am &lt;em&gt;only&lt;/em&gt; using the linear regressions so that code for running more complicated regressions does not take away from understanding the general superlearning algorithm.&lt;/p&gt;
&lt;p&gt;Superlearning actually works best if you use a diverse set, or &lt;strong&gt;superlearner library&lt;/strong&gt;, of base learners. For example, instead of three linear regressions, we could use a least absolute shrinkage estimator (LASSO), random forest, and multivariate adaptive splines (MARS). Any parametric or non-parametric supervised machine learning algorithm can be included as a base learner.&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 3: Obtain predictions for first CV-fold
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step3.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can then get use our validation data, &lt;code&gt;cv_index == 10&lt;/code&gt;, to obtain our first set of cross-validated predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_valid_1 &amp;lt;- obs[which(cv_index == 10),] # make a data set that only contains observations except in k=10
pred_1a &amp;lt;- predict(fit_1a, newdata = cv_valid_1) # use that data set as the validation for all the models in the SL library
pred_1b &amp;lt;- predict(fit_1b, newdata = cv_valid_1) 
pred_1c &amp;lt;- predict(fit_1c, newdata = cv_valid_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we have 5000 &lt;code&gt;obs&lt;/code&gt;ervations, that gives us three vectors of length 500: a set of predictions for each of our Learners A, B, and C.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(pred_1a) # double check we only have n/k predictions ...we do :-)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 500&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(cbind(pred_1a, pred_1b, pred_1c)), digits= 2, caption = &amp;quot;First CV round of predictions&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-6&#34;&gt;Table 2: &lt;/span&gt;First CV round of predictions&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;pred_1a&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pred_1b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pred_1c&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.77&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4.27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3.31&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.98&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.83&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 4: Obtain CV predictions for entire data set
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step4.png&#34; style=&#34;width:32.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We‚Äôll want to get those predictions for &lt;em&gt;every&lt;/em&gt; fold. So, using your favorite &lt;code&gt;for&lt;/code&gt; loop, &lt;code&gt;apply&lt;/code&gt; statement, or &lt;code&gt;map&lt;/code&gt;ping function, fit the base learners and obtain predictions for each of them, so that there are 1000 predictions ‚Äì one for every point in &lt;code&gt;obs&lt;/code&gt;ervations.&lt;/p&gt;
&lt;p&gt;The way I chose to code this was to make a generic function that combines Step 2 (base learners fit to the training data) and Step 3 (predictions on the validation data), then use &lt;code&gt;map_dfr()&lt;/code&gt; from the &lt;code&gt;purrr&lt;/code&gt; package to repeat over all 10 CV folds. I saved the results in a new data frame called &lt;code&gt;cv_preds&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_folds &amp;lt;- as.list(1:k)
names(cv_folds) &amp;lt;- paste0(&amp;quot;fold&amp;quot;,1:k)

get_preds &amp;lt;- function(fold){   # function that does the same procedure as step 2 and 3 for any CV fold
  cv_train &amp;lt;- obs[-which(cv_index == fold),]  # make a training data set that contains all data except fold k
  fit_a &amp;lt;- glm(y ~ x2 + x4, data=cv_train)  # fit all the base learners to that data
  fit_b &amp;lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=cv_train)
  fit_c &amp;lt;- glm(y ~ x1*x2*x3, data=cv_train)
  cv_valid &amp;lt;- obs[which(cv_index == fold),]  # make a validation data set that only contains data from fold k
  pred_a &amp;lt;- predict(fit_a, newdata = cv_valid)  # obtain predictions from all the base learners for that validation data
  pred_b &amp;lt;- predict(fit_b, newdata = cv_valid)
  pred_c &amp;lt;- predict(fit_c, newdata = cv_valid)
  return(data.frame(&amp;quot;obs_id&amp;quot; = cv_valid$id, &amp;quot;cv_fold&amp;quot; = fold, pred_a, pred_b, pred_c))  # save the predictions and the ids of the observations in a data frame
}

cv_preds &amp;lt;- purrr::map_dfr(cv_folds, ~get_preds(fold = .x)) # map_dfr loops through every fold (1:k) and binds the rows of the listed results together

cv_preds %&amp;gt;% arrange(obs_id) %&amp;gt;% head() %&amp;gt;% kable(digits=2, caption = &amp;quot;All CV predictions for all three base learners&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-7&#34;&gt;Table 3: &lt;/span&gt;All CV predictions for all three base learners&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;obs_id&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cv_fold&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pred_a&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pred_b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pred_c&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1‚Ä¶1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.42&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1‚Ä¶2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.77&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1‚Ä¶3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.81&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1‚Ä¶4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.77&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1‚Ä¶5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1‚Ä¶6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.96&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.94&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 5: Choose and compute loss function of interest via metalearner
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step5.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is the key step of the superlearner algorithm: we will use a new learner, a &lt;strong&gt;metalearner&lt;/strong&gt;, to take information from all of the base learners and create that new algorithm.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have cross-validated predictions for every observation in the data set, we want to merge those CV predictions back into our main data set‚Ä¶&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obs_preds &amp;lt;- 
  full_join(obs, cv_preds, by=c(&amp;quot;id&amp;quot; = &amp;quot;obs_id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;‚Ä¶so that we can minimize a final loss function of interest between the true outcome and each CV prediction. This is how we‚Äôre going to optimize our overall prediction algorithm: we want to make sure we‚Äôre ‚Äúlosing the least‚Äù in the way we combine our base learners‚Äô predictions to ultimately make final predictions. We can do this efficiently by choosing a new learner, a metalearner, which reflects the final loss function of interest.&lt;/p&gt;
&lt;p&gt;For simplicity, we‚Äôll use another linear regression as our metalearner. Using a linear regression as a metalearner will minimize the Cross-Validated Mean Squared Error (CV-MSE) when combining the base learner predictions. Note that we could use a variety of parametric or non-parametric regressions to minimize the CV-MSE.&lt;/p&gt;
&lt;p&gt;No matter what metalearner we choose, the predictors will always be the cross-validated predictions from each base learner, and the outcome will always be the true outcome, &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_fit &amp;lt;- glm(y ~ pred_a + pred_b + pred_c, data = obs_preds)
kable(broom::tidy(sl_fit), digits=3, caption = &amp;quot;Metalearner regression coefficients&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-9&#34;&gt;Table 4: &lt;/span&gt;Metalearner regression coefficients&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.003&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.002&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.447&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.148&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pred_a&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.017&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.004&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-4.739&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pred_b&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.854&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.007&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128.241&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pred_c&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.165&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.005&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.103&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This metalearner provides us with the coefficients, or weights, to apply to each of the base learners. In other words, if we have a set of predictions from Learner A, B, and C, we can obtain our best possible predictions by starting with an intercept of -0.003, then adding -0.017 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; predictions from Learner A, 0.854 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; predictions from Learner B, and 0.165 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; predictions from Learner C.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For more information on the metalearning step, check out the &lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 6: Fit base learners on entire data set
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step6.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After we fit the metalearner, we officially have our superlearner algorithm, so it‚Äôs time to input data and obtain predictions! To implement the algorithm and obtain final predictions, we first need to fit the base learners on the full data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_a &amp;lt;- glm(y ~ x2 + x4, data=obs)
fit_b &amp;lt;- glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=obs)
fit_c &amp;lt;- glm(y ~ x1*x2*x3, data=obs)&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 7: Obtain predictions from each base learner on entire data set
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step7.png&#34; style=&#34;width:40.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We‚Äôll use &lt;em&gt;those&lt;/em&gt; base learner fits to get predictions from each of the base learners for the entire data set, and then we will plug those predictions into the metalearner fit. Remember, we were previously using cross-validated predictions, rather than fitting the base learners on the whole data set. This was to avoid overfitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_a &amp;lt;- predict(fit_a)
pred_b &amp;lt;- predict(fit_b)
pred_c &amp;lt;- predict(fit_c)
full_data_preds &amp;lt;- tibble(pred_a, pred_b, pred_c)&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 8: Use metalearner fit to weight base learners
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_steps/step8.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once we have the predictions from the full data set, we can input them to the metalearner, where the output will be a final prediction for &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_predictions &amp;lt;- predict(sl_fit, newdata = full_data_preds)
kable(head(sl_predictions), col.names = &amp;quot;sl_predictions&amp;quot;, digits= 2, caption = &amp;quot;Final SL predictions (manual)&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-12&#34;&gt;Table 5: &lt;/span&gt;Final SL predictions (manual)&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;sl_predictions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.03&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And‚Ä¶ that‚Äôs it! Those are our superlearner predictions for the full data set.&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 9: Obtain predictions on new data
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;We can now modify Step 7 and Step 8 to accommodate any new observation(s):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;To predict on new data:&lt;/strong&gt;&lt;br&gt;¬†1. Use the fits from each base learner to obtain base learner predictions for the new observation(s).&lt;br&gt;¬†2. Plug those base learner predictions into the metalearner fit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can generate a single &lt;code&gt;new_obs&lt;/code&gt;ervation to see how this would work in practice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_obs &amp;lt;- tibble(x1 = .5, x2 = 0, x3 = 0, x4 = -3)
new_pred_a &amp;lt;- predict(fit_a, new_obs)
new_pred_b &amp;lt;- predict(fit_b, new_obs)
new_pred_c &amp;lt;- predict(fit_c, new_obs)
new_pred_df &amp;lt;- tibble(&amp;quot;pred_a&amp;quot; = new_pred_a, &amp;quot;pred_b&amp;quot; = new_pred_b, &amp;quot;pred_c&amp;quot; = new_pred_c)
predict(sl_fit, newdata = new_pred_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1 
## 0.1181052&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our superlearner model predicts that an observation with predictors &lt;span class=&#34;math inline&#34;&gt;\(X_1=.5\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_2=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_3=0\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(X_4=-3\)&lt;/span&gt; will have an outcome of &lt;span class=&#34;math inline&#34;&gt;\(Y=0.118\)&lt;/span&gt;.&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:#c30a0a&#34;&gt;
&lt;strong&gt;Step 10 and beyond‚Ä¶
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;We could compute the MSE of the ensemble superlearner predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_mse &amp;lt;- mean((obs$y - sl_predictions)^2)
sl_mse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01927392&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also add more algorithms to our base learner library (we definitely should, since we only used linear regressions!), and we could write functions to tune these algorithms‚Äô hyperparameters over various grids. For example, if we were to include random forest in our library, we may want to tune over a number of trees and maximum bucket sizes.&lt;/p&gt;
&lt;p&gt;We can then cross-validate this entire process to evaluate the predictive performance of our superlearner algorithm. Alternatively, we could leave a hold-out training data set to evaluate the performance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-superlearner-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the &lt;code&gt;SuperLearner&lt;/code&gt; package&lt;/h1&gt;
&lt;p&gt;Or‚Ä¶ we could use a package and avoid all the hand-coding. Here is how you would build an ensemble superlearner for our data with the base learner libraries of &lt;code&gt;ranger&lt;/code&gt; (random forests), &lt;code&gt;glmnet&lt;/code&gt; (LASSO, by default), and &lt;code&gt;earth&lt;/code&gt; (MARS) using the &lt;code&gt;SuperLearner&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(SuperLearner)
x_df &amp;lt;- obs %&amp;gt;% select(x1:x4) %&amp;gt;% as.data.frame()
sl_fit &amp;lt;- SuperLearner(Y = obs$y, X = x_df, family = gaussian(),
                     SL.library = c(&amp;quot;SL.ranger&amp;quot;, &amp;quot;SL.glmnet&amp;quot;, &amp;quot;SL.earth&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can specify the metalearner with the &lt;code&gt;method&lt;/code&gt; argument. The default is &lt;a href=&#34;##non-negative-least-squares&#34;&gt;Non-Negative Least Squares&lt;/a&gt; (NNLS).&lt;/p&gt;
&lt;div id=&#34;cv-risk-and-coefficient-weights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CV-Risk and Coefficient Weights&lt;/h2&gt;
&lt;p&gt;We can examine the cross-validated &lt;code&gt;Risk&lt;/code&gt; (loss function), and the &lt;code&gt;Coef&lt;/code&gt;ficient (weight) given to each of the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  
## SuperLearner(Y = obs$y, X = x_df, family = gaussian(), SL.library = c(&amp;quot;SL.ranger&amp;quot;,  
##     &amp;quot;SL.glmnet&amp;quot;, &amp;quot;SL.earth&amp;quot;)) 
## 
## 
##                      Risk      Coef
## SL.ranger_All 0.013278476 0.1619231
## SL.glmnet_All 0.097149642 0.0000000
## SL.earth_All  0.003168299 0.8380769&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this summary we can see that the CV-risk (the default risk is MSE) in this library of base learners is smallest for &lt;code&gt;SL.Earth&lt;/code&gt;. This translates to the largest coefficient, or weight, given to the predictions from &lt;code&gt;earth&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The LASSO model implemented by &lt;code&gt;glmnet&lt;/code&gt; has the largest CV-risk, and after the metalearning step, those predictions receive a coefficient, or weight, of 0. This means that the predictions from LASSO will not be incorporated into the final predictions at all.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;obtaining-the-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Obtaining the predictions&lt;/h2&gt;
&lt;p&gt;We can extract the predictions easily via the &lt;code&gt;SL.predict&lt;/code&gt; element of the &lt;code&gt;SuperLearner&lt;/code&gt; fit object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(head(sl_fit$SL.predict), digits=2, col.names = &amp;quot;sl_predictions&amp;quot;, caption = &amp;quot;Final SL predictions (package)&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-17&#34;&gt;Table 6: &lt;/span&gt;Final SL predictions (package)&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;sl_predictions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.68&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-1.08&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validated-superlearner&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross-validated Superlearner&lt;/h2&gt;
&lt;p&gt;Recall that we can cross-validate the entire model fitting process to evaluate the predictive performance of our superlearner algorithm. This is easy with the function &lt;code&gt;CV.SuperLearner()&lt;/code&gt;. Beware, this gets computationally burdensome very quickly!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_sl_fit &amp;lt;- CV.SuperLearner(Y = obs$y, X = x_df, family = gaussian(),
                     SL.library = c(&amp;quot;SL.ranger&amp;quot;, &amp;quot;SL.glmnet&amp;quot;, &amp;quot;SL.earth&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more information on the &lt;code&gt;SuperLearner&lt;/code&gt; package, take a look at this &lt;a href=&#34;https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html&#34;&gt;vignette&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-packages-to-superlearn&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alternative packages to superlearn&lt;/h2&gt;
&lt;p&gt;Other packages freely available in &lt;code&gt;R&lt;/code&gt; that can be used to implement the superlearner algorithm include &lt;a href=&#34;https://tlverse.org/tlverse-handbook/sl3.html&#34;&gt;&lt;code&gt;sl3&lt;/code&gt;&lt;/a&gt; (an update to the original &lt;code&gt;Superlearner&lt;/code&gt; package), &lt;a href=&#34;https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html&#34;&gt;&lt;code&gt;h2o&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://rdrr.io/cran/caretEnsemble/f/vignettes/caretEnsemble-intro.Rmd&#34;&gt;&lt;code&gt;caretEnsemble&lt;/code&gt;&lt;/a&gt;. I previously wrote a &lt;a href=&#34;https://www.khstats.com/blog/sl3_demo/sl/&#34;&gt;brief demo&lt;/a&gt; on using &lt;code&gt;sl3&lt;/code&gt; for an NYC R-Ladies demo.&lt;/p&gt;
&lt;p&gt;The authors of &lt;code&gt;tidymodels&lt;/code&gt; ‚Äì a suite of packages for machine learning including &lt;code&gt;recipes&lt;/code&gt;, &lt;code&gt;parsnip&lt;/code&gt;, and &lt;code&gt;rsample&lt;/code&gt; ‚Äì recently came out with a new package to perform superlearning/stacking called &lt;a href=&#34;%5Bhttps://stacks.tidymodels.org/articles/basics.html&#34;&gt;&lt;code&gt;stacks&lt;/code&gt;&lt;/a&gt;. Prior to this, Alex Hayes wrote a &lt;a href=&#34;https://www.alexpghayes.com/blog/implementing-the-super-learner-with-tidymodels/&#34;&gt;blog post&lt;/a&gt; on using &lt;code&gt;tidymodels&lt;/code&gt; infrastructure to implement superlearning.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coming-soon-when-prediction-is-not-the-end-goal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Coming soon‚Ä¶ when prediction is not the end goal&lt;/h1&gt;
&lt;p&gt;When prediction is not the end goal, superlearning combines well with semi-parametric estimation methods for statistical inference. This is the reason I was reading &lt;em&gt;Targeted Learning&lt;/em&gt; in the first place; I am a statistician with collaborators who typically want estimates of treatment effects with confidence intervals, not predictions!&lt;/p&gt;
&lt;p&gt;I‚Äôm working on a similar visual guide for one such semiparametric estimation method: &lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;Targeted Maximum Likelihood Estimation&lt;/a&gt; (TMLE)). TMLE allows the use of flexible statistical models like the superlearner algorithm when estimating treatment effects. If you found this superlearning tutorial helpful, check back here later for another one on TMLE. If you‚Äôre curious about TMLE in the meantime, I really like &lt;a href=&#34;https://migariane.github.io/TMLE.nb.html&#34;&gt;this tutorial&lt;/a&gt; by Miguel Angel Luque Fernandez.&lt;/p&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;/img/TMLE.jpg&#34;
         width=100%&#34;&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;These sections contain a bit of extra information on the superlearning algorithm, such as: intuition on manually computing the loss function of interest, explanation of the discrete superlearner, brief advice on choosing a metalearner, and a different summary visual provided in the &lt;em&gt;Targeted Learning&lt;/em&gt; book.&lt;/p&gt;
&lt;div id=&#34;manually-computing-the-mse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Manually computing the MSE&lt;/h3&gt;
&lt;p&gt;Let‚Äôs say we have chosen our loss function of interest to be the Mean Squared Error (MSE). We could first compute the squared error &lt;span class=&#34;math inline&#34;&gt;\((y - \hat{y})^2\)&lt;/span&gt; for each CV prediction A, B, and C.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_sq_error &amp;lt;-
  obs_preds %&amp;gt;%
  mutate(cv_sqrd_error_a = (y-pred_a)^2,   # compute squared error for each observation
         cv_sqrd_error_b = (y-pred_b)^2,
         cv_sqrd_error_c = (y-pred_c)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_sq_error %&amp;gt;% 
  pivot_longer(c(cv_sqrd_error_a, cv_sqrd_error_b, cv_sqrd_error_c), # make the CV squared errors long form for plotting
               names_to = &amp;quot;base_learner&amp;quot;,
               values_to = &amp;quot;squared_error&amp;quot;) %&amp;gt;%
  mutate(base_learner = toupper(str_remove(base_learner, &amp;quot;cv_sqrd_error_&amp;quot;))) %&amp;gt;%
  ggplot(aes(base_learner, squared_error, col=base_learner)) + # make box plots
  geom_boxplot() +
  theme_bw() +
  guides(col=F) +
  labs(x = &amp;quot;Base Learner&amp;quot;, y=&amp;quot;Squared Error&amp;quot;, title=&amp;quot;Squared Errors of Learner A, B, and C&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/sl/superlearning_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And then take the mean of those three cross-validated squared error columns, grouped by &lt;code&gt;cv_fold&lt;/code&gt;, to get the CV-MSE for each fold.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_risks &amp;lt;-
  cv_sq_error %&amp;gt;%
  group_by(cv_fold) %&amp;gt;%
  summarise(cv_mse_a = mean(cv_sqrd_error_a),
            cv_mse_b = mean(cv_sqrd_error_b),
            cv_mse_c = mean(cv_sqrd_error_c)
            )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_risks %&amp;gt;%
  pivot_longer(cv_mse_a:cv_mse_c,
               names_to = &amp;quot;base_learner&amp;quot;,
               values_to = &amp;quot;mse&amp;quot;) %&amp;gt;%
  mutate(base_learner = toupper(str_remove(base_learner,&amp;quot;cv_mse_&amp;quot;)))  %&amp;gt;%
  ggplot(aes(cv_fold, mse, col=base_learner)) +
  geom_point() +
  theme_bw()  +
    scale_x_continuous(breaks = 1:10) +
  labs(x = &amp;quot;Cross-Validation (CV) Fold&amp;quot;, y=&amp;quot;Mean Squared Error (MSE)&amp;quot;, col = &amp;quot;Base Learner&amp;quot;, title=&amp;quot;CV-MSEs for Base Learners A, B, and C&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/sl/superlearning_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that across each fold, Learner B consistently has an MSE around 0.02, while Learner C hovers around 0.1, and Learner A varies between 0.35 and 0.45. We can take another mean to get the overall CV-MSE for each learner.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_risks %&amp;gt;%
  select(-cv_fold) %&amp;gt;%
  summarise_all(mean) %&amp;gt;%
  kable(digits=2, caption = &amp;quot;CV-MSE for each base learner&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-22&#34;&gt;Table 7: &lt;/span&gt;CV-MSE for each base learner&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;cv_mse_a&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cv_mse_b&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cv_mse_c&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.38&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The base learner that performs the best using our chosen loss function of interest is clearly Learner B. We can see from our data simulation code why this is true ‚Äì Learner B is almost exactly the mimicking the data generating mechanism of &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Our results align with the linear regression fit from our metalearning step; Learner B predictions received a much larger coefficient relative to Learners A and C.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discrete-superlearner&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Discrete Superlearner&lt;/h3&gt;
&lt;p&gt;We &lt;em&gt;could&lt;/em&gt; stop after minimizing our loss function (MSE) and fit Learner B to our full data set, and that would be called using the &lt;strong&gt;discrete superlearner&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;discrete_sl_predictions &amp;lt;- predict(glm(y ~ x1 + x2 + x1*x3 + sin(x4), data=obs))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we can almost always create an even better prediction algorithm if we use information from &lt;em&gt;all&lt;/em&gt; of the algorithms‚Äô CV predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-metalearner&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Choosing a metalearner&lt;/h3&gt;
&lt;p&gt;In the &lt;a href=&#34;#references&#34;&gt;Reference&lt;/a&gt; papers on superlearning, the metalearner which yields the best results theoretically and in practice is a &lt;strong&gt;convex combination optimization&lt;/strong&gt; of learners. This means fitting the following regression, where &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_3\)&lt;/span&gt; are all non-negative and sum to 1.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{E}[Y|\hat{Y}_{LrnrA},\hat{Y}_{LrnrB},\hat{Y}_{LrnrC}] = \alpha_1\hat{Y}_{LrnrA} + \alpha_2\hat{Y}_{LrnrB} + \alpha_3\hat{Y}_{LrnrC}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The default in the &lt;code&gt;Superlearner&lt;/code&gt; package is to fit a non-negative least squares (NNLS) regression. NNLS fits the above equation where the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;‚Äôs must be greater than or equal to 0 but do not necessarily sum to 1. The package then reweights the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;‚Äôs to force them to sum to 1. This makes the weights a convex combination, but may not yield the same optimal results as an initial convex combination optimization.&lt;/p&gt;
&lt;p&gt;The metalearner should change with the goals of the prediction algorithm and the loss function of interest. In these examples it is the MSE, but if the goal is to build a prediction algorithm that is best for binary classification, the loss function of interest may be the rank loss, or &lt;span class=&#34;math inline&#34;&gt;\(1-AUC\)&lt;/span&gt;. It is outside the scope of this post, but for more information, I recommend this &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4912128/&#34;&gt;paper&lt;/a&gt; by Erin Ledell on maximizing the Area Under the Curve (AUC) with superlearner algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-visual-guide-for-superlearning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Another visual guide for superlearning&lt;/h3&gt;
&lt;p&gt;The steps of the superlearner algorithm are summarized nicely in this graphic in Chapter 3 of the &lt;em&gt;Targeted Learning&lt;/em&gt; book:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/sl_diagram.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Acknowledgments&lt;/h1&gt;
&lt;p&gt;Thank you to Eric Polley, Iv√°n D√≠az, Nick Williams, Anjile An, and Adam Peterson for very helpful content (and design!) suggestions for this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;MJ Van der Laan, EC Polley, AE Hubbard, Super Learner, Statistical applications in genetics and molecular, 2007&lt;/p&gt;
&lt;p&gt;Polley, Eric. ‚ÄúChapter 3: Superlearning.‚Äù Targeted Learning: Causal Inference for Observational and Experimental Data, by M. J. van der. Laan and Sherri Rose, Springer, 2011.&lt;/p&gt;
&lt;p&gt;Polley E, LeDell E, Kennedy C, van der Laan M. Super Learner: Super Learner Prediction. 2016 URL &lt;a href=&#34;https://CRAN.R-project.org/package=SuperLearner&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=SuperLearner&lt;/a&gt;. R package version 2.0-22.&lt;/p&gt;
&lt;p&gt;Naimi AI, Balzer LB. Stacked generalization: an introduction to super learning. &lt;em&gt;Eur J Epidemiol.&lt;/em&gt; 2018;33(5):459-464. &lt;a href=&#34;doi:10.1007/s10654-018-0390-z&#34; class=&#34;uri&#34;&gt;doi:10.1007/s10654-018-0390-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LeDell, E. (2015). Scalable Ensemble Learning and Computationally Efficient Variance Estimation. UC Berkeley. ProQuest ID: LeDell_berkeley_0028E_15235. Merritt ID: ark:/13030/m5wt1xp7. Retrieved from &lt;a href=&#34;https://escholarship.org/uc/item/3kb142r2&#34; class=&#34;uri&#34;&gt;https://escholarship.org/uc/item/3kb142r2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;M. Petersen and L. Balzer. Introduction to Causal Inference. UC Berkeley, August 2014. &lt;a href=&#34;www.ucbbiostat.com&#34;&gt;www.ucbbiostat.com&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] SuperLearner_2.0-26 nnls_1.4            knitr_1.28         
##  [4] forcats_0.5.0       stringr_1.4.0       dplyr_1.0.2        
##  [7] purrr_0.3.4         readr_1.3.1         tidyr_1.1.2        
## [10] tibble_3.0.3        ggplot2_3.3.2       tidyverse_1.3.0    
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.5         lubridate_1.7.9    lattice_0.20-38    plotmo_3.5.7      
##  [5] earth_5.1.2        assertthat_0.2.1   glmnet_3.0-2       digest_0.6.25     
##  [9] foreach_1.5.0      ranger_0.12.1      R6_2.4.1           cellranger_1.1.0  
## [13] backports_1.1.8    reprex_0.3.0       evaluate_0.14      httr_1.4.1        
## [17] highr_0.8          blogdown_0.19      pillar_1.4.6       TeachingDemos_2.12
## [21] rlang_0.4.7        readxl_1.3.1       rstudioapi_0.11    Matrix_1.2-18     
## [25] rmarkdown_2.1      labeling_0.3       munsell_0.5.0      broom_0.7.0       
## [29] compiler_3.6.3     modelr_0.1.6       xfun_0.14          pkgconfig_2.0.3   
## [33] shape_1.4.4        htmltools_0.4.0    tidyselect_1.1.0   bookdown_0.19     
## [37] codetools_0.2-16   fansi_0.4.1        crayon_1.3.4       dbplyr_1.4.3      
## [41] withr_2.2.0        cabinets_0.6.0     grid_3.6.3         jsonlite_1.6.1    
## [45] gtable_0.3.0       lifecycle_0.2.0    DBI_1.1.0          magrittr_1.5      
## [49] scales_1.1.1       cli_2.0.2          stringi_1.4.6      farver_2.0.3      
## [53] fs_1.4.1           xml2_1.3.0         ellipsis_0.3.1     generics_0.0.2    
## [57] vctrs_0.3.4        Formula_1.2-3      iterators_1.0.12   tools_3.6.3       
## [61] glue_1.4.2         hms_0.5.3          plotrix_3.7-7      yaml_2.2.1        
## [65] colorspace_1.4-1   rvest_0.3.5        haven_2.2.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Key to Learning Causal Inference</title>
      <link>/blog/collider-bias/hardest_part_of_causal/</link>
      <pubDate>Sat, 12 Sep 2020 21:13:14 -0500</pubDate>
      <guid>/blog/collider-bias/hardest_part_of_causal/</guid>
      <description>


&lt;p&gt;I‚Äôve been trying to learn causal inference for the past year and a half. Not in the way people say, ‚ÄúI‚Äôve been trying to learn guitar‚Äù or ‚ÄúI‚Äôve been trying to learn Japanese‚Äù, either. Trying as in, spent too many pre-pandemic Sundays at crowded coffee shops with a backpack causal inference books, and even dragged a binder of notes to a vacation in Costa Rica‚Ä¶&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/causal_at_beach.jpg&#34; /&gt;
I‚Äôm no expert in causal inference, but at this point I at least have an opinion on the best way to &lt;em&gt;learn&lt;/em&gt; causal inference. It‚Äôs actually a pretty simple concept, but I was really spinning my wheels until I learned it. So, in case you‚Äôre early into your causal inference journey and haven‚Äôt discovered it yet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I believe the key to learning (and applying) causal inference is to always keep &lt;strong&gt;identification&lt;/strong&gt; separate from &lt;strong&gt;estimation&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now what does &lt;em&gt;that&lt;/em&gt; mean!? Broadly, &lt;strong&gt;identification means figuring out whether you can write a causal estimand of interest in terms of data&lt;/strong&gt; (i.e., in forms of expectations of random variables).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;causal estimand&lt;/strong&gt; is just some summary quantity that would be interesting to evaluate in the system you‚Äôre studying. A few common causal estimands are the Average Treatment Effect and the Conditional Average Treatment Effect. There are many, many more though, such as the Dose Response Curve, Causal Odds Ratio, Causal Risk Ratio, and Marginal Structural Model.&lt;/p&gt;
&lt;p&gt;To determine whether you can identify your causal estimand of interest in your system of study and with the data you have, careful thought is required. It‚Äôs the stage of a problem when you, as a statistician, sit down and talk with subject matter experts about the precise question they‚Äôd like to answer. You may draw &lt;em&gt;Directed Acyclic Graphs&lt;/em&gt; (DAGs) or &lt;em&gt;Single World Intervention Graphs&lt;/em&gt; (SWIGs), or write out the &lt;em&gt;Non Parametric Structural Equations&lt;/em&gt; of the system.&lt;/p&gt;
&lt;p&gt;These tools will help you evaluate some of the main requirements of causal inference: for the causal estimand of interest to be identified, you must satisfy causal assumptions of &lt;em&gt;positivity&lt;/em&gt;, &lt;em&gt;exchangeability&lt;/em&gt;, and &lt;em&gt;ignorability&lt;/em&gt;, while still avoiding the usual pitfalls in statistics like &lt;em&gt;selection bias&lt;/em&gt; and &lt;em&gt;unmeasured confounding&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You‚Äôll eventually determine whether the causal estimand is possible to write out in terms of your data. For example, if your goal is to identify the Average Treatment Effect (ATE), i.e.¬†the mean difference in outcomes between the treated and untreated in a hypothetical world where everyone &lt;em&gt;could&lt;/em&gt; get each treatment &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, your identification of the ATE, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, may look like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta = E_W[E[Y|A=1,W]-E[Y|A=0,W]]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is a vector of confounding variables that affect both &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Once&lt;/p&gt;
&lt;p&gt;Everything I‚Äôve mentioned so far is part of the &lt;strong&gt;identification&lt;/strong&gt; side of causal inference. I think of it as the non-data side, because we are not yet dealing with data when we think about the study design, causal estimand, and whether the causal conditions are reasonable to assume. You should never be thinking about &lt;strong&gt;estimation methods&lt;/strong&gt; until you‚Äôve determined whether your causal estimand can be identified.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/causal_comic.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once you‚Äôve &lt;em&gt;identified&lt;/em&gt; your WHAT, or your estimand of interest, then you can start thinking about HOW you‚Äôre going to &lt;em&gt;estimate&lt;/em&gt; that estimand. There are many statistical estimation methods to estimate the same causal effect of interest.&lt;/p&gt;
&lt;p&gt;My favorite way to think about this is through a little graph like this. On the left, there are three causal estimands, or estimands of interest. On the top, we can see six different ways you might try to estimate any of these causal estimands of interest. Each of the estimation methods have pros and cons.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Odds ratio = E[Y]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Relative risk = E[Y]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathrm{ATE} = E[Y]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A common example is figuring out whether the Average Treatment Effect (ATE), or the difference in outcomes if everyone were to receive the exposure compared to if everyone had not received the exposure&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta^* = E[E[Y|A=1,W]-E[Y|A=0,W]]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Sure, propensity scores are part of causal inference, but they‚Äôre part of the &lt;em&gt;estimation&lt;/em&gt; side of causal inference. A propensity score can‚Äôt fix a research question with a causal estimand that does not meet causal assumptions.&lt;/p&gt;
&lt;p&gt;Causal&lt;/p&gt;
&lt;p&gt;I think causal inference is the most interesting topic in all of statistics‚Äì I think it‚Äôs so cool that sometimes I make really dweeby comics about it:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Conditional and Iterated Expectations with a Linear Regression Model</title>
      <link>/blog/iterated-expectations/iterated-expectations/</link>
      <pubDate>Sat, 14 Mar 2020 21:13:14 -0500</pubDate>
      <guid>/blog/iterated-expectations/iterated-expectations/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;hr /&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TL;DR&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can a regress an outcome on a grouping variable &lt;em&gt;plus any other variable(s)&lt;/em&gt; and the unadjusted and adjusted group means will be identical.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can see this in a simple example using the &lt;a href=&#34;https://github.com/allisonhorst/palmerpenguins&#34;&gt;&lt;code&gt;palmerpenguins&lt;/code&gt;&lt;/a&gt; data:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remotes::install_github(&amp;quot;allisonhorst/palmerpenguins&amp;quot;)
library(palmerpenguins)
library(tidyverse)
library(gt)

# use complete cases for simplicity
penguins &amp;lt;- drop_na(penguins)

penguins %&amp;gt;%
  # fit a linear regression for bill length given bill depth and species
  # make a new column containing the fitted values for bill length
  mutate(preds = predict(lm(bill_length_mm ~ bill_depth_mm + species, data = .))) %&amp;gt;%
  # compute unadjusted and adjusted group means
  group_by(species) %&amp;gt;%
  summarise(mean_bill_length = mean(bill_length_mm),
            mean_predicted_bill_length = mean(preds)) %&amp;gt;%
  gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#mcysheiceb .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#mcysheiceb .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#mcysheiceb .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#mcysheiceb .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#mcysheiceb .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#mcysheiceb .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#mcysheiceb .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#mcysheiceb .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#mcysheiceb .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#mcysheiceb .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#mcysheiceb .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#mcysheiceb .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#mcysheiceb .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#mcysheiceb .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#mcysheiceb .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#mcysheiceb .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#mcysheiceb .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#mcysheiceb .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#mcysheiceb .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#mcysheiceb .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#mcysheiceb .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#mcysheiceb .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#mcysheiceb .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#mcysheiceb .gt_left {
  text-align: left;
}

#mcysheiceb .gt_center {
  text-align: center;
}

#mcysheiceb .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#mcysheiceb .gt_font_normal {
  font-weight: normal;
}

#mcysheiceb .gt_font_bold {
  font-weight: bold;
}

#mcysheiceb .gt_font_italic {
  font-style: italic;
}

#mcysheiceb .gt_super {
  font-size: 65%;
}

#mcysheiceb .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;mcysheiceb&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;species&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_bill_length&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_predicted_bill_length&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Adelie&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Chinstrap&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Gentoo&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is because &lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z]=E[Y|Z=z]\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can view a fitted value from the regression, &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z]\)&lt;/span&gt;, as a random variable to help us see this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-by-step-proof&#34;&gt;Skip to the end&lt;/a&gt; to see the proof.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src=&#34;/img/expectations.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I‚Äôll admit I spent many weeks of my first probability theory course struggling to understand when and why my professor was writing &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. When I finally learned all the rules for expectations of random variables, I still had zero appreciation for their implications in my future work as an applied statistician.&lt;/p&gt;
&lt;p&gt;I recently found myself in a rabbit hole of expectation properties while trying to write a seemingly simple function in &lt;code&gt;R&lt;/code&gt;. Now that I have the output of my function all sorted out, I have a newfound appreciation for how I can use regressions ‚Äì a framework I‚Äôm very comfortable with ‚Äì to rethink some of the properties I learned in my probability theory courses.&lt;/p&gt;
&lt;p&gt;In the function, I was regressing an outcome on a few variables plus a grouping variable, and then returning the group means of the fitted values. My function kept outputting adjusted group means that were &lt;em&gt;identical&lt;/em&gt; to the unadjusted group means.&lt;/p&gt;
&lt;p&gt;I soon realized that for what I needed to do, my grouping variable should not be in the regression model. However, I was still perplexed as to how the adjusted and unadjusted group means could be the same.&lt;/p&gt;
&lt;p&gt;I created a very basic example to test this unexpected result. I regressed a variable from the new &lt;code&gt;penguins&lt;/code&gt; data set, &lt;code&gt;bill_length_mm&lt;/code&gt;, on another variable called &lt;code&gt;bill_depth_mm&lt;/code&gt; and a grouping variable &lt;code&gt;species&lt;/code&gt;. I then looked at the mean within each category of &lt;code&gt;species&lt;/code&gt; for both the unadjusted &lt;code&gt;bill_depth_mm&lt;/code&gt; and fitted values from my linear regression model for &lt;code&gt;bill_depth_mm&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins %&amp;gt;%
  # fit a linear regression for bill length given bill depth and species
  # make a new column containing the fitted values for bill length
  mutate(preds = predict(lm(bill_length_mm ~ bill_depth_mm + species, data = .))) %&amp;gt;%
  # compute unadjusted and adjusted group means
  group_by(species) %&amp;gt;%
  summarise(mean_bill_length = mean(bill_length_mm),
            mean_predicted_bill_length = mean(preds)) %&amp;gt;%
  gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#ecegfguyhs .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ecegfguyhs .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ecegfguyhs .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ecegfguyhs .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ecegfguyhs .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ecegfguyhs .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ecegfguyhs .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ecegfguyhs .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ecegfguyhs .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ecegfguyhs .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#ecegfguyhs .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ecegfguyhs .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ecegfguyhs .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#ecegfguyhs .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#ecegfguyhs .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ecegfguyhs .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#ecegfguyhs .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ecegfguyhs .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#ecegfguyhs .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ecegfguyhs .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ecegfguyhs .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ecegfguyhs .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#ecegfguyhs .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#ecegfguyhs .gt_left {
  text-align: left;
}

#ecegfguyhs .gt_center {
  text-align: center;
}

#ecegfguyhs .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ecegfguyhs .gt_font_normal {
  font-weight: normal;
}

#ecegfguyhs .gt_font_bold {
  font-weight: bold;
}

#ecegfguyhs .gt_font_italic {
  font-style: italic;
}

#ecegfguyhs .gt_super {
  font-size: 65%;
}

#ecegfguyhs .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;ecegfguyhs&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;species&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_bill_length&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_predicted_bill_length&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Adelie&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Chinstrap&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Gentoo&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;I saw the same strange output, even in my simple example. I realized this must be some statistics property I‚Äôd learned about and since forgotten, so I decided to write out what I was doing in expectations.&lt;/p&gt;
&lt;p&gt;First, I wrote down the unadjusted group means in the form of an expectation. I wrote down a conditional expectation, since we are looking at the mean of &lt;code&gt;bill_length_mm&lt;/code&gt; when &lt;code&gt;species&lt;/code&gt; is restricted to a certain category. We can explicitly show this by taking the expectation of a random variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Bill Length}\)&lt;/span&gt;, while setting another random variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;, equal to only one category at a time.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Adelie]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Chinstrap]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Gentoo]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;More generally, we could write out the unadjusted group mean using a group indicator variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;, which can take on all possible values &lt;span class=&#34;math inline&#34;&gt;\(species\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=species]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So that‚Äôs our unadjusted group means. What about the adjusted group mean? We can start by writing out the linear regression model, which is the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{BillLength}\)&lt;/span&gt;, conditional on the random variables &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{BillDepth}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When I used the &lt;code&gt;predict&lt;/code&gt; function on the fit of that linear regression model, I obtained the fitted values from that expectation, before I separated the fitted values by group to get the grouped means. We can see those fitted values as random variables themselves, and write out another conditional mean using a group indicator variable, just as we did for the unadjusted group means earlier.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]|\mathrm{Species}=species]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;My table of unadjusted and adjusted Bill Length means thus showed me that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]|\mathrm{Species}=species] \\ = E[\mathrm{BillLength}|\mathrm{Species}=species]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Or, in more general notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[Y|X,Z]|Z=z] = E[Y|Z=z]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Is it true?! Spoiler alert ‚Äì yes. Let‚Äôs work through the steps of the proof one by one.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;proof-set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Proof set-up&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Let‚Äôs pretend for the proof that both our &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (outcome), &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (adjustment variable), and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; (grouping variable) are categorical (discrete) variables. This is just to make the math a bit cleaner, since the expectation of a discrete variable (a weighted summation) is a little easier to show than the expectation of a continuous variable (the integral of a probability density function times the realization of the random variable).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A few fundamental expectation results we‚Äôll need:&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;conditional-probability&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Conditional probability&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A|B) = \frac{P(A ‚à© B)}{P(B)}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partition-theorem&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Partition theorem&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[A|B] = \sum_Ba \cdot P(A=a|B=b)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-distribution-from-a-joint-distribution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Marginal distribution from a joint distribution&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_A\sum_Ba\cdot P(A=a,B=b) = \sum_Aa\sum_B\cdot P(A=a,B=b) = \sum_Aa\cdot P(A=a)=E[A]\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-by-step-proof&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step-by-step Proof&lt;/h1&gt;
&lt;p&gt;Click on the superscript number after each step for more information.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=E[E[Y|X,Z=z]|Z=z]\)&lt;/span&gt; &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}E[Y|X=x,Z=z]\cdot P(X=x|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y P(Y=y|X=x,Z=z)\cdot P(X=x|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y \frac{P(Y=y,X=x,Z=z)}{P(X=x,Z=z)}\cdot \frac{P(X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y \frac{P(Y=y,X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y\sum_{X}\frac{P(Y=y,X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y\frac{P(Y=y,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y P(Y=y|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=E[Y|Z=z]\)&lt;/span&gt; &lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, we‚Äôve proved that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z] = E[Y|Z=z]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which, thankfully, means I have an answer to my function output confusion. It was a lightbulb moment for me to realize I should think of an inner expectation as a random variable, and all the rules I learned about conditional and iterated expectations can be revisited in the regressions I fit on a daily basis.&lt;/p&gt;
&lt;p&gt;Here‚Äôs hoping you too feel inspired to revisit probability theory from time to time, even if your work is very applied. It is, after all, a perfect activity for social distancing! üò∑&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0090081&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0090081&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.math.arizona.edu/~tgk/464_07/cond_exp.pdf&#34;&gt;A Conditional Expectation - Arizona Math&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Because we‚Äôre making our outer expectation conditional on &lt;span class=&#34;math inline&#34;&gt;\(Z=z\)&lt;/span&gt;, we can also move &lt;span class=&#34;math inline&#34;&gt;\(Z=z\)&lt;/span&gt; into our inner expectation. This becomes obvious in the &lt;code&gt;penguins&lt;/code&gt; example, since we only use the fitted values from one category of &lt;code&gt;species&lt;/code&gt; to get the adjusted group mean for that category.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;We can rewrite &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z=z]\)&lt;/span&gt; as the weighted summation of all possible values &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can take. &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z=z]\)&lt;/span&gt; will only ever be able to take values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; that vary over the range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; since our value &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is already fixed. We can weight each of these possible &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; values by &lt;span class=&#34;math inline&#34;&gt;\(P(X=x|Z=z)\)&lt;/span&gt;, since that‚Äôs the probabilty &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; will take value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; at our already-fixed &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. Thus, we can start to find &lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z=z]|Z=z]\)&lt;/span&gt; by weighting each &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(P(X=x|Z=z)\)&lt;/span&gt; and adding them all up (see Partition Theorem).&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;We can get the expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at each of those possible values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by a similar process as step 2 (weighting each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(P(Y=y|X=x, Z=z)\)&lt;/span&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;By the Law of Conditional Probability, we can rewrite our conditional probabilities as joint distributions.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The denominator of the first fraction cancels out with the numerator of the second fraction.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;We can switch the summations around so that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is outside the summation over all values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. This lets us get the joint distribution of only &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;This is a conditional expectation, written in the form of a joint distribution.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;By the Partition Theorem.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Rewriting the previous equation as an expectation.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
