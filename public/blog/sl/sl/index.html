<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Katherine Hoffman">

  
  
  
    
  
  <meta name="description" content="A Visual Guide Over the winter, I started reading Targeted Learning by Mark van der Laan and Sherri Rose. I was simultaneously learning Adobe InDesign, so I decided to make “cheat sheets” or “visual guides” for some of the chapters. This is the guide I made for Chapter 3: Superlearning by Rose, van der Laan, and Eric Polley:
A high-quality version of this visual guide exists on my Github. It’s ready-to-print on an 8.">

  
  <link rel="alternate" hreflang="en-us" href="/blog/sl/sl/">

  


  

  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.ce53f5d1ff75f9f89bb1c243b514bb45.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-136820093-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/blog/sl/sl/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@rkatlady">
  <meta property="twitter:creator" content="@rkatlady">
  
  <meta property="og:site_name" content="KHstats">
  <meta property="og:url" content="/blog/sl/sl/">
  <meta property="og:title" content="Become a superlearner! A visual guide and introductory tutorial for superlearning | KHstats">
  <meta property="og:description" content="A Visual Guide Over the winter, I started reading Targeted Learning by Mark van der Laan and Sherri Rose. I was simultaneously learning Adobe InDesign, so I decided to make “cheat sheets” or “visual guides” for some of the chapters. This is the guide I made for Chapter 3: Superlearning by Rose, van der Laan, and Eric Polley:
A high-quality version of this visual guide exists on my Github. It’s ready-to-print on an 8."><meta property="og:image" content="/img/icon-192.png">
  <meta property="twitter:image" content="/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-08-29T21:13:14-05:00">
    
    <meta property="article:modified_time" content="2020-08-29T21:13:14-05:00">
  

  


  





  <title>Become a superlearner! A visual guide and introductory tutorial for superlearning | KHstats</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">KHstats</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#hero"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience"><span>Experience</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#blogs"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Become a superlearner! A visual guide and introductory tutorial for superlearning</h1>

  

  
    



<meta content="2020-08-29 21:13:14 -0500 -0500" itemprop="datePublished">
<meta content="2020-08-29 21:13:14 -0500 -0500" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Aug 29, 2020</time>
  </span>
  

  

  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/r/">R</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/blog/sl/sl/&amp;text=Become%20a%20superlearner!%20A%20visual%20guide%20and%20introductory%20tutorial%20for%20superlearning" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/blog/sl/sl/&amp;t=Become%20a%20superlearner!%20A%20visual%20guide%20and%20introductory%20tutorial%20for%20superlearning" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Become%20a%20superlearner!%20A%20visual%20guide%20and%20introductory%20tutorial%20for%20superlearning&amp;body=/blog/sl/sl/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/blog/sl/sl/&amp;title=Become%20a%20superlearner!%20A%20visual%20guide%20and%20introductory%20tutorial%20for%20superlearning" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Become%20a%20superlearner!%20A%20visual%20guide%20and%20introductory%20tutorial%20for%20superlearning%20/blog/sl/sl/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/blog/sl/sl/&amp;title=Become%20a%20superlearner!%20A%20visual%20guide%20and%20introductory%20tutorial%20for%20superlearning" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<div id="a-visual-guide" class="section level1">
<h1>A Visual Guide</h1>
<p>Over the winter, I started reading <a href="https://www.springer.com/gp/book/9781441997814">Targeted Learning</a> by Mark van der Laan and Sherri Rose. I was simultaneously learning Adobe InDesign, so I decided to make “cheat sheets” or “visual guides” for some of the chapters. This is the guide I made for Chapter 3: Superlearning by Rose, van der Laan, and Eric Polley:</p>
<p><img src="/img/sl.png" /></p>
<p>A high-quality version of this visual guide exists on my <a href="https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/Superlearning.pdf">Github</a>. It’s ready-to-print on an 8.5x11" paper, should you wish to decorate your desk area with it, post-pandemic.</p>
<p>The guide is meant to be a step-by-step walkthrough of how the superlearner algorithm weights individual “learners” (or regressions, or statistical models, whatever terminology you’re most comfortable using).</p>
</div>
<div id="supercuts-of-superlearning" class="section level1">
<h1>Supercuts of superlearning</h1>
<ul>
<li><p><strong>Superlearning</strong> is a technique for prediction that involves <strong>combining many individual statistical algorithms</strong> (commonly called “data-adaptive” or “machine learning” algorithms) to <strong>create a new, single prediction algorithm that is at least as good as any of the individual algorithms</strong>.</p></li>
<li><p>The superlearner algorithm “decides” how to combine, or weight, the individual algorithms based upon how well each one <strong>minimizes a specified loss function</strong>, for example, the mean squared error (MSE). This is done using cross-validation to avoid overfitting.</p></li>
<li><p>The motivation for this type of “ensembling” is that <strong>no single algorithm is always the best for all kinds of data</strong>. For example, sometimes a penalized regression may be best, but in other situations a random forest may be superior. Even extreme gradient boosting is not <em>always</em> ideal! Superlearning allows you to use the beneficial information each algorithm provides to ultimately optimize your prediction capabilities.</p></li>
<li><p>Superlearning (or variations of it) is also called stacking, stacked generalizations, or weighted ensembling by different specializations within the realms of statistics and data science.</p></li>
</ul>
<p><img src="/img/spiderman_meme.jpg" /></p>
</div>
<div id="superlearning-step-by-step" class="section level1">
<h1>Superlearning, step by step</h1>
<p>In this tutorial I will go through the algorithm one step at a time using a simulated data set. It is a modified version of Maya Peterson and Laura Balzar’s <a href="https://99816be4-a59d-4405-9b7d-342a3eb577a3.filesusr.com/ugd/4cecb1_e95abbcfca3b473a982ff059c696e879.pdf">Superlearning lab</a>, which you can try out if you want more practice.</p>
<p>For simplicity I’ll show the concept of superlearning using only four variables (AKA features or predictors) to predict a continuous outcome. The individual algorithms or <strong>base learners</strong> that we will use will be three different linear regressions. I am <em>only</em> using the linear regressions so that code for running more complicated algorithms does not take away from understanding the general superlearning algorithm.</p>
<p>The same code could be translated to a more complicated set, or <strong>superlearner library</strong>, of base learners. For example, instead of three linear regressions, we could use a least absolute shrinkage estimator (LASSO), random forest, multivariate adaptive splines (MARS), and any other statistical learning algorithm equipped to handle continuous outcomes. The only limit to the number of base learners in a superlearning library is computational time and power.</p>
<div id="step-0-load-libraries-and-simulate-data" class="section level2">
<h2>Step 0: Load libraries and simulate data</h2>
<p>To start this example, we will simulate a continuous outcome, <code>y</code>, and four variables for prediction, <code>w1</code>, <code>w2</code>, <code>w3</code>, and <code>w4</code>.</p>
<pre class="r"><code>library(dplyr, warn.conflicts = F)
library(knitr)</code></pre>
<pre class="r"><code>n &lt;- 1000
obs &lt;- tibble::tibble(
  id = 1:n,
  w1 = rnorm(n),
  w2 = rbinom(n, 1, plogis(10*w1)),
  w3 = rbinom(n, 1, plogis(w1*w2 + .5*w2)),
  w4 = rnorm(n, mean=w1*w2, sd=.5*w3),
  y = w1 + w2 + w2*w3 + sin(w4)
)
kable(head(obs), digits=3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">id</th>
<th align="right">w1</th>
<th align="right">w2</th>
<th align="right">w3</th>
<th align="right">w4</th>
<th align="right">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.367</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0.246</td>
<td align="right">2.611</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.480</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0.164</td>
<td align="right">2.643</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">-0.632</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.000</td>
<td align="right">-0.632</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">-0.115</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">-0.762</td>
<td align="right">-0.805</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">1.834</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1.834</td>
<td align="right">3.800</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.089</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">-0.673</td>
<td align="right">1.466</td>
</tr>
</tbody>
</table>
</div>
<div id="step-1-split-the-data-into-k-folds" class="section level2">
<h2>Step 1: Split the data into K folds</h2>
<p>The superlearner algorithm relies on K-fold cross-validation (CV) to avoid building a prediction algorithm that is overfit to the data. We will start this process by splitting the data into 10 blocks, or folds. The easiest way to do this is by creating indices for each cross-validation fold.</p>
<pre class="r"><code>k &lt;- 10 # 10 fold cv
cv_index &lt;- sample(rep(1:k, each = n/k)) # indices for each fold, same length as our dataset `obs`</code></pre>
</div>
<div id="step-2-fit-base-learners-for-first-cv-fold" class="section level2">
<h2>Step 2: Fit base learners for first CV-fold</h2>
<p>Recall that in K-fold CV, each fold serves as the validation set one time. In this first round of CV, we will train all of our base learners (the three linear regressions) on all the cross-validation folds (k = 1,2,…) <em>except</em> for the very last one: <code>cv_index == 10</code>.</p>
<pre class="r"><code>cv_train_1 &lt;- obs[-which(cv_index == 10),] # make a data set that contains all observations except those in k=1
fit_1a &lt;- glm(y ~ w1 + w2 + w3 + w4, data=cv_train_1) # fit the first linear regression on that training data
fit_1b &lt;- glm(y ~ w1 + w2 + w3 + w4:w1, data=cv_train_1) # second LR fit on the training data
fit_1c &lt;- glm(y ~ w1:w2:w3, data=cv_train_1) # and the third LR</code></pre>
</div>
<div id="step-3-obtain-predictions-for-first-cv-fold" class="section level2">
<h2>Step 3: Obtain predictions for first CV-fold</h2>
<p>We can then get use our validation data, <code>cv_index == 10</code>, to obtain our first set of cross-validated predictions.</p>
<pre class="r"><code>cv_valid_1 &lt;- obs[which(cv_index == 10),] # make a data set that only contains observations except in k=10
pred_1a &lt;- predict(fit_1a, newdata = cv_valid_1) # use that data set as the validation for all the models in the SL library
pred_1b &lt;- predict(fit_1b, newdata = cv_valid_1)
pred_1c &lt;- predict(fit_1c, newdata = cv_valid_1)</code></pre>
<p>Since we have 1000 <code>obs</code>ervations, that gives us three vectors of length 100: a set of predictions for each of our learners a, b, and c.</p>
<pre class="r"><code>length(pred_1a) # double check we only have n/k predictions ...we do :-)</code></pre>
<pre><code>## [1] 100</code></pre>
<pre class="r"><code>knitr::kable(head(cbind(pred_1a, pred_1b, pred_1c)))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">pred_1a</th>
<th align="right">pred_1b</th>
<th align="right">pred_1c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4.1089782</td>
<td align="right">4.2289117</td>
<td align="right">4.7496320</td>
</tr>
<tr class="even">
<td align="right">2.4439492</td>
<td align="right">2.4709011</td>
<td align="right">0.7393092</td>
</tr>
<tr class="odd">
<td align="right">2.8384084</td>
<td align="right">2.6950207</td>
<td align="right">1.2522189</td>
</tr>
<tr class="even">
<td align="right">-0.0452288</td>
<td align="right">0.1248063</td>
<td align="right">0.2018409</td>
</tr>
<tr class="odd">
<td align="right">-0.0202547</td>
<td align="right">0.2834291</td>
<td align="right">0.2018409</td>
</tr>
<tr class="even">
<td align="right">4.9380419</td>
<td align="right">4.4531422</td>
<td align="right">5.3780037</td>
</tr>
</tbody>
</table>
</div>
<div id="step-4-obtain-cv-predictions-for-entire-data-set" class="section level2">
<h2>Step 4: Obtain CV predictions for entire data set</h2>
<p>We’ll want to get those predictions for <em>every</em> fold. So, using your favorite <code>for</code> loop, <code>apply</code> statement, or <code>map</code>ping function, fit the base learners and obtain predictions for each of them, so that there are 1000 predictions – one for every point in <code>obs</code>ervations.</p>
<pre class="r"><code>cv_folds &lt;- as.list(1:k)
names(cv_folds) &lt;- paste0(&quot;fold&quot;,1:k)
cv_preds &lt;-
  # map_dfr loops through every fold (1:k) and binds the rows of the listed results together
  purrr::map_dfr(cv_folds, function(x){
  # make a training data set that contains all data except fold k
  cv_train &lt;- obs[-which(cv_index == x),]
  # fit all the base learners to that data
  fit_a &lt;- glm(y ~ w2 + w4, data=cv_train)
  fit_b &lt;- glm(y ~ w1 + w2 + w1*w3 + sin(w4), data=cv_train)
  fit_c &lt;- glm(y ~ w1:w2:w3, data=cv_train)
  # make a validation data set that only contains data from fold k
  cv_valid &lt;- obs[which(cv_index == x),]
  # obtain predictions from all the base learners for that validation data
  pred_a &lt;- predict(fit_a, newdata = cv_valid)
  pred_b &lt;- predict(fit_b, newdata = cv_valid)
  pred_c &lt;- predict(fit_c, newdata = cv_valid)
  # save the predictions and the ids of the observations in a data frame
  return(tibble::tibble(&quot;obs_id&quot; = cv_valid$id, &quot;cv_fold&quot; = x, pred_a, pred_b, pred_c))
})

kable(head(cv_preds), digits=3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">obs_id</th>
<th align="right">cv_fold</th>
<th align="right">pred_a</th>
<th align="right">pred_b</th>
<th align="right">pred_c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">14</td>
<td align="right">1</td>
<td align="right">2.866</td>
<td align="right">3.093</td>
<td align="right">1.943</td>
</tr>
<tr class="even">
<td align="right">28</td>
<td align="right">1</td>
<td align="right">2.112</td>
<td align="right">2.272</td>
<td align="right">1.461</td>
</tr>
<tr class="odd">
<td align="right">30</td>
<td align="right">1</td>
<td align="right">-1.093</td>
<td align="right">-0.735</td>
<td align="right">0.248</td>
</tr>
<tr class="even">
<td align="right">41</td>
<td align="right">1</td>
<td align="right">3.320</td>
<td align="right">3.666</td>
<td align="right">2.674</td>
</tr>
<tr class="odd">
<td align="right">82</td>
<td align="right">1</td>
<td align="right">-0.556</td>
<td align="right">-3.301</td>
<td align="right">0.248</td>
</tr>
<tr class="even">
<td align="right">86</td>
<td align="right">1</td>
<td align="right">2.934</td>
<td align="right">2.972</td>
<td align="right">1.539</td>
</tr>
</tbody>
</table>
</div>
<div id="step-5-choose-and-compute-a-loss-function-of-interest-via-a-metalearner" class="section level2">
<h2>Step 5: Choose and compute a loss function of interest via a metalearner</h2>
<p>Okay, so we have a cross-validated prediction for every observation in the data set. Now we want to merge those CV predictions back into our main data set, and compute a loss function of interest for each CV prediction. This is how we’re going to optimize our prediction algorithm: we want to make sure we’re “losing the least” when we use it to eventually make predictions.</p>
<div id="optional-step-for-demonstration-manually-computing-the-mse" class="section level3">
<h3><em>Optional step for demonstration</em>: manually computing the MSE</h3>
<p>Let’s say we have chosen our loss function of interest to be the Mean Squared Error (MSE). We could first compute the squared error <span class="math inline">\((y - \hat{y})^2\)</span> for each CV prediction a, b, and c.</p>
<pre class="r"><code>obs_preds &lt;- 
  full_join(obs, cv_preds, by=c(&quot;id&quot; = &quot;obs_id&quot;)) %&gt;%
  mutate(cv_sqrd_error_a = (y-pred_a)^2,
         cv_sqrd_error_b = (y-pred_b)^2,
         cv_sqrd_error_c = (y-pred_c)^2
        )

obs_preds %&gt;% select(contains(&quot;sqrd_error&quot;)) %&gt;% head() %&gt;% kable(digits=3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">cv_sqrd_error_a</th>
<th align="right">cv_sqrd_error_b</th>
<th align="right">cv_sqrd_error_c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.027</td>
<td align="right">0.017</td>
<td align="right">1.915</td>
</tr>
<tr class="even">
<td align="right">0.070</td>
<td align="right">0.013</td>
<td align="right">1.212</td>
</tr>
<tr class="odd">
<td align="right">0.022</td>
<td align="right">0.014</td>
<td align="right">0.679</td>
</tr>
<tr class="even">
<td align="right">0.684</td>
<td align="right">0.036</td>
<td align="right">0.995</td>
</tr>
<tr class="odd">
<td align="right">0.209</td>
<td align="right">0.015</td>
<td align="right">12.842</td>
</tr>
<tr class="even">
<td align="right">0.007</td>
<td align="right">0.029</td>
<td align="right">1.016</td>
</tr>
</tbody>
</table>
<p>And then take the mean of those three cross-validated squared error columns, grouped by <code>cv_fold</code>, to get the CV-MSE for each fold.</p>
<pre class="r"><code>cv_risks &lt;-
  obs_preds %&gt;%
  group_by(cv_fold) %&gt;%
  summarise(cv_mse_a = mean(cv_sqrd_error_a),
            cv_mse_b = mean(cv_sqrd_error_b),
            cv_mse_c = mean(cv_sqrd_error_c)
            )</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre class="r"><code>kable(cv_risks, digits=3, caption = &quot;CV-MSE for each CV fold for each base learner&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-9">Table 1: </span>CV-MSE for each CV fold for each base learner</caption>
<thead>
<tr class="header">
<th align="right">cv_fold</th>
<th align="right">cv_mse_a</th>
<th align="right">cv_mse_b</th>
<th align="right">cv_mse_c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.421</td>
<td align="right">0.021</td>
<td align="right">2.125</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.381</td>
<td align="right">0.024</td>
<td align="right">2.000</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.392</td>
<td align="right">0.022</td>
<td align="right">1.723</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.335</td>
<td align="right">0.020</td>
<td align="right">1.606</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.391</td>
<td align="right">0.025</td>
<td align="right">2.194</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.351</td>
<td align="right">0.021</td>
<td align="right">1.528</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.351</td>
<td align="right">0.023</td>
<td align="right">1.546</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">0.445</td>
<td align="right">0.025</td>
<td align="right">1.971</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">0.439</td>
<td align="right">0.029</td>
<td align="right">2.392</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">0.356</td>
<td align="right">0.025</td>
<td align="right">1.622</td>
</tr>
</tbody>
</table>
<p>We can take the mean of those (or just take the mean without grouping by CV fold in the first place!) to get the overall CV-MSE for each learner.</p>
<pre class="r"><code>cv_risks %&gt;%
  select(-cv_fold) %&gt;%
  summarise_all(mean) %&gt;%
  kable(digits=3, caption = &quot;CV-MSE for each base learner&quot;)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-10">Table 2: </span>CV-MSE for each base learner</caption>
<thead>
<tr class="header">
<th align="right">cv_mse_a</th>
<th align="right">cv_mse_b</th>
<th align="right">cv_mse_c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.386</td>
<td align="right">0.023</td>
<td align="right">1.871</td>
</tr>
</tbody>
</table>
<p>We see that in this simple example, the base learner that performs the best using our chosen loss function of interest is learner b. In other words, if we had to choose one of the statistical learning algorithms in our library to be our prediction model based on the CV predictions, we “lose” the least, or we “take the smallest risk,” with learner b.</p>
<p><strong>We <em>could</em> stop here and fit learner b to our full data set, and that would be called using the <em>discrete superlearner</em>.</strong></p>
<pre class="r"><code>discrete_sl_predictions &lt;- predict(glm(y ~ w1 + w2 + w1*w3 + sin(w4), data=obs))</code></pre>
<p>However, we can almost always create an even better prediction algorithm if we use information from <em>all</em> of the algorithms’ CV predictions to create a new algorithm.</p>
</div>
<div id="using-a-metalearner-to-obtain-the-weights" class="section level3">
<h3>Using a metalearner to obtain the weights</h3>
<blockquote>
<p>This is the key step of the superlearner algorithm: we will use a new learner, a <strong>metalearner</strong>, to take information from all of the base learners and create that new algorithm.</p>
</blockquote>
<p>For simplicity, we’ll use yet another linear regression as our metalearner. The metalearner <em>could</em> be a more complicated statistical learning algorithm (LASSO, random forest, etc.), and in fact the default in the <code>SuperLearner</code> package is actually non-negative least squares (NNLS). For more information on non-negative least squares, see the <a href="#Appendix">Appendix</a>.</p>
<p>As with any statistical learning algorithm, the choice of metalearner reflects a loss function of interest. No matter what metalearner we choose, the predictors will always be the cross-validated predictions from each base learner, and the outcome will always be the true outcome, <code>y</code>.</p>
<pre class="r"><code>metalrnr_fit &lt;- glm(y ~ pred_a + pred_b + pred_c, data = obs_preds)
summary(metalrnr_fit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ pred_a + pred_b + pred_c, data = obs_preds)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.39353  -0.11431   0.02516   0.11858   0.38558  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.009485   0.005967   1.590  0.11221    
## pred_a      -0.037757   0.008665  -4.357 1.45e-05 ***
## pred_b       1.043014   0.009161 113.857  &lt; 2e-16 ***
## pred_c      -0.013737   0.004635  -2.964  0.00311 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.02303216)
## 
##     Null deviance: 4588.53  on 999  degrees of freedom
## Residual deviance:   22.94  on 996  degrees of freedom
## AIC: -926.99
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>This metalearner provides us with the coefficients, or weights, to apply to each of the base learners.</p>
</div>
</div>
<div id="step-6-fit-the-base-learners-on-the-entire-data-set" class="section level2">
<h2>Step 6: Fit the base learners on the entire data set</h2>
<p>We now have our superlearner algorithm, so it is time to input data and obtain predictions! To implement the algorithm and obtain final predictions, we first need to fit the base learners on the whole data set.</p>
<pre class="r"><code>fit_a &lt;- glm(y ~ w2 + w4, data=obs)
fit_b &lt;- glm(y ~ w1 + w2 + w1*w3 + sin(w4), data=obs)
fit_c &lt;- glm(y ~ w1:w2:w3, data=obs)</code></pre>
</div>
<div id="step-7-extract-predictions-from-each-base-learner-on-the-entire-data-set" class="section level2">
<h2>Step 7: Extract predictions from each base learner on the entire data set</h2>
<p>We’ll use <em>those</em> base learner fits to get predictions from each of the base learners for the entire data set, and then we will plug those predictions into the metalearner fit. Remember, we were previously using <em>cross-validated</em> predictions, rather than fitting the base learners on the entire data set. This was to avoid overfitting.</p>
<pre class="r"><code>pred_a &lt;- predict(fit_a)
pred_b &lt;- predict(fit_b)
pred_c &lt;- predict(fit_c)
full_data_preds &lt;- tibble(pred_a, pred_b, pred_c)</code></pre>
</div>
<div id="step-8-use-the-metalearner-fit-to-weight-the-base-learners" class="section level2">
<h2>Step 8: Use the metalearner fit to weight the base learners</h2>
<p>Once we have the predictions from the full data set, we can input them to the metalearner, where the output will be a final prediction for <code>y</code>.</p>
<pre class="r"><code>sl_predictions &lt;- predict(metalrnr_fit, newdata = full_data_preds)
kable(head(sl_predictions))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2.4873894</td>
</tr>
<tr class="even">
<td align="right">2.5374155</td>
</tr>
<tr class="odd">
<td align="right">-0.7542537</td>
</tr>
<tr class="even">
<td align="right">-0.5762125</td>
</tr>
<tr class="odd">
<td align="right">3.7028332</td>
</tr>
<tr class="even">
<td align="right">1.3057765</td>
</tr>
</tbody>
</table>
<p>And… that’s it! Those are our superlearner predictions.</p>
</div>
<div id="step-9-and-beyond" class="section level2">
<h2>Step 9 and beyond…</h2>
<p>We could compute the MSE of the ensemble superlearner predictions.</p>
<pre class="r"><code>sl_mse &lt;- mean((obs$y - sl_predictions)^2)
sl_mse</code></pre>
<pre><code>## [1] 0.02257862</code></pre>
<p>We could also add more algorithms to our base learner library (we definitely should, since we only used linear regressions!), and we could write functions to tune these algorithms’ hyperparameters over various grids. For example, if we were to include random forest in our library, we may want to tune over a number of trees and maximum bucket sizes.</p>
<p>We can then cross-validate this entire process to evaluate the predictive performance of the new ensemble superlearner algorithm. Alternatively, we could leave a hold out training data set to evaluate the performance.</p>
</div>
</div>
<div id="using-the-superlearner-package" class="section level1">
<h1>Using the <code>SuperLearner</code> package</h1>
<p>We could use the <code>SuperLearner</code> package and avoid all that hand-coding. Here is how you would specify an ensemble superlearner for our data with the base learner libraries of <code>ranger</code> (random forests), <code>glmnet</code> (LASSO, by default), and <code>earth</code> (MARS).</p>
<pre class="r"><code>library(SuperLearner)
x_df &lt;- obs %&gt;% select(w1:w4) %&gt;% as.data.frame()
sl_fit &lt;- SuperLearner(Y = obs$y, X = x_df, family = gaussian(),
                     SL.library = c(&quot;SL.ranger&quot;, &quot;SL.glmnet&quot;, &quot;SL.earth&quot;))</code></pre>
<p>Not so bad, right? You can specify the metalearner with the <code>method</code> argument. The default is non-negative least squares. For more information on the <code>SuperLearner</code> package, check out this <a href="https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html">vignette</a>.</p>
<p>We can examine the cross-validated <code>Risk</code> (loss function), and the <code>Coef</code>ficient (weight) given to each of the models.</p>
<pre class="r"><code>sl_fit</code></pre>
<pre><code>## 
## Call:  
## SuperLearner(Y = obs$y, X = x_df, family = gaussian(), SL.library = c(&quot;SL.ranger&quot;,  
##     &quot;SL.glmnet&quot;, &quot;SL.earth&quot;)) 
## 
## 
##                      Risk      Coef
## SL.ranger_All 0.018112617 0.1576503
## SL.glmnet_All 0.105417536 0.0000000
## SL.earth_All  0.004177706 0.8423497</code></pre>
</div>
<div id="other-superlearning-packages" class="section level1">
<h1>Other superlearning packages</h1>
<p>Other <code>R</code> packages besides <code>SuperLearner</code> that can be used to implement the superlearner algorithm include <code>sl3</code> (an update to the older <code>Superlearner</code> package), <code>ml3</code>, <code>h2o</code>, and <code>caretEnsemble</code>. I wrote a <a href="https://www.khstats.com/blog/sl3_demo/sl/">brief tutorial</a> on using <code>sl3</code> for an NYC R-Ladies meetup.</p>
<p>Python aficionados might find this <a href="https://machinelearningmastery.com/super-learner-ensemble-in-python/">blog post</a> useful.</p>
</div>
<div id="a-review" class="section level1">
<h1>A Review</h1>
<p>The steps of the superlearner algorithm are summarized nicely in this graphic in the <em>Targeted Learning</em> book:</p>
<p><img src="/img/sl_diagram.png" /></p>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<div id="non-negative-least-squares-regression" class="section level2">
<h2>Non-negative least squares regression</h2>
<p>The default in the <code>SuperLearner</code> package is non-negative least square regression for continuous variables. The non-negative least squares is exactly the same as “regular” linear least squares regression, but instead of just solving for the coefficients that yield the minimum hypotenuse distance between X and Y for a whole matrix (in other words, the <em>Euclidian distance</em>), you constrain your solution to only allow <em>positive</em> coefficients to minimize that distance.</p>
<p>It’s actually very intuitive to have a regression that doesn’t allow negative coefficients if you’re trying to “weight” an ensemble of predictions! For a manually superlearner algorithm that implements the <code>nnls</code> function for the metalearner, see this <a href="https://github.com/ainaimi/SuperLearnerIntro">introductory tutorial code</a>.</p>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Polley, Eric. “Chapter 3: Superlearning.” Targeted Learning: Causal Inference for Observational and Experimental Data, by M. J. van der. Laan and Sherri Rose, Springer, 2011.</p>
<p>Polley E, LeDell E, Kennedy C, van der Laan M. Super Learner: Super Learner Prediction. 2016 URL <a href="https://CRAN.R-project.org/package=SuperLearner" class="uri">https://CRAN.R-project.org/package=SuperLearner</a>. R package version 2.0-22.</p>
<p>Naimi AI, Balzer LB. Stacked generalization: an introduction to super learning. <em>Eur J Epidemiol.</em> 2018;33(5):459-464. <a href="doi:10.1007/s10654-018-0390-z" class="uri">doi:10.1007/s10654-018-0390-z</a></p>
</div>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/r/">R</a>
  
</div>



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hub64370a9fa8c7276c9d6cc9d2e350075_235979_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/">Katherine Hoffman</a></h5>
      <h6 class="card-subtitle">Research Biostatistician II</h6>
      <p class="card-text" itemprop="description">I am passionate about meaningful, reproducible medical research.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/Rkatlady" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/hoffmakl" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/blog/corr-plots/corr-plots/">Customizable correlation plots in R</a></li>
          
          <li><a href="/blog/iterated-expectations/iterated-expectations/">Understanding Conditional and Iterated Expectations with a Linear Regression Model</a></li>
          
          <li><a href="/blog/trt-timelines/trt-timelines/">Patient Treatment Timelines for Longitudinal Survival Data</a></li>
          
          <li><a href="/blog/sl3_demo/sl/">A short and sweet tutorial on using `sl3` for superlearning</a></li>
          
          <li><a href="/talk/power-sims/">Power Simulations in R</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.16bbb3750feb7244c9bc409a5a4fe678.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
        
  </p>
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
