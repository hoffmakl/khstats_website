<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics | KHstats</title>
    <link>/tags/statistics/</link>
      <atom:link href="/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 13 Oct 2020 21:13:14 -0500</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>statistics</title>
      <link>/tags/statistics/</link>
    </image>
    
    <item>
      <title>Whatcha measuring?? Collider Bias Examples in R</title>
      <link>/blog/collider-bias/race/</link>
      <pubDate>Tue, 13 Oct 2020 21:13:14 -0500</pubDate>
      <guid>/blog/collider-bias/race/</guid>
      <description>


&lt;p&gt;Anyways, I wholeheartedly agree with this. I find many of the tools in causal inference (specifically Directed Acyclic Graphs) to be incredibly useful in recognizing what I &lt;em&gt;cannot&lt;/em&gt; unbiasedly estimate.&lt;/p&gt;
&lt;p&gt;I am an expert lurker on the statistics twitter (war) threads is a favorite pasttime of mine, so I was admittedly a bit sad it didn’t gain more steam.&lt;/p&gt;
&lt;p&gt;So now your investigator wants to know the attributable mortality of race once someone is hospitalized. “We don’t want your causal inference, we just want the &lt;em&gt;independent association&lt;/em&gt;!” they tell you emphatically. “We &lt;em&gt;PROMISE&lt;/em&gt; we won’t talk about anything causally!” You think about Miguel Hernan’s &lt;em&gt;The C-Word&lt;/em&gt; paper. You don’t believe them at all, but you let it go, for a moment.&lt;/p&gt;
&lt;p&gt;Let’s start with simulating race. Let’s say our population of interest is approximately 50% of one race and 50% of another. We’ll simulate that with a binomial distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;race &amp;lt;- rbinom(n, 1, 0.5) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll simulate COVID-19 disease severity. Although there is some evidence suggesting COVID-19 disease severity is not random (i.e. it could be related to viral load, blood type, etc.), let’s assume for the sake of example that it is completely random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disease_severity &amp;lt;- rbinom(n, 1, 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s think about a common adjustment scientists use to reduce confounding of race: socioeconomic status (SES). SES is unarguably affected by race. Again, for simplicity, let’s pretend SES is a binary outcome. It could represent if a person with COVID-19 lives “above” or “below” the poverty line. We’ll simulate SES as a binomial distribution where the probability someone will be above the poverty line is affected by their race.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ses &amp;lt;- rbinom(n, 1, plogis(-race))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve made our population, so now let’s figure out whether we’re actually going to get to see each person’s data. We only have access to COVID-19 patients who were hospitalized. Whether or not someone is hospitalized is affected by their disease severity, of course.&lt;/p&gt;
&lt;p&gt;However, let’s say for our example that hospitalization status is also affected by a person’s SES (whether or not they have insurance may affect their decision to go to the hospital, for example). It also may be affected by some part of the social construct that we use to define race/ethnicity. If individuals in one race/ethnicity are more or less likely to go to the hospital than individuals of another race/ethnicity, then that will affect hospitalization. This is very reasonable in COVID-19 (and in other diseases) where Black and Hispanic persons are disproportionately dying. If a person knows a family member or friend who is on a …&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hospitalized &amp;lt;- rbinom(n, 1, plogis(-3 + 2 * disease_severity + 2 * ses + 3 * race))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we simulate our outcome. Let’s keep it simple and say whether or not someone dies is directly affected by disease severity and whether or not the person is cared for in a hospital:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;death &amp;lt;- rbinom(n, 1, plogis(-(2 * disease_severity + hospitalized)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s put this simulated data into a data frame. I’m going to relabel the data with interpretable to make it clearer how harmful doing these analyses can be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_df &amp;lt;-
  data.frame(
  race = factor(race, levels=c(1,0), labels=c(&amp;quot;White&amp;quot;,&amp;quot;Black&amp;quot;)),
  disease_severity = factor(disease_severity, levels=c(1,0), labels=c(&amp;quot;Mild&amp;quot;,&amp;quot;Severe&amp;quot;)),
  ses = factor(ses, levels=c(0,1), labels=c(&amp;quot;Above poverty line&amp;quot;, &amp;quot;Below poverty line&amp;quot;)),
  hospitalized = factor(hospitalized, levels=0:1, labels=c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;)),
  death = factor(death, levels=0:1, labels=c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I’ll make a data frame that contains only the patients who were put in the hospital.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hospitalized_df &amp;lt;-
  covid_df %&amp;gt;%
  filter(hospitalized == &amp;quot;Yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The association between race and mortality for people who are hospitalized. Can you get the true association?&lt;/p&gt;
&lt;p&gt;Let’s pretend more a moment that you think you can. You’re a pro at &lt;code&gt;ggplot2&lt;/code&gt; so you whip out a nice bar chart. Look, race and mortality in hospitalized patients! It’s Figure 1 of a paper if I’ve ever seen one!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;race_death_count &amp;lt;-
  hospitalized_df %&amp;gt;%
  group_by(race) %&amp;gt;%
  count(death) %&amp;gt;%
  mutate(sum_n = sum(n)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(prop = n/sum_n)

race_death_count %&amp;gt;%
  filter(death == &amp;quot;Yes&amp;quot;) %&amp;gt;%
  ggplot(aes(x=race, y=prop)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  theme_classic() +
  labs(x=&amp;quot;Race&amp;quot;,y=&amp;quot;In-hospital Mortality&amp;quot;,title=&amp;quot;Race and Mortality in COVID-19&amp;quot;,subtitle=&amp;quot;Among Hospitalized Patients&amp;quot;) +
  scale_y_continuous(labels = scales::percent_format(), expand=c(0,0), limits=c(0,.25)) +
  geom_text(aes(label=paste(n, &amp;quot;/&amp;quot;, sum_n), y=prop+.01))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/collider-bias/race_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pretty_logistic_table &amp;lt;- function(model_fit) {
  model_fit %&amp;gt;%
    broom::tidy(exponentiate=T, conf.int=T) %&amp;gt;%
    filter(term != &amp;quot;(Intercept)&amp;quot;) %&amp;gt;%
     mutate( term = case_when(term == &amp;quot;raceBlack&amp;quot; ~ &amp;quot;Race: Black&amp;quot;,
                             term == &amp;quot;disease_severitySevere&amp;quot; ~ &amp;quot;Disease severity: Severe&amp;quot;,
                             term == &amp;quot;sesBelow poverty line&amp;quot; ~ &amp;quot;SES: Below poverty level&amp;quot;,
                             TRUE ~ term),
           odds_ratio = paste0(round(estimate,1),&amp;quot; (&amp;quot;,round(conf.low,1),&amp;quot;, &amp;quot;,round(conf.high,1),&amp;quot;)&amp;quot;),
           p_value = case_when(p.value &amp;lt; .001 ~ &amp;quot;&amp;lt;.001&amp;quot;,
                               TRUE ~ as.character(round(p.value, 3)))) %&amp;gt;%
             select(term, odds_ratio, p_value) %&amp;gt;%
    gt::gt() %&amp;gt;%
    gt::cols_label(
       term = &amp;quot;Coefficient&amp;quot;,
               odds_ratio = &amp;quot;Odds Ratio (95% CI)&amp;quot;,
               p_value = &amp;quot;P-value&amp;quot;
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;start by just looking at the unadjusted Odds Ratio of &lt;strong&gt;race&lt;/strong&gt; on &lt;strong&gt;mortality&lt;/strong&gt;. Simple enough&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race, data = hospitalized_df, family = binomial()) %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#fefxepmids .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#fefxepmids .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#fefxepmids .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#fefxepmids .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fefxepmids .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#fefxepmids .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#fefxepmids .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#fefxepmids .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#fefxepmids .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#fefxepmids .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#fefxepmids .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#fefxepmids .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#fefxepmids .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#fefxepmids .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#fefxepmids .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#fefxepmids .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#fefxepmids .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#fefxepmids .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#fefxepmids .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#fefxepmids .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#fefxepmids .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#fefxepmids .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#fefxepmids .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#fefxepmids .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#fefxepmids .gt_left {
  text-align: left;
}

#fefxepmids .gt_center {
  text-align: center;
}

#fefxepmids .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#fefxepmids .gt_font_normal {
  font-weight: normal;
}

#fefxepmids .gt_font_bold {
  font-weight: bold;
}

#fefxepmids .gt_font_italic {
  font-style: italic;
}

#fefxepmids .gt_super {
  font-size: 65%;
}

#fefxepmids .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;fefxepmids&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.7 (0.5, 0.9)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.002&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;“we’re not interested in this,”
but what are you interested in, then? what meaning does it hold to conclude that
perhaps you’re going to say that providers don’t have any additional effect on patients; there’s no inherent racism. but is that actually what you can conclude? all your estimates are now biased&lt;/p&gt;
&lt;p&gt;statistician drawing dag
I. don’t. want. causation.
“okay, great!”
I just want to show that within my hospitalized patients
&lt;em&gt;whiteboard, draws H&lt;/em&gt;
there is an effect of race
&lt;em&gt;draws race&lt;/em&gt;
on mortality
&lt;em&gt;draws mortality&lt;/em&gt;
independent of confounders
&lt;em&gt;draws confounders&lt;/em&gt;
WITHIN A HOSPITALIZED POPULATION
&lt;em&gt;draws box&lt;/em&gt;
Yep, still can’t answer that. could be correlated, could not be
THAT’SWHAT I WANT TO WRITE ABOUT
but what are you going to SAY about that?!?!
&#34;This estimate of correlation between race and mortality in a hospitalized population – which could be completely incorrect, either in the null or wrong direction – is evidence of… What??? The point of research is to ask a question you can set up an experiment to answer, and then to answer it as best you can, not ask a question you &lt;em&gt;know&lt;/em&gt; you cannot answer and then conjecture about the number you obtain.&lt;/p&gt;
&lt;p&gt;but everyone knows Black people are set up for failure in the healthcare system, this will only be accumulating evidence
but what if it’s not right, what if you find being black is protective, or being Black has no effect on mortality. Any of these are possible!&lt;/p&gt;
&lt;p&gt;There’s no way there won’t be an effect of race on mortality among hospitalized patients. Race …&lt;/p&gt;
&lt;p&gt;So why are you doing this study?&lt;/p&gt;
&lt;p&gt;So what do you suggest I do?&lt;/p&gt;
&lt;p&gt;I suggest you leave this research question to the people who actually have the population level data on race, hospitalization rates, and mortality, to answer the effect of race on mortality. Or, I suggest you go find the people who have that data and try to team up with them. I’m not saying your question is not important, I’m saying you don’t have the data to correctly answer it, and whatever estimate you come up with will be incorrect, and could do more harm than good.&lt;/p&gt;
&lt;p&gt;Headlines: RACE IS A RISK FACTOR FOR DYING FROM COVID-19 IN HOSPITALIZED PATIENTS&lt;/p&gt;
&lt;p&gt;Fitting a model among the entire population and hospitalized patients only:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race + disease_severity + ses, data=covid_df, family = binomial()) %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#eexirarxtj .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#eexirarxtj .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#eexirarxtj .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#eexirarxtj .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#eexirarxtj .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#eexirarxtj .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#eexirarxtj .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#eexirarxtj .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#eexirarxtj .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#eexirarxtj .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#eexirarxtj .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#eexirarxtj .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#eexirarxtj .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#eexirarxtj .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#eexirarxtj .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#eexirarxtj .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#eexirarxtj .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#eexirarxtj .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#eexirarxtj .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#eexirarxtj .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#eexirarxtj .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#eexirarxtj .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#eexirarxtj .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#eexirarxtj .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#eexirarxtj .gt_left {
  text-align: left;
}

#eexirarxtj .gt_center {
  text-align: center;
}

#eexirarxtj .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#eexirarxtj .gt_font_normal {
  font-weight: normal;
}

#eexirarxtj .gt_font_bold {
  font-weight: bold;
}

#eexirarxtj .gt_font_italic {
  font-style: italic;
}

#eexirarxtj .gt_super {
  font-size: 65%;
}

#eexirarxtj .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;eexirarxtj&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;2 (1.7, 2.3)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Disease severity: Severe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;11.7 (9.8, 14)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;SES: Below poverty level&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.7 (0.6, 0.9)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(death ~ race + disease_severity + ses, data=hospitalized_df, family = binomial())  %&amp;gt;%
  pretty_logistic_table()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#wfcbehcipe .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#wfcbehcipe .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#wfcbehcipe .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#wfcbehcipe .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wfcbehcipe .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#wfcbehcipe .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#wfcbehcipe .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#wfcbehcipe .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#wfcbehcipe .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#wfcbehcipe .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#wfcbehcipe .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#wfcbehcipe .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#wfcbehcipe .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#wfcbehcipe .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#wfcbehcipe .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#wfcbehcipe .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#wfcbehcipe .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#wfcbehcipe .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#wfcbehcipe .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#wfcbehcipe .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#wfcbehcipe .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wfcbehcipe .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#wfcbehcipe .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#wfcbehcipe .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#wfcbehcipe .gt_left {
  text-align: left;
}

#wfcbehcipe .gt_center {
  text-align: center;
}

#wfcbehcipe .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#wfcbehcipe .gt_font_normal {
  font-weight: normal;
}

#wfcbehcipe .gt_font_bold {
  font-weight: bold;
}

#wfcbehcipe .gt_font_italic {
  font-style: italic;
}

#wfcbehcipe .gt_super {
  font-size: 65%;
}

#wfcbehcipe .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;wfcbehcipe&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Coefficient&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Odds Ratio (95% CI)&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;P-value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Race: Black&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1 (0.7, 1.3)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.856&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;Disease severity: Severe&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;9.2 (7, 12.2)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;&amp;lt;.001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;SES: Below poverty level&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1.1 (0.8, 1.4)&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.695&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;We miss the effect of race (through hospitalization) on death.&lt;/p&gt;
&lt;p&gt;There is no effect of race on mortality in hospitalized patients… but there is definitely an effect of race on mortality in the full population, because it affects whether someone gets hospitalized.&lt;/p&gt;
&lt;p&gt;So what are you estimating? What are you going to do with that information?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Visual Guide to the Targeted Maximum Likelihood Estimation (TMLE) Algorithm</title>
      <link>/blog/tmle/intro-to-tmle/</link>
      <pubDate>Sat, 10 Oct 2020 21:13:14 -0500</pubDate>
      <guid>/blog/tmle/intro-to-tmle/</guid>
      <description>


&lt;blockquote&gt;
&lt;p&gt;A beginner’s guide to understanding the Targeted Maximum Likelihood Estimation (TMLE) algorithm.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;/img/TMLE.jpg&#34;
         width=100%&#34;&gt;
&lt;figcaption&gt;
&lt;em&gt;&lt;a href=&#34;&#34;&gt;T&lt;/a&gt;here is a condensed version of this tutorial as an 8.5x11&#34; pdf on my Github in case you would like to print it out for reference as you read more formal TMLE explanations.&lt;/em&gt;
&lt;/figcaption&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;hr /&gt;
&lt;div id=&#34;what-is-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. What is TMLE?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Targeted Maximum Likelihood Estimation (TMLE) is a general semiparametric estimation framework which allows you to &lt;strong&gt;estimate a statistical estimand of interest using&lt;/strong&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;the expected outcome of an individual, given the treatment they received and their baseline confounders&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the probability an individual received the treatment of interest, given their baseline confounders (propensity score)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;These estimates can come from &lt;strong&gt;flexible, data-adaptive statistical models&lt;/strong&gt; we commonly categorize as machine learning, allowing us to place &lt;strong&gt;minimal assumptions on the distribution of the data&lt;/strong&gt;. However, unlike estimates normally obtained from data-adaptive algorithms, the &lt;strong&gt;final TMLE estimate will still have valid standard errors for statistical inference&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;an-ms-level-statisticians-motivation-for-learning-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. An MS-level Statistician’s Motivation for Learning TMLE&lt;/h1&gt;
&lt;p&gt;When I graduated with my MS in Biostatistics two years ago, I had a mental framework of statistics and data science that I think is pretty common among new graduates. It is this idea that we should use flexible &lt;strong&gt;machine learning&lt;/strong&gt; models when the goal is prediction, and interpretable (usually parametric) models when the goal is inference.&lt;/p&gt;
&lt;p&gt;Going into my first job, I had a clear plan: if I was given a data set and told to predict an outcome, I would use “black box” algorithms like random forests or support vector machines and then look at prediction performance metrics and variable importance. If I was instead asked to figure out the &lt;em&gt;effect&lt;/em&gt; of some individual predictor on an outcome, I would fit a model which yielded point estimates with standard errors so I could compute 95% confidence intervals and p-values: logistic regression, Cox Proportional Hazards, and linear mixed effects, to name a few.&lt;/p&gt;
&lt;!-- I&#39;d learned a bit about causal inference during school: enough to know that I might want to first model the  probability of treatment in a logistic regression, and then up or down-weight my observations according to how likely they were to be treated. --&gt;
&lt;!-- This [two-cultures](breiman_two_cultures.pdf) thinking changed when I actually started my first job. My first project was with a statistician, Ivan Diaz, who specializes in semiparametric methods for causal inference. When I asked Ivan for methods guidance, he asked if I&#39;d ever heard of Targeted Maximum Likelihood Estimation (TMLE). Definitely not. --&gt;
&lt;!-- And so began my journey in learning about semiparametric estimation methods and causal inference... --&gt;
&lt;p&gt;This &lt;a href=&#34;breiman_two_cultures.pdf&#34;&gt;two-cultures&lt;/a&gt; mentality changed once I actually started working. I began learning about semiparametric estimation methods in the context of causal inference; TMLE was the first such method I learned. I realized TMLE allows the use of flexible, adaptive models – the ones I always thought were reserved for prediction – in order to “target” a specific treatment of interest for inference. Whether you’re formally doing causal inference or not, it allows you to estimate values that are closer to the truth.&lt;/p&gt;
&lt;!-- Two years later and I am convinced that semiparametric estimation methods like TMLE are the future of statistics and data science. TMLE allows the use of flexible, adaptive models -- the ones I always thought were reserved for prediction -- in order to &#34;target&#34; a specific treatment of interest. Whether you&#39;re formally doing causal inference or not, it allows you to compute estimates that are closer to the truth.  --&gt;
&lt;!-- The **why** and **how** TMLE works is quite complicated; it relies on a lot of semiparametric estimation and empirical process theory. Besides the fact that I am in no way qualified to explain it, it&#39;s too much for one little blog post. Instead I&#39;ll be tackling *what* the algorithm does and *where* you can learn more. --&gt;
&lt;p&gt;&lt;img src=&#34;/img/venn_dia.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;!-- Even though I&#39;m a certified statistician, my background is stronger in biology than mathematical theory. --&gt;
&lt;p&gt;TMLE was developed in 2007 by Mark van der Laan at UC Berkeley, and it is slowly but surely starting to see more widespread use. Since learning about TMLE, I’ve wanted to create a starter guide for those who who are very applied thinkers: using less mathematical notation when possible, lots of visualizations, and in-line &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;p&gt;I hope you find the way I think about the algorithm useful, but if not, I hope you check out some of the &lt;a href=&#34;#6.-references&#34;&gt;references&lt;/a&gt; I’ve listed at the end, because I think TMLE and other semiparametric estimation methods are super important, interesting, and undoubtedly part of the future of statistics and data science.&lt;/p&gt;
&lt;!-- # 3. The Motivation for a Visual Guide --&gt;
&lt;!-- This post contains a step-by-step demonstration of TMLE with `R` code and simulated data. I may tackle the highlights of *why* and *how* TMLE works in future posts, but if you&#39;re interested in the meantime, check out my references section at the end. Before I discuss the algorithm, two small background notes: --&gt;
&lt;/div&gt;
&lt;div id=&#34;background-notes-on-the-tutorial&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Background notes on the tutorial&lt;/h1&gt;
&lt;!-- ### Notation and Data --&gt;
&lt;!-- I&#39;ve tried to use the same notation in this post as is commonly used in the [TMLE literature](#references) to make it easier to move from one resource to another. I&#39;ve also used the same data generating code as Miguel Angel Luque Fernandez used in his [excellent and slightly more technical tutorial on TMLE](https://migariane.github.io/TMLE.nb.html), so that you can move back and forth between our tutorials, if you wish. Lastly, in the rest of this post I use **treatment** to refer to any exposure, or variable, whose effect we want to &#34;target&#34; in our estimation. --&gt;
&lt;div id=&#34;superlearning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Superlearning&lt;/h3&gt;
&lt;p&gt;I use the ensemble learning method &lt;strong&gt;superlearning&lt;/strong&gt; (also known as “stacking”) to demonstrate TMLE. This is because superlearning is theoretically and empirically proven to yield the best results in TMLE.&lt;/p&gt;
&lt;p&gt;For a tutorial on superlearning, you can check out my &lt;a href=&#34;www.khstats.com/sl/superlearning&#34;&gt;previous blog post&lt;/a&gt;. If you’re new to superlearning/stacking, the necessary knowledge for this post is that it allows you to combine many statistical learning algorithms for prediction. When I use &lt;code&gt;SuperLearner()&lt;/code&gt; in the following example code, I could have used &lt;code&gt;glm()&lt;/code&gt;, &lt;code&gt;randomForest()&lt;/code&gt;, or any other parametric or non-parametric supervised learning algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tmle-and-causal-inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TMLE and Causal Inference&lt;/h3&gt;
&lt;p&gt;Although TMLE was developed for causal inference due to its many attractive statistical properties (discussed later), it cannot be considered causal inference by itself. Causal inference is a process that first requires identification of a causal estimand of interest &lt;em&gt;before&lt;/em&gt; it can be considered a statistical estimand with a causal interpretation.&lt;/p&gt;
&lt;p&gt;TMLE can be used to estimate various statistical estimands (odds ratio, risk ratio, mean outcome difference, etc.) even when causal assumptions are not met. TMLE is, as its name implies, simply a tool for estimation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-tmle-algorithm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. The TMLE Algorithm&lt;/h1&gt;
&lt;p&gt;In this tutorial I’ll show a very basic form of TMLE: estimating the mean difference in outcomes, adjusted for baseline confounders, for a binary outcome and binary treatment.&lt;/p&gt;
&lt;div id=&#34;initial-set-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Initial set up&lt;/h3&gt;
&lt;p&gt;Let’s first load the necessary libraries and set a seed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # for data manipulation
library(kableExtra) # for table printing
library(SuperLearner) # for ensemble learning

set.seed(7) # for reproducible results&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s simulate a data set for demonstration of the algorithm. This data will have a very simple structure: a binary treatment, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, binary outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and four confounders: &lt;span class=&#34;math inline&#34;&gt;\(W_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W_3\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(W_4\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/1_data_structure.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;generate_data &amp;lt;- function(n){ 
    W1 &amp;lt;- rbinom(n, size=1, prob=0.3)
    W2 &amp;lt;- rbinom(n, size=1, prob=0.7)
    W3 &amp;lt;- runif(n, min=0, max=3)
    W4 &amp;lt;- runif(n, min=0, max=8)
    A  &amp;lt;- rbinom(n, size=1, prob= plogis(-2 + 0.3*W2 + 0.1*W3 + 0.3*W4 + 0.4*W2*W4))
    Y &amp;lt;- rbinom(n, size=1, prob= plogis(-0.5 + A -0.2*W1 + 0.4*W2 + 0.2*W4 + 0.3*W3 + 0.2*W2*W4))
    return(tibble(W1, W2, W3, W4, A, Y))
}

n &amp;lt;- 5000
dat_obs &amp;lt;- generate_data(n) # generate a data set with n observations

kable(head(dat_obs), digits=2, caption = &amp;quot;Simulated data set.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Simulated data set.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W3
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
W4
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.52
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As mentioned earlier, TMLE can estimate many different statistical estimands of interest. In this example, the statistical estimand is the mean difference in outcomes between those who received the treatment and those who did not, adjusting for confounders.&lt;/p&gt;
&lt;p&gt;Under causal inference assumptions, this could be identifiable as the Average Treatment Effect (ATE). Let’s pretend for this example that we previously met causal assumptions and call our statistical estimand, &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, the ATE.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Psi = ATE = E_W[E[Y|A=1,W] - E[Y|A=0,W]]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;At this point in set-up, we should also pick our statistical learning algorithms to combine when we use the superlearner to estimate the expected outcome and probability of treatment. Let’s use LASSO (&lt;code&gt;glmnet&lt;/code&gt;), random forests (&lt;code&gt;ranger&lt;/code&gt;), and Multivariate Adaptive Regression Splines (MARS) (&lt;code&gt;earth&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sl_libs &amp;lt;- c(&amp;#39;SL.glmnet&amp;#39;, &amp;#39;SL.ranger&amp;#39;, &amp;#39;SL.earth&amp;#39;) # a library of machine learning algorithms (penalized regression, random forests, and multivariate adaptive regression splines)&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 1: Estimate the Outcome
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;The very first step of TMLE is to fit a statistical model to estimate the expected value of the outcome using treatment and confounders as predictors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/2_outcome_fit.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(A,W) = \mathrm{E}[Y|A,W]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ll use the &lt;code&gt;SuperLearner()&lt;/code&gt; function to fit a weighted combination of multiple machine learning models (defined earlier in &lt;code&gt;sl_libs&lt;/code&gt;). This function takes the outcome &lt;code&gt;Y&lt;/code&gt; as a vector and a data frame &lt;code&gt;X&lt;/code&gt; as predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Y &amp;lt;- dat_obs$Y
X &amp;lt;- dat_obs %&amp;gt;% select(-Y) # remove the outcome to make a matrix of predictors (A, W1, W2, W3, W4) for SuperLearner
Q &amp;lt;- SuperLearner(Y = Y, # Y is the outcome vector
                  X = X, # X is the matrix of predictors
                  family=binomial(), # specify we have a binary outcome
                  SL.library = sl_libs) # specify our superlearner library of LASSO, RF, and MARS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: earth&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: ranger&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we should predict the outcome for every observation under three different scenarios:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. If every observation received the treatment they &lt;em&gt;actually&lt;/em&gt; received.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can get this expected outcome by simply calling &lt;code&gt;predict()&lt;/code&gt; on the model fit, and not specifying any new data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/3_QA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(A,W) = \mathrm{\hat{E}}[Y|A,W]\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_A &amp;lt;- as.vector(predict(Q)$pred) # obtain predictions for everyone using the treatment they actually received&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. If every observation received the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To do this, we’ll first need to create a data set where every observation received the treatment of interest, whether they actually did or not. Then we can call the &lt;code&gt;predict()&lt;/code&gt; function on that data set.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/4_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(1,W) = \mathrm{\hat{E}}[Y|A=1,W]\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_A1 &amp;lt;- X %&amp;gt;% mutate(A = 1)  # data set where everyone received treatment
Q_1 &amp;lt;- as.vector(predict(Q, newdata = X_A1)$pred) # predict on that everyone-exposed data set&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. If every observation received the control.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Similarly, we create a data set where every observation did not receive the treatment of interest, whether they actually did or not, and call the &lt;code&gt;predict()&lt;/code&gt; function again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/5_Q1.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Q}(0,W) = \mathrm{\hat{E}}[Y|A=0,W]\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_A0 &amp;lt;- X %&amp;gt;% mutate(A = 0) # data set where no one received treatment
Q_0 &amp;lt;- as.vector(predict(Q, newdata = X_A0)$pred)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s create a new data frame, &lt;code&gt;dat_tmle&lt;/code&gt;, to hold the three vectors we’ve created so far, along with the treatment status &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and observed outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Notice that when &lt;span class=&#34;math inline&#34;&gt;\(A=1\)&lt;/span&gt;, the expected outcome &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,W]\)&lt;/span&gt; equals the expected outcome under treatment &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=1,W]\)&lt;/span&gt; and when &lt;span class=&#34;math inline&#34;&gt;\(A=0\)&lt;/span&gt;, the expected outcome &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,W]\)&lt;/span&gt; equals the expected outcome under no treatment &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A=0,W]\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_tmle &amp;lt;- tibble(Y = dat_obs$Y, A = dat_obs$A, Q_A, Q_0, Q_1)
kable(head(dat_tmle), digits=2, caption = &amp;quot;TMLE Algorithm after Step 1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-8&#34;&gt;Table 2: &lt;/span&gt;TMLE Algorithm after Step 1
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.91
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.92
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.82
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.88
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Also note that our expected outcomes are on the original outcome scale (i.e. probability, rather than the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; probability).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We could stop here and get our estimate of the ATE by computing &lt;code&gt;Q_1 - Q_0&lt;/code&gt;, which &lt;em&gt;would&lt;/em&gt; be the mean difference in the expected outcomes, conditional on confounders. However, those expected outcome estimates have the optimal bias-variance tradeoff for estimating the outcomes, not the ATE! Our estimate of the ATE might be biased, and furthermore, we don’t have a formula to compute standard errors after using machine learning. We need to incorporate information about the treatment mechanism to fix this.&lt;/p&gt;
&lt;!-- We could stop here and just do: --&gt;
&lt;!-- $$\mathrm{\hat{E}}[\mathrm{\hat{E}}[Y|A=1,W]-\mathrm{\hat{E}}[Y|A=0,W]]$$ --&gt;
&lt;!-- to get our estimate of the ATE. This is often referred to as **G-computation**. However, because we&#39;ve used machine learning to obtain our outcome estimates, they do not have the right bias-variance tradeoff for treatment (the bias-variance tradeoff is instead optimized for the outcome). Besides the potential for bias in the G-computation ATE estimate itself, we have no way to obtain valid standard errors. --&gt;
&lt;!-- We need to incorporate information about the treatment mechanism in order to fix the incorrect bias-variance trade-off. --&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 2: Estimate the Probability of Treatment
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;The next step is to estimate the probability of treatment, given confounders. This quantity is often called the &lt;strong&gt;propensity score&lt;/strong&gt;, as in it gives the &lt;em&gt;propensity&lt;/em&gt; that an observation will receive a treatment of interest.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[g(W) = \mathrm{Pr}(A=1|W)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/6_treatment_fit.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Pr}(A=1|W)\)&lt;/span&gt; in the same way as we estimated &lt;span class=&#34;math inline&#34;&gt;\(E[Y|A,W]\)&lt;/span&gt;: using the superlearner algorithm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A &amp;lt;- dat_obs$A
X_A &amp;lt;- dat_obs %&amp;gt;% select(-Y, -A) # matrix of predictors that only contains the confounders W1, W2, W3, and W4

g &amp;lt;- SuperLearner(Y = A,
                  X = X_A,
                  family=binomial(),
                  SL.library=sl_libs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we need to compute three different quantities from this model fit:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. The inverse probability of receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/7_H1.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(1,W) = \frac{1}{g(W)} = \frac{1}{\mathrm{Pr}(A=1|W)}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_w &amp;lt;- as.vector(predict(g)$pred)
H_1 &amp;lt;- 1/g_w&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. The negative inverse probability of not receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/8_H0.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(0,W) = -\frac{1}{1-g(W)}= -\frac{1}{\mathrm{Pr}(A=0|W)}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;H_0 &amp;lt;- -1/(1-g_w)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/9_HA.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H(A,W) = \frac{\mathrm{I}(A=1)}{\mathrm{Pr}(A=1|W)}-\frac{\mathrm{I}(A=0)}{\mathrm{Pr}(A=0|W)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To calculate this, we’ll first add the &lt;span class=&#34;math inline&#34;&gt;\(H(1,W)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H(0,W)\)&lt;/span&gt; vectors to our &lt;code&gt;dat_tmle&lt;/code&gt; data frame, and then we can use &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; to assign &lt;span class=&#34;math inline&#34;&gt;\(H(A,W)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_tmle &amp;lt;- # add clever covariate data to dat_tmle
  dat_tmle %&amp;gt;%
  bind_cols(
         H_1 = H_1,
         H_0 = H_0) %&amp;gt;%
  mutate(H_A = case_when(A == 1 ~ H_1, # if A is 1 (treated), assign H_1
                       A == 0 ~ H_0))  # if A is 0 (not treated), assign H_0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our initial estimates of the outcome, and the estimates of the probability of treatment:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(head(dat_tmle), digits=2, caption=&amp;quot;TMLE Algorithm after Step 2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-13&#34;&gt;Table 3: &lt;/span&gt;TMLE Algorithm after Step 2
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Q_1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
H_A
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.84
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.77
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.78
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.02
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.52
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.66
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 3: Estimate the Fluctuation Parameter
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;
HTML Image as link
&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a href=&#34;&#34;&gt;
&lt;img alt=&#34;cheatsheet&#34; src=&#34;/img/bear_with_me.jpg&#34;
               style=&#34;float:right; padding-left:40px; padding-top:20px
         width=55%&#34;&gt;
&lt;/a&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;The next step is to fit a model to help us solve for a &lt;strong&gt;fluctuation parameter&lt;/strong&gt;. Fair warning: what we have to do for this is easy to code, but the &lt;strong&gt;why&lt;/strong&gt; and &lt;strong&gt;how&lt;/strong&gt; this works is too complicated for this blog post. I’ll attempt to explain at a high level after I show the step.&lt;/p&gt;
&lt;p&gt;Practically, we fit a logistic regression with only one covariate, &lt;span class=&#34;math inline&#34;&gt;\(H(A,W)\)&lt;/span&gt;, and the initial outcome estimate, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,W]\)&lt;/span&gt;, as the intercept. The outcome of the logistic regression is the observed outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[logit(\mathrm{E}[Y|A,W]) = logit(\mathrm{\hat{E}}[Y|A,W]) + \epsilon H(A,W)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Two technical points for application: since we’re fitting a logistic regression, the initial estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\hat{E}}[Y|A,W]\)&lt;/span&gt; needs to be on the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; scale (so we use &lt;code&gt;qlogis&lt;/code&gt; to transform the probabilities). Also, the &lt;code&gt;R&lt;/code&gt; code for a fixed intercept is &lt;code&gt;-1 + offset(fixed_intercept)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/10_logistic_regression.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm_fit &amp;lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle, family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note that even when the outcome is not binary, this will still be a logistic regression.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Next we need to save the coefficient from that logistic regression, which we will call &lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/11_epsilon.png&#34; style=&#34;width:40.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eps &amp;lt;- coef(glm_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- The logistic regression in this case is unrelated to the outcome type; it is used because we want to solve for the log likelihood (remember in parametric MLE, you take the derivative of the log-likelihood to get the score...?) It&#39;s related to that. --&gt;
&lt;p&gt;Okay, so why did we do that? The way we set up that logistic regression model was a trick to solve for the &lt;strong&gt;Efficient Influence Function&lt;/strong&gt; (EIF) of our estimate, which tells us two important and related pieces of information:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. How much to change, or fluctuate, the initial estimates of the expected outcome.&lt;/strong&gt; We knew those initial outcome regressions were wrong, because their bias and variance were optimized for the outcome, not the effect of treatment. The fluctuation parameter, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}\)&lt;/span&gt;, combined with information about the treatment mechanism, &lt;span class=&#34;math inline&#34;&gt;\(H(A,W)\)&lt;/span&gt;, will help us update the outcomes to get closer to the true value of the final estimand, the ATE.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. How much influence each observation has on the final estimate.&lt;/strong&gt; The reason we’re able to get standard errors for inference even after using data-adaptive machine learning algorithms like the superlearner is because the TMLE estimator has an EIF (not all estimators do). We’ll be able to use it, just like we use the score function in parametric MLE, to estimate our standard errors.&lt;/p&gt;
&lt;p&gt;There’s a lot of semi-parametric theory behind this over-simplified explanation. If you’re interested in learning more, I’ve linked my favorite resources at the end of the post.&lt;/p&gt;
&lt;!-- Intuitively, we do this step because we know the initial outcome estimates we used machine learning models to fit have the wrong bias-variance tradeoff for estimating the effect of treatment. We&#39;re trying to figure out how much we need to change those initial estimates of the outcome, using information about treatment, to achieve the correct bias-variance trade-off for our final statistical estimand of interest, the ATE.  --&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 4: Update the Initial Estimates of the Expected Outcome
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;We’ve done all the hard parts, so now all we have to do is update our estimates of the expected outcome using our fluctuation parameter, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To do this, we need to put the expected outcome estimates on the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; scale (using &lt;code&gt;qlogis()&lt;/code&gt;) because that’s the scale we used to solve the fluctuation parameter in Step 3. Then we can add our update: &lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon} \times H(A,W)\)&lt;/span&gt;. Finally, we can put the updated estimates back on the true outcome scale (in the case of the binary outcome, probability, using &lt;code&gt;plogis()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We can use &lt;span class=&#34;math inline&#34;&gt;\(expit\)&lt;/span&gt; to show the inverse of the &lt;span class=&#34;math inline&#34;&gt;\(logit\)&lt;/span&gt; function, and we will denote updates to the outcome regressions as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathrm{E}}^*\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathrm{E}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A,W] = expit(logit(\mathrm{\hat{E}}[Y|A,W]) + \hat{\epsilon}H(A,W))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/update_qAW.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;H_A &amp;lt;- dat_tmle$H_A # for cleaner code in Q_A_update
Q_A_update &amp;lt;- plogis(qlogis(Q_A) + eps*H_A)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A=1,W] = expit(logit(\mathrm{\hat{E}}[Y|A=1,W]) + \hat{\epsilon}H(A,1))\]&lt;/span&gt;
&lt;img src=&#34;/img/tmle/12_update_Q1.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_1_update &amp;lt;- plogis(qlogis(Q_1) + eps*H_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mathrm{E}}^*[Y|A=0,W] = expit(logit(\mathrm{\hat{E}}[Y|A=0,W]) + \hat{\epsilon}H(A,0))\]&lt;/span&gt;
&lt;img src=&#34;/img/tmle/13_update_Q0.png&#34; style=&#34;width:70.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q_0_update &amp;lt;- plogis(qlogis(Q_0) + eps*H_0)&lt;/code&gt;&lt;/pre&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 5: Compute the Statistical Estimand of Interest
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;Now that we have updated expected outcome estimates, we can compute the ATE as the mean difference in the updated outcome estimates under treatment and no treatment:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\Psi} = \hat{ATE} = \hat{E}_W[\hat{E^*}[Y|A=1,W] - \hat{E^*}[Y|A=0,W]]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/14_compute_ATE.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmle_ate &amp;lt;- mean(Q_1_update - Q_0_update)
tmle_ate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1281863&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could interpret our estimate as, “the average treatment effect was estimated to be 12.8%.” If causal assumptions were not met, we would say, “the proportion of observations who experienced the outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, after adjusting for baseline confounders, was estimated to be 12.8% higher for those who received treatment compared to those who did not.”&lt;/p&gt;
&lt;html&gt;
&lt;body&gt;
&lt;h2 style=&#34;color:navy&#34; &gt;
&lt;strong&gt;Step 6: Calculate the Standard Errors, Confidence Intervals, and P-values
&lt;/h1&gt;
&lt;/strong&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;To find the standard error (SE) of a TMLE estimate, we first compute the &lt;strong&gt;Influence Curve&lt;/strong&gt; (IC). This is what we solved for when we fit the logistic regression in Step 3; we wanted to find the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; that solved for the Efficient Influence Function (EIF). The terms EIF and IC are, for most intents and purposes, used interchangably; the latter tends to refer to an estimate with actual data.&lt;/p&gt;
&lt;p&gt;Anyways, we can compute the IC for our estimate, which tells us how much influence each observation has on the estimate. This is the semi-parametric equivalent to the score function in parametric MLE.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{IC} = (Y-\hat{E^*}[Y|A,W])H(A,W) + \hat{E^*}[Y|A=1,W] - \hat{E^*}[Y|A=0,W] - \hat{ATE}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ic &amp;lt;- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the IC, we can take the square-root of its variance divided by the number of observations to get the standard error of our estimate.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle/15_ses.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{SE} = \sqrt{\frac{var(\hat{IC})}{N}} \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmle_se &amp;lt;- sqrt(var(ic)/nrow(dat_obs))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the standard error, we can easily get the 95% confidence interval and p-value of our estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ci_lo &amp;lt;- tmle_ate - 1.96*tmle_se
ci_hi &amp;lt;- tmle_ate + 1.96*tmle_se

pval &amp;lt;- 2 * (1 - pnorm(abs(tmle_ate / tmle_se)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can successfully report our ATE as 0.128 (95% CI: 0.103, 0.153).&lt;/p&gt;
&lt;p&gt;Whew, that was a lot! Luckily there are &lt;code&gt;R&lt;/code&gt; packages so that you don’t have to hand code TMLE yourself. &lt;code&gt;R&lt;/code&gt; packages to implement the TMLE algorithm include &lt;a href=&#34;https://www.jstatsoft.org/article/view/v051i13&#34;&gt;&lt;code&gt;tmle&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://tlverse.org/tlverse-handbook/tmle3.html&#34;&gt;&lt;code&gt;tmle3&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.jstatsoft.org/article/view/v081i01&#34;&gt;&lt;code&gt;ltmle&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://htmlpreview.github.io/?https://gist.githubusercontent.com/nt-williams/ddd44c48390b8d976fad71750e48d8bf/raw/45db700a02bf92e2a55790e60ed48266a97ca4e7/intro-lmtp.html&#34;&gt;&lt;code&gt;lmtp&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;properties-of-tmle&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Properties of TMLE&lt;/h1&gt;
&lt;p&gt;TMLE is a &lt;strong&gt;doubly robust&lt;/strong&gt; estimator, which means that if either the regression to estimate the expected outcome, or the regression to estimate the probability of treatment, are correctly specified (formally, their bias goes to zero as sample size grows large, meaning they are &lt;strong&gt;consistent&lt;/strong&gt;), the final TMLE estimate will be consistent.&lt;/p&gt;
&lt;p&gt;If both regressions are consistent, the final estimate will reach the smallest possible sample variance in the fewest number of observations (formally: it will be &lt;strong&gt;efficient&lt;/strong&gt;). The reason we use superlearning for estimating the outcome and treatment models fits is to give us the best possible chance of having correctly specified models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tmle_props.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. References&lt;/h1&gt;
&lt;p&gt;If you’d like to learn more, I recommend the following resources:&lt;/p&gt;
&lt;div id=&#34;tmle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TMLE&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The paper I referred to most often while learning TMLE was &lt;a href=&#34;https://academic.oup.com/aje/article/185/1/65/2662306&#34;&gt;&lt;em&gt;Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies&lt;/em&gt;&lt;/a&gt; by Megan S. Schuler and Sherri Rose. It has a nice step-by-step written explanation and Figure 1 is a good summary of how TMLE compares to other common estimation methods in causal inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I also really like the written explanations in the &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-4419-9782-1&#34;&gt;&lt;em&gt;Targeted Learning&lt;/em&gt;&lt;/a&gt; book by Mark van der Laan and Sherri Rose. The notation was too difficult for me to follow, but the words themselves make a lot of sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Miguel Luque wrote an &lt;a href=&#34;https://migariane.github.io/TMLE.nb.html&#34;&gt;excellent bookdown tutorial on TMLE&lt;/a&gt;, also with step-by-step &lt;code&gt;R&lt;/code&gt; code. It is more technical and thorough than my post, but still aimed at an applied audience.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;semiparametric-theory&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Semiparametric Theory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Edward Kennedy has several well-written pieces on semiparametric estimation in causal inference. I recommend starting with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;His introductory paper on &lt;a href=&#34;https://arxiv.org/pdf/1709.06418.pdf&#34;&gt;Semiparametric Theory&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;His &lt;a href=&#34;http://www.ehkennedy.com/uploads/5/8/4/5/58450265/unc_2019_cirg.pdf&#34;&gt;slideshow tutorial&lt;/a&gt; &lt;em&gt;Nonparametric efficiency theory and machine learning in causal inference&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;influence-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Influence Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;My favorite resource so far for learning specifically about influence functions has been &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/00031305.2020.1717620&#34;&gt;Visually Communicating Influence Functions&lt;/a&gt; by Aaron Fisher and Edward Kennedy. However, this paper didn’t make sense to me until I worked through this &lt;a href=&#34;https://observablehq.com/@herbps10/one-step-estimators-and-pathwise-derivatives&#34;&gt;interactive tutorial&lt;/a&gt; by Herb Susmann. I suggest playing around with the interactive examples first, and then trying to work through the paper.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Causal Inference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you want to learn more about the foundations of causal inference, I suggest &lt;a href=&#34;http://bayes.cs.ucla.edu/PRIMER/&#34;&gt;&lt;em&gt;Causal Inference in Statistics: A Primer&lt;/em&gt;&lt;/a&gt; by Judea Pearl and &lt;a href=&#34;https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/&#34;&gt;&lt;em&gt;What If&lt;/em&gt;&lt;/a&gt; (Part I) by Miguel Hernan and James Robins. These are both good starters for learning about the &lt;em&gt;identification&lt;/em&gt; side of causal inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I also think the previously mentioned &lt;em&gt;Targeted Learning&lt;/em&gt; book does a good job of setting up the goals and steps of causal inference.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll continue to update this page with resources as I discover them.&lt;/p&gt;
&lt;p&gt;Feedback on this post is welcome, either from the new learners of TMLE or experts in causal inference. The best way to reach me is through &lt;a href=&#34;mailto:kathoffman.stats@gmail.com&#34;&gt;email&lt;/a&gt;. This is just a side hobby of mine, so please be patient with my response time. :-)&lt;/p&gt;
&lt;p&gt;Thank you to my colleague Iván Díaz for answering my many, many questions on TMLE, and to Miguel Luque for very helpful feedback on the visual guide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] SuperLearner_2.0-26 nnls_1.4            kableExtra_1.1.0   
##  [4] forcats_0.5.0       stringr_1.4.0       dplyr_1.0.2        
##  [7] purrr_0.3.4         readr_1.3.1         tidyr_1.1.2        
## [10] tibble_3.0.3        ggplot2_3.3.2       tidyverse_1.3.0    
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.5         lubridate_1.7.9    lattice_0.20-38    plotmo_3.5.7      
##  [5] earth_5.1.2        assertthat_0.2.1   glmnet_3.0-2       digest_0.6.25     
##  [9] foreach_1.5.0      ranger_0.12.1      R6_2.4.1           cellranger_1.1.0  
## [13] backports_1.1.8    reprex_0.3.0       evaluate_0.14      httr_1.4.1        
## [17] highr_0.8          blogdown_0.19      pillar_1.4.6       TeachingDemos_2.12
## [21] rlang_0.4.7        readxl_1.3.1       rstudioapi_0.11    Matrix_1.2-18     
## [25] rmarkdown_2.1      webshot_0.5.2      munsell_0.5.0      broom_0.7.0       
## [29] compiler_3.6.3     modelr_0.1.6       xfun_0.14          pkgconfig_2.0.3   
## [33] shape_1.4.4        htmltools_0.4.0    tidyselect_1.1.0   bookdown_0.19     
## [37] codetools_0.2-16   fansi_0.4.1        viridisLite_0.3.0  crayon_1.3.4      
## [41] dbplyr_1.4.3       withr_2.2.0        cabinets_0.6.0     grid_3.6.3        
## [45] jsonlite_1.6.1     gtable_0.3.0       lifecycle_0.2.0    DBI_1.1.0         
## [49] magrittr_1.5       scales_1.1.1       cli_2.0.2          stringi_1.4.6     
## [53] fs_1.4.1           xml2_1.3.0         ellipsis_0.3.1     generics_0.0.2    
## [57] vctrs_0.3.4        Formula_1.2-3      iterators_1.0.12   tools_3.6.3       
## [61] glue_1.4.2         hms_0.5.3          plotrix_3.7-7      yaml_2.2.1        
## [65] colorspace_1.4-1   rvest_0.3.5        knitr_1.28         haven_2.2.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Conditional and Iterated Expectations with a Linear Regression Model</title>
      <link>/blog/iterated-expectations/iterated-expectations/</link>
      <pubDate>Sat, 14 Mar 2020 21:13:14 -0500</pubDate>
      <guid>/blog/iterated-expectations/iterated-expectations/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;hr /&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TL;DR&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can a regress an outcome on a grouping variable &lt;em&gt;plus any other variable(s)&lt;/em&gt; and the unadjusted and adjusted group means will be identical.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can see this in a simple example using the &lt;a href=&#34;https://github.com/allisonhorst/palmerpenguins&#34;&gt;&lt;code&gt;palmerpenguins&lt;/code&gt;&lt;/a&gt; data:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remotes::install_github(&amp;quot;allisonhorst/palmerpenguins&amp;quot;)
library(palmerpenguins)
library(tidyverse)
library(gt)

# use complete cases for simplicity
penguins &amp;lt;- drop_na(penguins)

penguins %&amp;gt;%
  # fit a linear regression for bill length given bill depth and species
  # make a new column containing the fitted values for bill length
  mutate(preds = predict(lm(bill_length_mm ~ bill_depth_mm + species, data = .))) %&amp;gt;%
  # compute unadjusted and adjusted group means
  group_by(species) %&amp;gt;%
  summarise(mean_bill_length = mean(bill_length_mm),
            mean_predicted_bill_length = mean(preds)) %&amp;gt;%
  gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#mcysheiceb .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#mcysheiceb .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#mcysheiceb .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#mcysheiceb .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#mcysheiceb .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#mcysheiceb .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#mcysheiceb .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#mcysheiceb .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#mcysheiceb .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#mcysheiceb .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#mcysheiceb .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#mcysheiceb .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#mcysheiceb .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#mcysheiceb .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#mcysheiceb .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#mcysheiceb .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#mcysheiceb .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#mcysheiceb .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#mcysheiceb .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#mcysheiceb .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#mcysheiceb .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#mcysheiceb .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#mcysheiceb .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#mcysheiceb .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#mcysheiceb .gt_left {
  text-align: left;
}

#mcysheiceb .gt_center {
  text-align: center;
}

#mcysheiceb .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#mcysheiceb .gt_font_normal {
  font-weight: normal;
}

#mcysheiceb .gt_font_bold {
  font-weight: bold;
}

#mcysheiceb .gt_font_italic {
  font-style: italic;
}

#mcysheiceb .gt_super {
  font-size: 65%;
}

#mcysheiceb .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;mcysheiceb&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;species&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_bill_length&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_predicted_bill_length&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Adelie&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Chinstrap&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Gentoo&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is because &lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z]=E[Y|Z=z]\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can view a fitted value from the regression, &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z]\)&lt;/span&gt;, as a random variable to help us see this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;#step-by-step-proof&#34;&gt;Skip to the end&lt;/a&gt; to see the proof.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src=&#34;/img/expectations.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ll admit I spent many weeks of my first probability theory course struggling to understand when and why my professor was writing &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. When I finally learned all the rules for expectations of random variables, I still had zero appreciation for their implications in my future work as an applied statistician.&lt;/p&gt;
&lt;p&gt;I recently found myself in a rabbit hole of expectation properties while trying to write a seemingly simple function in &lt;code&gt;R&lt;/code&gt;. Now that I have the output of my function all sorted out, I have a newfound appreciation for how I can use regressions – a framework I’m very comfortable with – to rethink some of the properties I learned in my probability theory courses.&lt;/p&gt;
&lt;p&gt;In the function, I was regressing an outcome on a few variables plus a grouping variable, and then returning the group means of the fitted values. My function kept outputting adjusted group means that were &lt;em&gt;identical&lt;/em&gt; to the unadjusted group means.&lt;/p&gt;
&lt;p&gt;I soon realized that for what I needed to do, my grouping variable should not be in the regression model. However, I was still perplexed as to how the adjusted and unadjusted group means could be the same.&lt;/p&gt;
&lt;p&gt;I created a very basic example to test this unexpected result. I regressed a variable from the new &lt;code&gt;penguins&lt;/code&gt; data set, &lt;code&gt;bill_length_mm&lt;/code&gt;, on another variable called &lt;code&gt;bill_depth_mm&lt;/code&gt; and a grouping variable &lt;code&gt;species&lt;/code&gt;. I then looked at the mean within each category of &lt;code&gt;species&lt;/code&gt; for both the unadjusted &lt;code&gt;bill_depth_mm&lt;/code&gt; and fitted values from my linear regression model for &lt;code&gt;bill_depth_mm&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;penguins %&amp;gt;%
  # fit a linear regression for bill length given bill depth and species
  # make a new column containing the fitted values for bill length
  mutate(preds = predict(lm(bill_length_mm ~ bill_depth_mm + species, data = .))) %&amp;gt;%
  # compute unadjusted and adjusted group means
  group_by(species) %&amp;gt;%
  summarise(mean_bill_length = mean(bill_length_mm),
            mean_predicted_bill_length = mean(preds)) %&amp;gt;%
  gt()&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#ecegfguyhs .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ecegfguyhs .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ecegfguyhs .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ecegfguyhs .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ecegfguyhs .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ecegfguyhs .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ecegfguyhs .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ecegfguyhs .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ecegfguyhs .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ecegfguyhs .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#ecegfguyhs .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ecegfguyhs .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ecegfguyhs .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#ecegfguyhs .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#ecegfguyhs .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ecegfguyhs .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#ecegfguyhs .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ecegfguyhs .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#ecegfguyhs .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ecegfguyhs .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ecegfguyhs .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ecegfguyhs .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#ecegfguyhs .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ecegfguyhs .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#ecegfguyhs .gt_left {
  text-align: left;
}

#ecegfguyhs .gt_center {
  text-align: center;
}

#ecegfguyhs .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ecegfguyhs .gt_font_normal {
  font-weight: normal;
}

#ecegfguyhs .gt_font_bold {
  font-weight: bold;
}

#ecegfguyhs .gt_font_italic {
  font-style: italic;
}

#ecegfguyhs .gt_super {
  font-size: 65%;
}

#ecegfguyhs .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;ecegfguyhs&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;species&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_bill_length&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;mean_predicted_bill_length&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Adelie&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;38.82397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Chinstrap&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;48.83382&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;Gentoo&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;47.56807&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;I saw the same strange output, even in my simple example. I realized this must be some statistics property I’d learned about and since forgotten, so I decided to write out what I was doing in expectations.&lt;/p&gt;
&lt;p&gt;First, I wrote down the unadjusted group means in the form of an expectation. I wrote down a conditional expectation, since we are looking at the mean of &lt;code&gt;bill_length_mm&lt;/code&gt; when &lt;code&gt;species&lt;/code&gt; is restricted to a certain category. We can explicitly show this by taking the expectation of a random variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Bill Length}\)&lt;/span&gt;, while setting another random variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;, equal to only one category at a time.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Adelie]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Chinstrap]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=Gentoo]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;More generally, we could write out the unadjusted group mean using a group indicator variable, &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;, which can take on all possible values &lt;span class=&#34;math inline&#34;&gt;\(species\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{Species}=species]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So that’s our unadjusted group means. What about the adjusted group mean? We can start by writing out the linear regression model, which is the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{BillLength}\)&lt;/span&gt;, conditional on the random variables &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{BillDepth}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{Species}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When I used the &lt;code&gt;predict&lt;/code&gt; function on the fit of that linear regression model, I obtained the fitted values from that expectation, before I separated the fitted values by group to get the grouped means. We can see those fitted values as random variables themselves, and write out another conditional mean using a group indicator variable, just as we did for the unadjusted group means earlier.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]|\mathrm{Species}=species]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;My table of unadjusted and adjusted Bill Length means thus showed me that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[\mathrm{BillLength}|\mathrm{BillDepth},\mathrm{Species}]|\mathrm{Species}=species] \\ = E[\mathrm{BillLength}|\mathrm{Species}=species]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Or, in more general notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[E[Y|X,Z]|Z=z] = E[Y|Z=z]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Is it true?! Spoiler alert – yes. Let’s work through the steps of the proof one by one.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;proof-set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Proof set-up&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Let’s pretend for the proof that both our &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (outcome), &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (adjustment variable), and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; (grouping variable) are categorical (discrete) variables. This is just to make the math a bit cleaner, since the expectation of a discrete variable (a weighted summation) is a little easier to show than the expectation of a continuous variable (the integral of a probability density function times the realization of the random variable).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A few fundamental expectation results we’ll need:&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;conditional-probability&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Conditional probability&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A|B) = \frac{P(A ∩ B)}{P(B)}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partition-theorem&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Partition theorem&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[A|B] = \sum_Ba \cdot P(A=a|B=b)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-distribution-from-a-joint-distribution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Marginal distribution from a joint distribution&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sum_A\sum_Ba\cdot P(A=a,B=b) = \sum_Aa\sum_B\cdot P(A=a,B=b) = \sum_Aa\cdot P(A=a)=E[A]\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-by-step-proof&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step-by-step Proof&lt;/h1&gt;
&lt;p&gt;Click on the superscript number after each step for more information.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=E[E[Y|X,Z=z]|Z=z]\)&lt;/span&gt; &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}E[Y|X=x,Z=z]\cdot P(X=x|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y P(Y=y|X=x,Z=z)\cdot P(X=x|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y \frac{P(Y=y,X=x,Z=z)}{P(X=x,Z=z)}\cdot \frac{P(X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{X}\sum_{Y}y \frac{P(Y=y,X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y\sum_{X}\frac{P(Y=y,X=x,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y\frac{P(Y=y,Z=z)}{P(Z=z)}\)&lt;/span&gt; &lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=\sum_{Y}y P(Y=y|Z=z)\)&lt;/span&gt; &lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(=E[Y|Z=z]\)&lt;/span&gt; &lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, we’ve proved that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z]|Z=z] = E[Y|Z=z]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which, thankfully, means I have an answer to my function output confusion. It was a lightbulb moment for me to realize I should think of an inner expectation as a random variable, and all the rules I learned about conditional and iterated expectations can be revisited in the regressions I fit on a daily basis.&lt;/p&gt;
&lt;p&gt;Here’s hoping you too feel inspired to revisit probability theory from time to time, even if your work is very applied. It is, after all, a perfect activity for social distancing! 😷&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Gorman KB, Williams TD, Fraser WR (2014) Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis). PLoS ONE 9(3): e90081. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0090081&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0090081&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.math.arizona.edu/~tgk/464_07/cond_exp.pdf&#34;&gt;A Conditional Expectation - Arizona Math&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Because we’re making our outer expectation conditional on &lt;span class=&#34;math inline&#34;&gt;\(Z=z\)&lt;/span&gt;, we can also move &lt;span class=&#34;math inline&#34;&gt;\(Z=z\)&lt;/span&gt; into our inner expectation. This becomes obvious in the &lt;code&gt;penguins&lt;/code&gt; example, since we only use the fitted values from one category of &lt;code&gt;species&lt;/code&gt; to get the adjusted group mean for that category.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;We can rewrite &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z=z]\)&lt;/span&gt; as the weighted summation of all possible values &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can take. &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X,Z=z]\)&lt;/span&gt; will only ever be able to take values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; that vary over the range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; since our value &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is already fixed. We can weight each of these possible &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; values by &lt;span class=&#34;math inline&#34;&gt;\(P(X=x|Z=z)\)&lt;/span&gt;, since that’s the probabilty &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; will take value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; at our already-fixed &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. Thus, we can start to find &lt;span class=&#34;math inline&#34;&gt;\(E[E[Y|X,Z=z]|Z=z]\)&lt;/span&gt; by weighting each &lt;span class=&#34;math inline&#34;&gt;\(E[Y|X=x,Z=z]\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(P(X=x|Z=z)\)&lt;/span&gt; and adding them all up (see Partition Theorem).&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;We can get the expectation of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at each of those possible values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by a similar process as step 2 (weighting each &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(P(Y=y|X=x, Z=z)\)&lt;/span&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;By the Law of Conditional Probability, we can rewrite our conditional probabilities as joint distributions.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The denominator of the first fraction cancels out with the numerator of the second fraction.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;We can switch the summations around so that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is outside the summation over all values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. This lets us get the joint distribution of only &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;This is a conditional expectation, written in the form of a joint distribution.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;By the Partition Theorem.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Rewriting the previous equation as an expectation.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Day in the Life of a Biostatistician</title>
      <link>/blog/ditl-biostats/</link>
      <pubDate>Tue, 16 Apr 2019 21:13:14 -0500</pubDate>
      <guid>/blog/ditl-biostats/</guid>
      <description>


&lt;p&gt;It seems fitting that my first blog post is on a topic that I tried and failed to find via Google search a few years ago.&lt;/p&gt;
&lt;p&gt;I’ll back up for a second. A few years ago I was a recent college graduate, and trying hard to “figure out my life.” My major was biochemistry, which is one of those degrees where 99%* of people just keep on going to school.&lt;/p&gt;
&lt;p&gt;I was working full-time night-shift at a hospital as a patient care technician. The key word in that sentence is “night-shift” which meant that even on my days off, I didn’t sleep at night, but all my friends and family did. So, I was often alone and awake, with a lot of time to think about my future… and surf the web for potential careers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/sadcat.png&#34; width=&#34;340px&#34; height=&#34;450px&#34; alt=&#34;cat&#34;&gt;&lt;/p&gt;
&lt;p&gt;I knew I wanted a job in healthcare, but I was confused as to where in medicine would be a good fit for me. I could visualize very clearly what my days at work would look like if I were to become a physician, or a physician assistant, or a registered nurse (all careers I developed lengthy pros and cons lists for).&lt;/p&gt;
&lt;p&gt;However, there was another career involving medicine that I was drawn to but didn’t know enough about. “Biostatistics” was a class biochemistry majors took at my university, but I had been exempted because I took AP Statistics in high school.&lt;/p&gt;
&lt;p&gt;To me, biostatistics seemed to be the application of some high school-level math to biological problems. I had no concept of what a degree in biostatistics, and much less a career as a biostatistician, could entail. Endless Google searches with some variant of “what does a biostatistician do” and even “day in the life of a biostatistician” had not given me a very good picture of what I would actually be doing as a biostatistician.&lt;/p&gt;
&lt;p&gt;I ultimately lucked out during a conversation with a professor during my senior year of college. I was rambling about my many life plans and he mentioned his cousin was the chair of a well-known Biostatistics program. He encouraged me to email her my questions about biostatistics, and I am so grateful she took time to respond with detailed answers. Her description of life as a biostatistician was enough for me to choose going to graduate school for an MS in biostatistics over medical school/physician assistant school/second-degree nursing.&lt;/p&gt;
&lt;p&gt;I feel wholeheartedly that although I would have enjoyed my life as an MD/PA/RN, biostatistics is the right career for me. So, in honor of my confused younger self, and as a way of paying it forward, I’ve dedicated this topic for my very first blog post!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/maz.png&#34; alt=&#34;ma&#34;width=&#34;400px&#34; height=&#34;400px&#34; &gt;&lt;/p&gt;
&lt;p&gt;A full disclaimer - what follows is a day in the life of one masters-level, academic research-focused biostatistician and I cannot make claims about the careers of statisticians in industry or pharmaceuticals or hospitals or government. In addition, here’s a bit more background before I get into the granular details of my work:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;My official job description is to assist investigators (i.e. physicians or PhD-level researchers with a scientific question) throughout all stages of the scientific research process. This means helping with study design, data collection, data cleaning (also known as getting the data in the right form for analysis and making sure nothing is obviously incorrect), data visualization, statistical analysis, reporting and explaining my results, and writing methods and results sections for scientific papers. You will soon see that on any given day I am working on multiple projects at various stages of this process.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I will mention R/Rstudio a bit. For those who are not familiar with it, &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; is an open-source (which means anyone can help contribute) programming language that is well-equipped for statistical analysis. It’s arguably very similar to &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;, which is a more widely used language, but statisticians tend to use R more because its statistical packages are very well-developed. During grad school I used Python because I worked in a computational biology lab, and I learned &lt;a href=&#34;https://www.sas.com/en_us/home.html&#34;&gt;SAS&lt;/a&gt; (another statistical programming language) in some of my classes, but R is what I prefer these days. &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt; is a platform that makes it more user friendly to use R.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, without further ado! An average day**:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/desk.png&#34; width=&#34;500px&#34; height=&#34;375px&#34; alt=&#34;desky&#34;&gt;&lt;/p&gt;
&lt;p&gt;9:30AM - I arrive at my office and spend a few minutes chatting with my coworkers. To set the scene for you, I have a fairly spacious cubicle within a group of five other cubicles. I sit next to another research biostatistician, two health informatics professionals, and two clinical trial grant specialists. I’m actually not completely sure what that last pair’s title is, but I know their primary task is to make sure several multi-million dollar clinical trial grants stay funded (woah). Everyone I sit by is young and goofy, but very driven, making for a fun office environment.&lt;/p&gt;
&lt;p&gt;9:45AM - I check and answer new emails from researchers I collaborate on projects with. I send my availability for a meeting to a group of doctors who want to go over the results of a recent analysis I did on Body Mass Index and death rates in the Intensive Care Unit. In a different thread of emails, I thank several researchers from another university for clarifying their methods and sending me code for an analysis similar to one I will soon work on.&lt;/p&gt;
&lt;p&gt;10:00AM - I open a manuscript draft for a paper I received yesterday. It’s from a group of residents and medical students I worked with a few months ago. Their study looks at the association between blood levels of a certain biomarker and the time to death in cancer patients. My role in the analysis was to examine the associations between several biomarkers such as phosphorus, phosphate, and calcitriol. I then fit a regression model, just like y=mx+b, but with way more math. For this analysis I used a model for when your y is a time to an event (death, in this case), fittingly called a survival model. After adjusting for confounding factors like age, which affects both tumor progression and biomarker levels, there was a significant association between the biomarker and time to death in cancer patients.&lt;/p&gt;
&lt;p&gt;The researchers have asked for my assistance in writing the methods section. The methods section of a scientific article is the steps the scientists took to analyze data explicitly written out for anyone looking to review or learn about their work. I read through their current draft of the paper carefully, make some edits, and send it back. They are hoping to submit this paper to a peer-reviewed journal within a few weeks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/KM.png&#34; width=&#34;650px&#34; height=&#34;500px&#34; alt=&#34;kmplot&#34;&gt;&lt;/p&gt;
&lt;p&gt;10:45AM - I have a weekly meeting a few blocks from my office with a neurologist I spend a large portion of my time working with. She is a leader in the field of Alzheimer’s research, and I find it very rewarding to work on her data and be a small part of a growing body of research in the field. I am “contracted” out to her research as I am to all of the researchers I work with—it’s how my institution budgets funding for grants. One of the faculty-level biostatisticians in my department—which means he has a PhD and specializes in certain statistical methods—is also part of this contract, and some days, like today, he joins me at these meetings.&lt;/p&gt;
&lt;p&gt;11:00AM - This week’s meeting is pretty straightforward. We discuss how we can improve one of the neurologist’s National Institute of Health grants from a statistics standpoint. The statistical methods for this project can get complicated, in part because we are looking at the brain scans of women in different stages of menopause over time, and we have to consider age as a confounding factor. We want to convey to the reviewers of our grant how we plan to do this. Since this is a methods-focused meeting, I mostly listen and take notes along with two neurology research coordinators that also attend these weekly meetings. When the meeting concludes I have for less work than usual - I only need to make a few graphs representing our study design and past results for inclusion to the grant.&lt;/p&gt;
&lt;p&gt;12:00PM - I head to lunch with a group of coworkers. They have gotten food from a nearby salad place, and we sit in one of our favorite buildings on campus and eat together. Our jokes oscillate from incredibly nerdy to pretty stupid. One of my coworkers points out that our hair is styled the same way for the third day in a row.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/katalan.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;12:45PM - I get back to my desk and type up my handwritten notes from the neurology meeting and put them in that project’s “Notes” folder. It’s important to me that I keep track of all my meetings electronically - I fear losing my notebooks or someone else having to decipher my cursive should I ever have to pass off a project.&lt;/p&gt;
&lt;p&gt;1:00PM - I start to make a plan for a different analysis I’m working on. This project is something new for me - it involves a protein assay and data for 1000+ different protein expression levels. The researcher I’m working with wants to know which proteins are over- and under-expressed in people with a specific autoimmune disorder and a certain type of lung disease. I’ve recently spoken to some bioinformaticians and have a clearer idea of the analysis I need to do. I draw out a little map of the code organization I think would be the most efficient for this analysis and open up Rstudio.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/notes.jpg&#34; width=&#34;400px&#34; height=&#34;350px&#34; alt=&#34;notes&#34;&gt;&lt;/p&gt;
&lt;p&gt;1:30PM - I get to work writing up functions, which is just a fancy programming way of saying your code can do the same thing to multiple data sets (or subsets of patients, as is the case of this protein expression study I’m doing). Sometimes it takes a bit longer to write my functions than it would if I were to just copy and paste my code several different times, but the final code is much more readable and less prone to errors. By the time I’m done working, I have some interactive plots showing the significant and non-significant results. When you hover over them, they show what protein corresponds to which point on the graph. They look like this, except this is not the real data we used in her study.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/volcano.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I use Rstudio and its amazing Rmarkdown tool to craft a draft report to the researcher, and I save it with today’s date in my “Reports” folder for that project. The report so far includes unadjusted and adjusted models of all the protein expressions using very small p-values to account for the 1000+ statistical comparisons we’re making. I show the results in various plots such as the one above (called a “volcano plot” for its shape).&lt;/p&gt;
&lt;p&gt;I have also started writing code for models to determine which proteins are most different, or uniquely expressed, between subgroups of patients. Tomorrow, I will use a technique common in machine learning, called clustering, to see if these protein expressions can correctly classify subgroups of patients. The goal is to find a minimum group of proteins to identify patients of interest who have both the autoimmune disorder and the lung disease my collaborator is interested in. One way this research could be impactful is that it may help determine which proteins pharmaceuticals should develop drugs to target.&lt;/p&gt;
&lt;p&gt;I close the report; I will continue working on this analysis tomorrow.&lt;/p&gt;
&lt;p&gt;4:45PM - It’s time for my last meeting of the day. I head to another floor of my office building and get my laptop set up in a conference room. I await the arrival of several doctors. It will be my first time meeting most of them, and our task today is to discuss the data collection process for a future study involving both genetic mutation data from tumor biopsies and clinical data from electronic health records on thousands of patients with lung cancer.&lt;/p&gt;
&lt;p&gt;5:00PM - The doctors arrive and, after introductions, we talk about the current stage of the project and what the goals are. We discuss the timing of starting chart reviews of patients, how we will upload the information to a database efficiently, and what might be the best way to condense the highly detailed genetics data into useful information for an analysis. Our solution will likely involve a series of iterative searches through the columns containing genetic information.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/redcap.png&#34; alt=&#34;ma&#34;width=&#34;600px&#34; height=&#34;425px&#34; &gt;&lt;/p&gt;
&lt;p&gt;6:00PM - The meeting ends, and I go back to my desk to record more handwritten notes. I log the hours I spent on each project that day into Toggl, which is the time-tracking application our team uses. This is so we know how much time we’re spending on each project, and is as much for our own sake as it is anyone else’s. I update my to-do list, which is a giant color-coded excel spreadsheet, and eat a few chocolate covered raisins as my reward for a productive day.&lt;/p&gt;
&lt;p&gt;6:30PM - I leave work! I typically have some kind of activity, like happy hour (see my cute cubicle buddies below), a sports game, or Spanish class that I’m heading off to. Some days I attend coding workshops hosted by groups such as R-ladies. On nights when I’m feeling especially nerdy, I’ll go home and read a statistics paper or sift through the #rstats tips on twitter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffmakl/khoffman_website/master/content/blog/coworkers.jpg&#34; width=&#34;500px&#34; height=&#34;375px&#34; alt=&#34;cowies&#34;&gt;&lt;/p&gt;
&lt;p&gt;…So, there you have it. One average daily experience as an early career masters-level statistician. All in all, I have a fantastic work-life balance and overall work environment. Every day I get to learn more about science, medicine, statistics, and the intersection of these wonderful ideas. Although it varies quite a bit, approximately 10% of each day involves writing, 20% interacting with other researchers, and the rest of it is spent thinking critically and finding answers to problems I am passionate about.&lt;/p&gt;
&lt;p&gt;I hope if there are any 22, 42, or 14 year-olds out there considering a career in biostatistics and struggling to figure out what on earth we actually do, that you find this post and it lessens your confusion! Feel free to reach out to me if you have questions.&lt;/p&gt;
&lt;p&gt;All the best,&lt;/p&gt;
&lt;p&gt;Kat&lt;/p&gt;
&lt;p&gt;*Not a real statistic.&lt;/p&gt;
&lt;p&gt;**Exact details and diseases of the studies I am currently working on have been generalized or altered to protect the research interests of my collaborators.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
