---
title: "A Visual Guide to the Targeted Maximum Likelihood Estimation (TMLE) Algorithm"
author: "Katherine Hoffman"
draft: true
date: 2020-10-10T21:13:14-05:00
categories: ["statistics","R"]
math: true
tags: ["statistics","causal inference","R","lmtp","TMLE","Targeted Maximum Likelihood Estimation","introduction to TMLE","targeted learning"]
output: 
  html_document:
    toc: true
    toc_float: true
    smart: false
---



<blockquote>
<p>A beginner’s guide to understanding the Targeted Maximum Likelihood Estimation (TMLE) algorithm.</p>
</blockquote>
<html>
<head>
<title>
HTML Image as link
</title>
</head>
<body>
<a href="https://github.com/hoffmakl/CI-visual-guides/blob/master/visual-guides/TMLE.pdf">
<img alt="cheatsheet" src="/img/TMLE.jpg"
         width=100%">
<figcaption>
<em><a href="">T</a>here is a condensed version of this tutorial as an 8.5x11" pdf on my Github in case you would like to print it out for reference as you read more formal TMLE explanations.</em>
</figcaption>
</a>
</body>
</html>
<hr />
<div id="what-is-tmle" class="section level1">
<h1>1. What is TMLE?</h1>
<ul>
<li><p>Targeted Maximum Likelihood Estimation (TMLE) is a general semiparametric estimation framework which allows you to <strong>estimate a statistical estimand of interest using</strong>:</p>
<ol style="list-style-type: decimal">
<li><p>the expected outcome of an individual, given the treatment they received and their baseline confounders</p></li>
<li><p>the probability an individual received the treatment of interest, given their baseline confounders (propensity score)</p></li>
</ol></li>
<li><p>These estimates can come from <strong>flexible, data-adaptive statistical models</strong> we commonly categorize as machine learning, allowing us to place <strong>minimal assumptions on the distribution of the data</strong>. However, unlike estimates normally obtained from data-adaptive algorithms, the <strong>final TMLE estimate will still have valid standard errors for statistical inference</strong>.</p></li>
</ul>
</div>
<div id="an-ms-level-statisticians-motivation-for-learning-tmle" class="section level1">
<h1>2. An MS-level Statistician’s Motivation for Learning TMLE</h1>
<p>When I graduated with my MS in Biostatistics two years ago, I had a mental framework of statistics and data science that I think is pretty common among new graduates. It is this idea that we should use flexible <strong>machine learning</strong> models when the goal is prediction, and interpretable (usually parametric) models when the goal is inference.</p>
<p>Going into my first job, I had a clear plan: if I was given a data set and told to predict an outcome, I would use “black box” algorithms like random forests or support vector machines and then look at prediction performance metrics and variable importance. If I was instead asked to figure out the <em>effect</em> of some individual predictor on an outcome, I would fit a model which yielded point estimates with standard errors so I could compute 95% confidence intervals and p-values: logistic regression, Cox Proportional Hazards, and linear mixed effects, to name a few.</p>
<!-- I'd learned a bit about causal inference during school: enough to know that I might want to first model the  probability of treatment in a logistic regression, and then up or down-weight my observations according to how likely they were to be treated. -->
<!-- This [two-cultures](breiman_two_cultures.pdf) thinking changed when I actually started my first job. My first project was with a statistician, Ivan Diaz, who specializes in semiparametric methods for causal inference. When I asked Ivan for methods guidance, he asked if I'd ever heard of Targeted Maximum Likelihood Estimation (TMLE). Definitely not. -->
<!-- And so began my journey in learning about semiparametric estimation methods and causal inference... -->
<p>This <a href="breiman_two_cultures.pdf">two-cultures</a> mentality changed once I actually started working. I began learning about semiparametric estimation methods in the context of causal inference; TMLE was the first such method I learned. I realized TMLE allows the use of flexible, adaptive models – the ones I always thought were reserved for prediction – in order to “target” a specific treatment of interest for inference. Whether you’re formally doing causal inference or not, it allows you to estimate values that are closer to the truth.</p>
<!-- Two years later and I am convinced that semiparametric estimation methods like TMLE are the future of statistics and data science. TMLE allows the use of flexible, adaptive models -- the ones I always thought were reserved for prediction -- in order to "target" a specific treatment of interest. Whether you're formally doing causal inference or not, it allows you to compute estimates that are closer to the truth.  -->
<!-- The **why** and **how** TMLE works is quite complicated; it relies on a lot of semiparametric estimation and empirical process theory. Besides the fact that I am in no way qualified to explain it, it's too much for one little blog post. Instead I'll be tackling *what* the algorithm does and *where* you can learn more. -->
<p><img src="/img/venn_dia.png" style="width:80.0%" /></p>
<!-- Even though I'm a certified statistician, my background is stronger in biology than mathematical theory. -->
<p>TMLE was developed in 2007 by Mark van der Laan at UC Berkeley, and it is slowly but surely starting to see more widespread use. Since learning about TMLE, I’ve wanted to create a starter guide for those who who are very applied thinkers: using less mathematical notation when possible, lots of visualizations, and in-line <code>R</code> code.</p>
<p>I hope you find the way I think about the algorithm useful, but if not, I hope you check out some of the <a href="#6.-references">references</a> I’ve listed at the end, because I think TMLE and other semiparametric estimation methods are super important, interesting, and undoubtedly part of the future of statistics and data science.</p>
<!-- # 3. The Motivation for a Visual Guide -->
<!-- This post contains a step-by-step demonstration of TMLE with `R` code and simulated data. I may tackle the highlights of *why* and *how* TMLE works in future posts, but if you're interested in the meantime, check out my references section at the end. Before I discuss the algorithm, two small background notes: -->
</div>
<div id="background-notes-on-the-tutorial" class="section level1">
<h1>3. Background notes on the tutorial</h1>
<!-- ### Notation and Data -->
<!-- I've tried to use the same notation in this post as is commonly used in the [TMLE literature](#references) to make it easier to move from one resource to another. I've also used the same data generating code as Miguel Angel Luque Fernandez used in his [excellent and slightly more technical tutorial on TMLE](https://migariane.github.io/TMLE.nb.html), so that you can move back and forth between our tutorials, if you wish. Lastly, in the rest of this post I use **treatment** to refer to any exposure, or variable, whose effect we want to "target" in our estimation. -->
<div id="superlearning" class="section level3">
<h3>Superlearning</h3>
<p>I use the ensemble learning method <strong>superlearning</strong> (also known as “stacking”) to demonstrate TMLE. This is because superlearning is theoretically and empirically proven to yield the best results in TMLE.</p>
<p>For a tutorial on superlearning, you can check out my <a href="www.khstats.com/sl/superlearning">previous blog post</a>. If you’re new to superlearning/stacking, the necessary knowledge for this post is that it allows you to combine many statistical learning algorithms for prediction. When I use <code>SuperLearner()</code> in the following example code, I could have used <code>glm()</code>, <code>randomForest()</code>, or any other parametric or non-parametric supervised learning algorithm.</p>
</div>
<div id="tmle-and-causal-inference" class="section level3">
<h3>TMLE and Causal Inference</h3>
<p>Although TMLE was developed for causal inference due to its many attractive statistical properties (discussed later), it cannot be considered causal inference by itself. Causal inference is a process that first requires identification of a causal estimand of interest <em>before</em> it can be considered a statistical estimand with a causal interpretation.</p>
<p>TMLE can be used to estimate various statistical estimands (odds ratio, risk ratio, mean outcome difference, etc.) even when causal assumptions are not met. TMLE is, as its name implies, simply a tool for estimation.</p>
</div>
</div>
<div id="the-tmle-algorithm" class="section level1">
<h1>4. The TMLE Algorithm</h1>
<p>In this tutorial I’ll show a very basic form of TMLE: estimating the mean difference in outcomes, adjusted for baseline confounders, for a binary outcome and binary treatment.</p>
<div id="initial-set-up" class="section level3">
<h3>Initial set up</h3>
<p>Let’s first load the necessary libraries and set a seed.</p>
<pre class="r"><code>library(tidyverse) # for data manipulation
library(kableExtra) # for table printing
library(SuperLearner) # for ensemble learning

set.seed(7) # for reproducible results</code></pre>
<p>Next, let’s simulate a data set for demonstration of the algorithm. This data will have a very simple structure: a binary treatment, <span class="math inline">\(A\)</span>, binary outcome, <span class="math inline">\(Y\)</span>, and four confounders: <span class="math inline">\(W_1\)</span>, <span class="math inline">\(W_2\)</span>, <span class="math inline">\(W_3\)</span>, and <span class="math inline">\(W_4\)</span>.</p>
<p><img src="/img/tmle/1_data_structure.png" style="width:80.0%" /></p>
<pre class="r"><code>generate_data &lt;- function(n){ 
    W1 &lt;- rbinom(n, size=1, prob=0.3)
    W2 &lt;- rbinom(n, size=1, prob=0.7)
    W3 &lt;- runif(n, min=0, max=3)
    W4 &lt;- runif(n, min=0, max=8)
    A  &lt;- rbinom(n, size=1, prob= plogis(-2 + 0.3*W2 + 0.1*W3 + 0.3*W4 + 0.4*W2*W4))
    Y &lt;- rbinom(n, size=1, prob= plogis(-0.5 + A -0.2*W1 + 0.4*W2 + 0.2*W4 + 0.3*W3 + 0.2*W2*W4))
    return(tibble(W1, W2, W3, W4, A, Y))
}

n &lt;- 5000
dat_obs &lt;- generate_data(n) # generate a data set with n observations

kable(head(dat_obs), digits=2, caption = &quot;Simulated data set.&quot;)</code></pre>
<table>
<caption>
<span id="tab:unnamed-chunk-2">Table 1: </span>Simulated data set.
</caption>
<thead>
<tr>
<th style="text-align:right;">
W1
</th>
<th style="text-align:right;">
W2
</th>
<th style="text-align:right;">
W3
</th>
<th style="text-align:right;">
W4
</th>
<th style="text-align:right;">
A
</th>
<th style="text-align:right;">
Y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.99
</td>
<td style="text-align:right;">
4.52
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
1.09
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.99
</td>
<td style="text-align:right;">
2.70
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1.15
</td>
<td style="text-align:right;">
6.54
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1.95
</td>
<td style="text-align:right;">
5.29
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p>As mentioned earlier, TMLE can estimate many different statistical estimands of interest. In this example, the statistical estimand is the mean difference in outcomes between those who received the treatment and those who did not, adjusting for confounders.</p>
<p>Under causal inference assumptions, this could be identifiable as the Average Treatment Effect (ATE). Let’s pretend for this example that we previously met causal assumptions and call our statistical estimand, <span class="math inline">\(\Psi\)</span>, the ATE.</p>
<p><span class="math display">\[\Psi = ATE = E_W[E[Y|A=1,W] - E[Y|A=0,W]]\]</span></p>
<p>At this point in set-up, we should also pick our statistical learning algorithms to combine when we use the superlearner to estimate the expected outcome and probability of treatment. Let’s use LASSO (<code>glmnet</code>), random forests (<code>ranger</code>), and Multivariate Adaptive Regression Splines (MARS) (<code>earth</code>).</p>
<pre class="r"><code>sl_libs &lt;- c(&#39;SL.glmnet&#39;, &#39;SL.ranger&#39;, &#39;SL.earth&#39;) # a library of machine learning algorithms (penalized regression, random forests, and multivariate adaptive regression splines)</code></pre>
<html>
<body>
<h2 style="color:navy" >
<strong>Step 1: Estimate the Outcome
</h1>
</strong>
</body>
</html>
<p>The very first step of TMLE is to fit a statistical model to estimate the expected value of the outcome using treatment and confounders as predictors.</p>
<p><img src="/img/tmle/2_outcome_fit.png" style="width:70.0%" /></p>
<p><span class="math display">\[Q(A,W) = \mathrm{E}[Y|A,W]\]</span></p>
<p>We’ll use the <code>SuperLearner()</code> function to fit a weighted combination of multiple machine learning models (defined earlier in <code>sl_libs</code>). This function takes the outcome <code>Y</code> as a vector and a data frame <code>X</code> as predictors.</p>
<pre class="r"><code>Y &lt;- dat_obs$Y
X &lt;- dat_obs %&gt;% select(-Y) # remove the outcome to make a matrix of predictors (A, W1, W2, W3, W4) for SuperLearner
Q &lt;- SuperLearner(Y = Y, # Y is the outcome vector
                  X = X, # X is the matrix of predictors
                  family=binomial(), # specify we have a binary outcome
                  SL.library = sl_libs) # specify our superlearner library of LASSO, RF, and MARS</code></pre>
<pre><code>## Loading required namespace: glmnet</code></pre>
<pre><code>## Loading required namespace: earth</code></pre>
<pre><code>## Loading required namespace: ranger</code></pre>
<p>Then, we should predict the outcome for every observation under three different scenarios:</p>
<p><strong>1. If every observation received the treatment they <em>actually</em> received.</strong></p>
<p>We can get this expected outcome by simply calling <code>predict()</code> on the model fit, and not specifying any new data.</p>
<p><img src="/img/tmle/3_QA.png" style="width:50.0%" /></p>
<p><span class="math display">\[\hat{Q}(A,W) = \mathrm{\hat{E}}[Y|A,W]\]</span></p>
<pre class="r"><code>Q_A &lt;- as.vector(predict(Q)$pred) # obtain predictions for everyone using the treatment they actually received</code></pre>
<p><strong>2. If every observation received the treatment.</strong></p>
<p>To do this, we’ll first need to create a data set where every observation received the treatment of interest, whether they actually did or not. Then we can call the <code>predict()</code> function on that data set.</p>
<p><img src="/img/tmle/4_Q1.png" style="width:80.0%" /></p>
<p><span class="math display">\[\hat{Q}(1,W) = \mathrm{\hat{E}}[Y|A=1,W]\]</span></p>
<pre class="r"><code>X_A1 &lt;- X %&gt;% mutate(A = 1)  # data set where everyone received treatment
Q_1 &lt;- as.vector(predict(Q, newdata = X_A1)$pred) # predict on that everyone-exposed data set</code></pre>
<p><strong>3. If every observation received the control.</strong></p>
<p>Similarly, we create a data set where every observation did not receive the treatment of interest, whether they actually did or not, and call the <code>predict()</code> function again.</p>
<p><img src="/img/tmle/5_Q1.png" style="width:80.0%" /></p>
<p><span class="math display">\[\hat{Q}(0,W) = \mathrm{\hat{E}}[Y|A=0,W]\]</span></p>
<pre class="r"><code>X_A0 &lt;- X %&gt;% mutate(A = 0) # data set where no one received treatment
Q_0 &lt;- as.vector(predict(Q, newdata = X_A0)$pred)</code></pre>
<p>Let’s create a new data frame, <code>dat_tmle</code>, to hold the three vectors we’ve created so far, along with the treatment status <span class="math inline">\(A\)</span> and observed outcome <span class="math inline">\(Y\)</span>. Notice that when <span class="math inline">\(A=1\)</span>, the expected outcome <span class="math inline">\(\mathrm{\hat{E}}[Y|A,W]\)</span> equals the expected outcome under treatment <span class="math inline">\(\mathrm{\hat{E}}[Y|A=1,W]\)</span> and when <span class="math inline">\(A=0\)</span>, the expected outcome <span class="math inline">\(\mathrm{\hat{E}}[Y|A,W]\)</span> equals the expected outcome under no treatment <span class="math inline">\(\mathrm{\hat{E}}[Y|A=0,W]\)</span>.</p>
<pre class="r"><code>dat_tmle &lt;- tibble(Y = dat_obs$Y, A = dat_obs$A, Q_A, Q_0, Q_1)
kable(head(dat_tmle), digits=2, caption = &quot;TMLE Algorithm after Step 1&quot;)</code></pre>
<table>
<caption>
<span id="tab:unnamed-chunk-8">Table 2: </span>TMLE Algorithm after Step 1
</caption>
<thead>
<tr>
<th style="text-align:right;">
Y
</th>
<th style="text-align:right;">
A
</th>
<th style="text-align:right;">
Q_A
</th>
<th style="text-align:right;">
Q_0
</th>
<th style="text-align:right;">
Q_1
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.93
</td>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
0.93
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.63
</td>
<td style="text-align:right;">
0.63
</td>
<td style="text-align:right;">
0.84
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.91
</td>
<td style="text-align:right;">
0.77
</td>
<td style="text-align:right;">
0.91
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.92
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.59
</td>
<td style="text-align:right;">
0.59
</td>
<td style="text-align:right;">
0.82
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.88
</td>
</tr>
</tbody>
</table>
<p><em>Also note that our expected outcomes are on the original outcome scale (i.e. probability, rather than the <span class="math inline">\(logit\)</span> probability).</em></p>
<p>We could stop here and get our estimate of the ATE by computing <code>Q_1 - Q_0</code>, which <em>would</em> be the mean difference in the expected outcomes, conditional on confounders. However, those expected outcome estimates have the optimal bias-variance tradeoff for estimating the outcomes, not the ATE! Our estimate of the ATE might be biased, and furthermore, we don’t have a formula to compute standard errors after using machine learning. We need to incorporate information about the treatment mechanism to fix this.</p>
<!-- We could stop here and just do: -->
<!-- $$\mathrm{\hat{E}}[\mathrm{\hat{E}}[Y|A=1,W]-\mathrm{\hat{E}}[Y|A=0,W]]$$ -->
<!-- to get our estimate of the ATE. This is often referred to as **G-computation**. However, because we've used machine learning to obtain our outcome estimates, they do not have the right bias-variance tradeoff for treatment (the bias-variance tradeoff is instead optimized for the outcome). Besides the potential for bias in the G-computation ATE estimate itself, we have no way to obtain valid standard errors. -->
<!-- We need to incorporate information about the treatment mechanism in order to fix the incorrect bias-variance trade-off. -->
<html>
<body>
<h2 style="color:navy" >
<strong>Step 2: Estimate the Probability of Treatment
</h1>
</strong>
</body>
</html>
<p>The next step is to estimate the probability of treatment, given confounders. This quantity is often called the <strong>propensity score</strong>, as in it gives the <em>propensity</em> that an observation will receive a treatment of interest.</p>
<p><span class="math display">\[g(W) = \mathrm{Pr}(A=1|W)\]</span></p>
<p><img src="/img/tmle/6_treatment_fit.png" style="width:60.0%" /></p>
<p>We will estimate <span class="math inline">\(\mathrm{Pr}(A=1|W)\)</span> in the same way as we estimated <span class="math inline">\(E[Y|A,W]\)</span>: using the superlearner algorithm.</p>
<pre class="r"><code>A &lt;- dat_obs$A
X_A &lt;- dat_obs %&gt;% select(-Y, -A) # matrix of predictors that only contains the confounders W1, W2, W3, and W4

g &lt;- SuperLearner(Y = A,
                  X = X_A,
                  family=binomial(),
                  SL.library=sl_libs)</code></pre>
<p>Then we need to compute three different quantities from this model fit:</p>
<p><strong>1. The inverse probability of receiving treatment.</strong></p>
<p><img src="/img/tmle/7_H1.png" style="width:60.0%" /></p>
<p><span class="math display">\[H(1,W) = \frac{1}{g(W)} = \frac{1}{\mathrm{Pr}(A=1|W)}\]</span></p>
<pre class="r"><code>g_w &lt;- as.vector(predict(g)$pred)
H_1 &lt;- 1/g_w</code></pre>
<p><strong>2. The negative inverse probability of not receiving treatment.</strong></p>
<p><img src="/img/tmle/8_H0.png" style="width:70.0%" /></p>
<p><span class="math display">\[H(0,W) = -\frac{1}{1-g(W)}= -\frac{1}{\mathrm{Pr}(A=0|W)}\]</span></p>
<pre class="r"><code>H_0 &lt;- -1/(1-g_w)</code></pre>
<p><strong>3. If the observation was treated, the inverse probability of receiving treatment, and if they were not treated, the negative inverse probability of not receiving treatment.</strong></p>
<p><img src="/img/tmle/9_HA.png" style="width:50.0%" /></p>
<p><span class="math display">\[H(A,W) = \frac{\mathrm{I}(A=1)}{\mathrm{Pr}(A=1|W)}-\frac{\mathrm{I}(A=0)}{\mathrm{Pr}(A=0|W)}\]</span></p>
<p>To calculate this, we’ll first add the <span class="math inline">\(H(1,W)\)</span> and <span class="math inline">\(H(0,W)\)</span> vectors to our <code>dat_tmle</code> data frame, and then we can use <span class="math inline">\(A\)</span> to assign <span class="math inline">\(H(A,W)\)</span>.</p>
<pre class="r"><code>dat_tmle &lt;- # add clever covariate data to dat_tmle
  dat_tmle %&gt;%
  bind_cols(
         H_1 = H_1,
         H_0 = H_0) %&gt;%
  mutate(H_A = case_when(A == 1 ~ H_1, # if A is 1 (treated), assign H_1
                       A == 0 ~ H_0))  # if A is 0 (not treated), assign H_0</code></pre>
<p>We now have our initial estimates of the outcome, and the estimates of the probability of treatment:</p>
<pre class="r"><code>kable(head(dat_tmle), digits=2, caption=&quot;TMLE Algorithm after Step 2&quot;)</code></pre>
<table>
<caption>
<span id="tab:unnamed-chunk-13">Table 3: </span>TMLE Algorithm after Step 2
</caption>
<thead>
<tr>
<th style="text-align:right;">
Y
</th>
<th style="text-align:right;">
A
</th>
<th style="text-align:right;">
Q_A
</th>
<th style="text-align:right;">
Q_0
</th>
<th style="text-align:right;">
Q_1
</th>
<th style="text-align:right;">
H_1
</th>
<th style="text-align:right;">
H_0
</th>
<th style="text-align:right;">
H_A
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.93
</td>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
0.93
</td>
<td style="text-align:right;">
1.23
</td>
<td style="text-align:right;">
-5.30
</td>
<td style="text-align:right;">
1.23
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.63
</td>
<td style="text-align:right;">
0.63
</td>
<td style="text-align:right;">
0.84
</td>
<td style="text-align:right;">
3.25
</td>
<td style="text-align:right;">
-1.44
</td>
<td style="text-align:right;">
-1.44
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.91
</td>
<td style="text-align:right;">
0.77
</td>
<td style="text-align:right;">
0.91
</td>
<td style="text-align:right;">
1.78
</td>
<td style="text-align:right;">
-2.29
</td>
<td style="text-align:right;">
1.78
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.79
</td>
<td style="text-align:right;">
0.92
</td>
<td style="text-align:right;">
1.98
</td>
<td style="text-align:right;">
-2.02
</td>
<td style="text-align:right;">
-2.02
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.59
</td>
<td style="text-align:right;">
0.59
</td>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
4.29
</td>
<td style="text-align:right;">
-1.30
</td>
<td style="text-align:right;">
-1.30
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.88
</td>
<td style="text-align:right;">
2.52
</td>
<td style="text-align:right;">
-1.66
</td>
<td style="text-align:right;">
-1.66
</td>
</tr>
</tbody>
</table>
<html>
<body>
<h2 style="color:navy" >
<strong>Step 3: Estimate the Fluctuation Parameter
</h1>
</strong>
</body>
</html>
<html>
<head>
<title>
HTML Image as link
</title>
</head>
<body>
<a href="">
<img alt="cheatsheet" src="/img/bear_with_me.jpg"
               style="float:right; padding-left:40px; padding-top:20px
         width=55%">
</a>
</body>
</html>
<p>The next step is to fit a model to help us solve for a <strong>fluctuation parameter</strong>. Fair warning: what we have to do for this is easy to code, but the <strong>why</strong> and <strong>how</strong> this works is too complicated for this blog post. I’ll attempt to explain at a high level after I show the step.</p>
<p>Practically, we fit a logistic regression with only one covariate, <span class="math inline">\(H(A,W)\)</span>, and the initial outcome estimate, <span class="math inline">\(\mathrm{\hat{E}}[Y|A,W]\)</span>, as the intercept. The outcome of the logistic regression is the observed outcome, <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[logit(\mathrm{E}[Y|A,W]) = logit(\mathrm{\hat{E}}[Y|A,W]) + \epsilon H(A,W)\]</span></p>
<p>Two technical points for application: since we’re fitting a logistic regression, the initial estimate <span class="math inline">\(\mathrm{\hat{E}}[Y|A,W]\)</span> needs to be on the <span class="math inline">\(logit\)</span> scale (so we use <code>qlogis</code> to transform the probabilities). Also, the <code>R</code> code for a fixed intercept is <code>-1 + offset(fixed_intercept)</code>.</p>
<p><img src="/img/tmle/10_logistic_regression.png" /></p>
<pre class="r"><code>glm_fit &lt;- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle, family=binomial)</code></pre>
<p><em>Note that even when the outcome is not binary, this will still be a logistic regression.</em></p>
<p>Next we need to save the coefficient from that logistic regression, which we will call <span class="math inline">\(\hat{\epsilon}\)</span>:</p>
<p><img src="/img/tmle/11_epsilon.png" style="width:40.0%" /></p>
<pre class="r"><code>eps &lt;- coef(glm_fit)</code></pre>
<!-- The logistic regression in this case is unrelated to the outcome type; it is used because we want to solve for the log likelihood (remember in parametric MLE, you take the derivative of the log-likelihood to get the score...?) It's related to that. -->
<p>Okay, so why did we do that? The way we set up that logistic regression model was a trick to solve for the <strong>Efficient Influence Function</strong> (EIF) of our estimate, which tells us two important and related pieces of information:</p>
<p><strong>1. How much to change, or fluctuate, the initial estimates of the expected outcome.</strong> We knew those initial outcome regressions were wrong, because their bias and variance were optimized for the outcome, not the effect of treatment. The fluctuation parameter, <span class="math inline">\(\hat{\epsilon}\)</span>, combined with information about the treatment mechanism, <span class="math inline">\(H(A,W)\)</span>, will help us update the outcomes to get closer to the true value of the final estimand, the ATE.</p>
<p><strong>2. How much influence each observation has on the final estimate.</strong> The reason we’re able to get standard errors for inference even after using data-adaptive machine learning algorithms like the superlearner is because the TMLE estimator has an EIF (not all estimators do). We’ll be able to use it, just like we use the score function in parametric MLE, to estimate our standard errors.</p>
<p>There’s a lot of semi-parametric theory behind this over-simplified explanation. If you’re interested in learning more, I’ve linked my favorite resources at the end of the post.</p>
<!-- Intuitively, we do this step because we know the initial outcome estimates we used machine learning models to fit have the wrong bias-variance tradeoff for estimating the effect of treatment. We're trying to figure out how much we need to change those initial estimates of the outcome, using information about treatment, to achieve the correct bias-variance trade-off for our final statistical estimand of interest, the ATE.  -->
<html>
<body>
<h2 style="color:navy" >
<strong>Step 4: Update the Initial Estimates of the Expected Outcome
</h1>
</strong>
</body>
</html>
<p>We’ve done all the hard parts, so now all we have to do is update our estimates of the expected outcome using our fluctuation parameter, <span class="math inline">\(\hat{\epsilon}\)</span>.</p>
<p>To do this, we need to put the expected outcome estimates on the <span class="math inline">\(logit\)</span> scale (using <code>qlogis()</code>) because that’s the scale we used to solve the fluctuation parameter in Step 3. Then we can add our update: <span class="math inline">\(\hat{\epsilon} \times H(A,W)\)</span>. Finally, we can put the updated estimates back on the true outcome scale (in the case of the binary outcome, probability, using <code>plogis()</code>).</p>
<p>We can use <span class="math inline">\(expit\)</span> to show the inverse of the <span class="math inline">\(logit\)</span> function, and we will denote updates to the outcome regressions as <span class="math inline">\(\hat{\mathrm{E}}^*\)</span> instead of <span class="math inline">\(\hat{\mathrm{E}}\)</span>.</p>
<p><strong>1. Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders.</strong></p>
<p><span class="math display">\[\hat{\mathrm{E}}^*[Y|A,W] = expit(logit(\mathrm{\hat{E}}[Y|A,W]) + \hat{\epsilon}H(A,W))\]</span></p>
<p><img src="/img/tmle/update_qAW.png" style="width:60.0%" /></p>
<pre class="r"><code>H_A &lt;- dat_tmle$H_A # for cleaner code in Q_A_update
Q_A_update &lt;- plogis(qlogis(Q_A) + eps*H_A)</code></pre>
<p><strong>2. Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment.</strong></p>
<p><span class="math display">\[\hat{\mathrm{E}}^*[Y|A=1,W] = expit(logit(\mathrm{\hat{E}}[Y|A=1,W]) + \hat{\epsilon}H(A,1))\]</span>
<img src="/img/tmle/12_update_Q1.png" style="width:70.0%" /></p>
<pre class="r"><code>Q_1_update &lt;- plogis(qlogis(Q_1) + eps*H_1)</code></pre>
<p><strong>2. Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment.</strong></p>
<p><span class="math display">\[\hat{\mathrm{E}}^*[Y|A=0,W] = expit(logit(\mathrm{\hat{E}}[Y|A=0,W]) + \hat{\epsilon}H(A,0))\]</span>
<img src="/img/tmle/13_update_Q0.png" style="width:70.0%" /></p>
<pre class="r"><code>Q_0_update &lt;- plogis(qlogis(Q_0) + eps*H_0)</code></pre>
<html>
<body>
<h2 style="color:navy" >
<strong>Step 5: Compute the Statistical Estimand of Interest
</h1>
</strong>
</body>
</html>
<p>Now that we have updated expected outcome estimates, we can compute the ATE as the mean difference in the updated outcome estimates under treatment and no treatment:</p>
<p><span class="math display">\[\hat{\Psi} = \hat{ATE} = \hat{E}_W[\hat{E^*}[Y|A=1,W] - \hat{E^*}[Y|A=0,W]]\]</span></p>
<p><img src="/img/tmle/14_compute_ATE.png" style="width:60.0%" /></p>
<pre class="r"><code>tmle_ate &lt;- mean(Q_1_update - Q_0_update)
tmle_ate</code></pre>
<pre><code>## [1] 0.1281863</code></pre>
<p>We could interpret our estimate as, “the average treatment effect was estimated to be 12.8%.” If causal assumptions were not met, we would say, “the proportion of observations who experienced the outcome <span class="math inline">\(Y\)</span>, after adjusting for baseline confounders, was estimated to be 12.8% higher for those who received treatment compared to those who did not.”</p>
<html>
<body>
<h2 style="color:navy" >
<strong>Step 6: Calculate the Standard Errors, Confidence Intervals, and P-values
</h1>
</strong>
</body>
</html>
<p>To find the standard error (SE) of a TMLE estimate, we first compute the <strong>Influence Curve</strong> (IC). This is what we solved for when we fit the logistic regression in Step 3; we wanted to find the <span class="math inline">\(\epsilon\)</span> that solved for the Efficient Influence Function (EIF). The terms EIF and IC are, for most intents and purposes, used interchangably; the latter tends to refer to an estimate with actual data.</p>
<p>Anyways, we can compute the IC for our estimate, which tells us how much influence each observation has on the estimate. This is the semi-parametric equivalent to the score function in parametric MLE.</p>
<p><span class="math display">\[\hat{IC} = (Y-\hat{E^*}[Y|A,W])H(A,W) + \hat{E^*}[Y|A=1,W] - \hat{E^*}[Y|A=0,W] - \hat{ATE}\]</span></p>
<pre class="r"><code>ic &lt;- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate</code></pre>
<p>Once we have the IC, we can take the square-root of its variance divided by the number of observations to get the standard error of our estimate.</p>
<p><img src="/img/tmle/15_ses.png" style="width:100.0%" /></p>
<p><span class="math display">\[\hat{SE} = \sqrt{\frac{var(\hat{IC})}{N}} \]</span></p>
<pre class="r"><code>tmle_se &lt;- sqrt(var(ic)/nrow(dat_obs))</code></pre>
<p>Once we have the standard error, we can easily get the 95% confidence interval and p-value of our estimate.</p>
<pre class="r"><code>ci_lo &lt;- tmle_ate - 1.96*tmle_se
ci_hi &lt;- tmle_ate + 1.96*tmle_se

pval &lt;- 2 * (1 - pnorm(abs(tmle_ate / tmle_se)))</code></pre>
<p>Then we can successfully report our ATE as 0.128 (95% CI: 0.103, 0.153).</p>
<p>Whew, that was a lot! Luckily there are <code>R</code> packages so that you don’t have to hand code TMLE yourself. <code>R</code> packages to implement the TMLE algorithm include <a href="https://www.jstatsoft.org/article/view/v051i13"><code>tmle</code></a>, <a href="https://tlverse.org/tlverse-handbook/tmle3.html"><code>tmle3</code></a>, <a href="https://www.jstatsoft.org/article/view/v081i01"><code>ltmle</code></a>, and <a href="https://htmlpreview.github.io/?https://gist.githubusercontent.com/nt-williams/ddd44c48390b8d976fad71750e48d8bf/raw/45db700a02bf92e2a55790e60ed48266a97ca4e7/intro-lmtp.html"><code>lmtp</code></a>.</p>
</div>
</div>
<div id="properties-of-tmle" class="section level1">
<h1>5. Properties of TMLE</h1>
<p>TMLE is a <strong>doubly robust</strong> estimator, which means that if either the regression to estimate the expected outcome, or the regression to estimate the probability of treatment, are correctly specified (formally, their bias goes to zero as sample size grows large, meaning they are <strong>consistent</strong>), the final TMLE estimate will be consistent.</p>
<p>If both regressions are consistent, the final estimate will reach the smallest possible sample variance in the fewest number of observations (formally: it will be <strong>efficient</strong>). The reason we use superlearning for estimating the outcome and treatment models fits is to give us the best possible chance of having correctly specified models.</p>
<p><img src="/img/tmle_props.png" /></p>
</div>
<div id="references" class="section level1">
<h1>6. References</h1>
<p>If you’d like to learn more, I recommend the following resources:</p>
<div id="tmle" class="section level3">
<h3>TMLE</h3>
<ul>
<li><p>The paper I referred to most often while learning TMLE was <a href="https://academic.oup.com/aje/article/185/1/65/2662306"><em>Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies</em></a> by Megan S. Schuler and Sherri Rose. It has a nice step-by-step written explanation and Figure 1 is a good summary of how TMLE compares to other common estimation methods in causal inference.</p></li>
<li><p>I also really like the written explanations in the <a href="https://link.springer.com/book/10.1007/978-1-4419-9782-1"><em>Targeted Learning</em></a> book by Mark van der Laan and Sherri Rose. The notation was too difficult for me to follow, but the words themselves make a lot of sense.</p></li>
<li><p>Miguel Luque wrote an <a href="https://migariane.github.io/TMLE.nb.html">excellent bookdown tutorial on TMLE</a>, also with step-by-step <code>R</code> code. It is more technical and thorough than my post, but still aimed at an applied audience.</p></li>
</ul>
</div>
<div id="semiparametric-theory" class="section level3">
<h3>Semiparametric Theory</h3>
<ul>
<li><p>Edward Kennedy has several well-written pieces on semiparametric estimation in causal inference. I recommend starting with:</p>
<ul>
<li><p>His introductory paper on <a href="https://arxiv.org/pdf/1709.06418.pdf">Semiparametric Theory</a></p></li>
<li><p>His <a href="http://www.ehkennedy.com/uploads/5/8/4/5/58450265/unc_2019_cirg.pdf">slideshow tutorial</a> <em>Nonparametric efficiency theory and machine learning in causal inference</em></p></li>
</ul></li>
</ul>
</div>
<div id="influence-functions" class="section level3">
<h3>Influence Functions</h3>
<ul>
<li>My favorite resource so far for learning specifically about influence functions has been <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2020.1717620">Visually Communicating Influence Functions</a> by Aaron Fisher and Edward Kennedy. However, this paper didn’t make sense to me until I worked through this <a href="https://observablehq.com/@herbps10/one-step-estimators-and-pathwise-derivatives">interactive tutorial</a> by Herb Susmann. I suggest playing around with the interactive examples first, and then trying to work through the paper.</li>
</ul>
</div>
<div id="causal-inference" class="section level3">
<h3>Causal Inference</h3>
<ul>
<li><p>If you want to learn more about the foundations of causal inference, I suggest <a href="http://bayes.cs.ucla.edu/PRIMER/"><em>Causal Inference in Statistics: A Primer</em></a> by Judea Pearl and <a href="https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/"><em>What If</em></a> (Part I) by Miguel Hernan and James Robins. These are both good starters for learning about the <em>identification</em> side of causal inference.</p></li>
<li><p>I also think the previously mentioned <em>Targeted Learning</em> book does a good job of setting up the goals and steps of causal inference.</p></li>
</ul>
<p>I’ll continue to update this page with resources as I discover them.</p>
<p>Feedback on this post is welcome, either from the new learners of TMLE or experts in causal inference. The best way to reach me is through <a href="mailto:kathoffman.stats@gmail.com">email</a>. This is just a side hobby of mine, so please be patient with my response time. :-)</p>
<p>Thank you to my colleague Iván Díaz for answering my many, many questions on TMLE, and to Miguel Luque for very helpful feedback on the visual guide.</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] SuperLearner_2.0-26 nnls_1.4            kableExtra_1.1.0   
##  [4] forcats_0.5.0       stringr_1.4.0       dplyr_1.0.2        
##  [7] purrr_0.3.4         readr_1.3.1         tidyr_1.1.2        
## [10] tibble_3.0.3        ggplot2_3.3.2       tidyverse_1.3.0    
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.5         lubridate_1.7.9    lattice_0.20-38    plotmo_3.5.7      
##  [5] earth_5.1.2        assertthat_0.2.1   glmnet_3.0-2       digest_0.6.25     
##  [9] foreach_1.5.0      ranger_0.12.1      R6_2.4.1           cellranger_1.1.0  
## [13] backports_1.1.8    reprex_0.3.0       evaluate_0.14      httr_1.4.1        
## [17] highr_0.8          blogdown_0.19      pillar_1.4.6       TeachingDemos_2.12
## [21] rlang_0.4.7        readxl_1.3.1       rstudioapi_0.11    Matrix_1.2-18     
## [25] rmarkdown_2.1      webshot_0.5.2      munsell_0.5.0      broom_0.7.0       
## [29] compiler_3.6.3     modelr_0.1.6       xfun_0.14          pkgconfig_2.0.3   
## [33] shape_1.4.4        htmltools_0.4.0    tidyselect_1.1.0   bookdown_0.19     
## [37] codetools_0.2-16   fansi_0.4.1        viridisLite_0.3.0  crayon_1.3.4      
## [41] dbplyr_1.4.3       withr_2.2.0        cabinets_0.6.0     grid_3.6.3        
## [45] jsonlite_1.6.1     gtable_0.3.0       lifecycle_0.2.0    DBI_1.1.0         
## [49] magrittr_1.5       scales_1.1.1       cli_2.0.2          stringi_1.4.6     
## [53] fs_1.4.1           xml2_1.3.0         ellipsis_0.3.1     generics_0.0.2    
## [57] vctrs_0.3.4        Formula_1.2-3      iterators_1.0.12   tools_3.6.3       
## [61] glue_1.4.2         hms_0.5.3          plotrix_3.7-7      yaml_2.2.1        
## [65] colorspace_1.4-1   rvest_0.3.5        knitr_1.28         haven_2.2.0</code></pre>
</div>
</div>
