---
title: "An Illustrated Guide to TMLE, Part I: An Analyst's Motivation to Learn"
author: "Katherine Hoffman"
date: 2020-12-11T00:13:14-05:00
draft: false
categories: ["statistics","R"]
math: true
tags: ["statistics","causal inference","R","TMLE","Targeted Maximum Likelihood Estimation","introduction to TMLE","targeted learning"]
output:
  blogdown::html_page:
    toc: false
    toc_depth: 1
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<blockquote>
<p>The first post of a three-part series to help beginners and/or visual learners understand Targeted Maximum Likelihood Estimation (TMLE). This section contains a brief overview of the targeted learning framework and motivation for semiparametric estimation methods for inference, including causal inference.</p>
</blockquote>
<hr />
<div id="table-of-contents" class="section level1">
<h1>Table of Contents</h1>
<p>This blog post series has three parts:</p>
<div id="part-i-motivation" class="section level3">
<h3>Part I: Motivation</h3>
<ol style="list-style-type: decimal">
<li><a href="#tmle-in-three-sentences">TMLE in three sentences 🎯</a></li>
<li><a href="#an-analysts-motivation-for-learning-tmle">An Analyst’s Motivation for Learning TMLE 👩🏼‍💻</a></li>
<li><a href="#is-tmle-causal-inference">Is TMLE Causal Inference? 🤔</a></li>
</ol>
</div>
<div id="part-ii-algorithm" class="section level3">
<h3>Part II: Algorithm</h3>
<ol start="4" style="list-style-type: decimal">
<li><a href="https://khstats.com/blog/tutorial/tutorial-pt2/#why-the-visual-guide">Why the Visual Guide? 🎨</a></li>
<li><a href="https://khstats.com/blog/tutorial/tutorial-pt2/#tmle-step-by-step">TMLE, Step-by-Step 🚶🏽</a></li>
<li><a href="https://khstats.com/blog/tutorial/tutorial-pt2/#using-the-tmle-package">Using the <code>tmle</code> package 📦</a></li>
</ol>
</div>
<div id="part-iii-evaluation" class="section level3">
<h3>Part III: Evaluation</h3>
<ol start="7" style="list-style-type: decimal">
<li><a href="https://khstats.com/blog/tutorial/tutorial-pt3/#properties-of-tmle">Properties of TMLE 📈</a></li>
<li><a href="https://khstats.com/blog/tutorial/tutorial-pt3/#why-does-tmle-work">Why does TMLE work? ✨</a></li>
<li><a href="https://khstats.com/blog/tutorial/tutorial-pt3/#resources-to-learn-more">Resources to learn more 🤓</a></li>
</ol>
<hr />
</div>
</div>
<div id="tmle-in-three-sentences" class="section level1">
<h1>TMLE in three sentences 🎯</h1>
<p>Targeted Maximum Likelihood Estimation (TMLE) is a general semiparametric estimation framework to <strong>estimate a statistical quantity of interest</strong>. TMLE allows the use of <strong>machine learning</strong> (ML) models which place <strong>minimal assumptions on the distribution of the data</strong>. Unlike estimates normally obtained from ML, the <strong>final TMLE estimate will still have valid standard errors for statistical inference</strong>.</p>
</div>
<div id="an-analysts-motivation-for-learning-tmle" class="section level1">
<h1>An Analyst’s Motivation for Learning TMLE 👩🏼‍💻</h1>
<p>When I graduated with my MS in Biostatistics two years ago, I had a mental framework of statistics and data science that I think is pretty common among new graduates. It went like this:</p>
<ol style="list-style-type: decimal">
<li><p>If the goal is <span style="color: #3366ff;">inference</span> (e.g., an effect size with a confidence interval), use an <span style="color: #3366ff;">interpretable, usually parametric, model</span> and explain what the coefficients and their standard errors mean.</p></li>
<li><p>If the goal is <span style="color: #cc0000;">prediction</span>, use <span style="color: #cc0000;">data-adaptive machine learning algorithms</span> and then look at performance metrics and perhaps variable importance, with the understanding that standard errors, and sometimes even coefficients, do not exist.</p></li>
</ol>
<p>This mentality changed drastically when I started learning about semiparametric estimation methods like TMLE in the context of causal inference. I quickly realized two flaws in this mental framework.</p>
<p>First, I was thinking about inference backwards: I was choosing a model based on my outcome type (binary, continuous, time-to-event, repeated measures) and then interpreting specific coefficients as my estimates of interest. Yet it makes way more sense to <em>first</em> determine the statistical quantity, or <strong>estimand</strong>, that best answers a scientific question, and <em>then</em> use the method, or <strong>estimator</strong>, best suited for estimating it. This is the paradigm TMLE is based upon: <strong>we want to build an algorithm, or estimator, targeted to an estimand of interest</strong>.</p>
<figure>
<img src="/img/tmle/estimator.png"
    width= 70%
         alt="Estimator and Estimand">
<figcaption>
<em>An estimand is a quantity that answers a scientific question of interest. Once we figure out the estimand, we can build an algorithm, or estimator, to estimate it. Image courtesy of Dr. Laura Hatfield and <a href="https://diff.healthpolicydatascience.org/">diff.healthpolicydatascience.org</a>.</em>
</figcaption>
</figure>
<p>Second, I thought flexible, data-adaptive statistical models we commonly classify as <strong>machine learning</strong> (e.g. LASSO, random forests, gradient boosting, etc.) could only be used for prediction, since they don’t have <strong>asymptotic properties for inference</strong> (i.e. standard errors). However, certain <strong>semiparametric estimation methods</strong><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> like TMLE can actually use these models to <strong>obtain a final estimate that is closer to the target quantity</strong> than would be obtained using standard parametric models (e.g. linear and logistic regression). This is because machine learning models are generally designed to accommodate <strong>large numbers of covariates</strong> with <strong>complex, non-linear relationships</strong>.</p>
<img src="/img/tmle/parametric_assumptions_comic.jpg" width=80%>
<figcaption>
<em>Semiparametric estimation methods like TMLE can rely on machine learning to avoid making unrealistic parametric assumptions about the underlying distribution of the data (e.g. multivariate normality).</em>
</figcaption>
<p>The way we use the machine learning estimates in TMLE, surprisingly enough, yields <strong>known asymptotic properties of bias and variance</strong> – just like we see in parametric maximum likelihood estimation – for our target estimand.</p>
<p>Besides allowing us to compute 95% confidence intervals and p-values around our estimands of interest even after using ML “under the hood,” TMLE boasts a number of other beneficial properties, such as double robustness. These are discussed further in <a href="https://khstats.com/blog/tutorial/tutorial-pt3/"><em>Part III</em></a>.</p>
</div>
<div id="is-tmle-causal-inference" class="section level1">
<h1>Is TMLE Causal Inference? 🤔</h1>
<p>If you’ve heard about TMLE before, it was likely in the context of <strong>causal inference</strong>. Although TMLE was developed for causal inference due to its many attractive statistical properties, it cannot be considered causal inference by itself. Causal inference is a two-step process that first requires <strong>causal assumptions</strong><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> before a statistical estimand can be interpreted causally.</p>
<p><strong>TMLE can be used to estimate various statistical estimands</strong> (odds ratio, risk ratio, mean outcome difference, etc.) <strong>even when causal assumptions are not met</strong>. TMLE is, as its name implies, simply a tool for estimation.</p>
<p>In <a href="https://khstats.com/blog/tutorial/tutorial-pt2/"><em>Part II</em></a>, I’ll walk step-by-step through a basic version of the TMLE algorithm: estimating the mean difference in outcomes, adjusted for baseline confounders, for a binary outcome and binary treatment. If causal assumptions are met, this is called the <strong>Average Treatment Effect (ATE)</strong>, or the mean difference in outcomes for a world in which everyone had received the treatment compared to a world in which everyone had not.</p>
<p>⤴️<a href="#table-of-contents"><em>Back to the top</em></a></p>
<hr />
<div id="references" class="section level3">
<h3><em>References</em></h3>
<p>My primary reference for all three posts is <em>Targeted Learning</em> by Mark van der Laan and Sherri Rose. I detail many other resources I’ve used to learn TMLE, semiparametric theory, and causal inference in <a href="https://khstats.com/blog/tutorial/tutorial-pt3/"><em>Part III</em></a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>An informal way to think about semiparametric estimation is that we’re allowing at least one of the parameters in the model to go without a particular functional form. You can contrast this to linear regression, which is parametric and assumes that each coefficient is normally distributed with mean 0 and a standard deviation of <span class="math inline">\(\sigma^2\)</span>. I’ve found this <a href="https://sites.stat.washington.edu/jaw/JAW-papers/NR/jaw-BKR-EncylSS.pdf">review of semiparametric inference</a> to be helpful.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>I won’t discuss causal assumptions in these posts, but this is referring to foundational concepts in causal inference like consistency, exchangeability, and positivity. A primary motivation for using TMLE and other semiparametric estimation methods for causal inference is that once you’ve spent time carefully designing a study and evaluating these assumptions, it does not make sense to risk damaging an otherwise well-designed analysis by making an unrealistic assumption about the underlying distribution of the data.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
