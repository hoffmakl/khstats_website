---
title: "What in tarnation is Targeted Maximum Likelihood Estimation? A Visual Guide and Introductory R Tutorial on TMLE"
author: "Katherine Hoffman"
draft: true
date: 2020-10-10T21:13:14-05:00
categories: ["R"]
tags: ["R"]
output: 
  html_document:
    toc: true
    toc_float: true
    smart: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> Are you an applied statistician? Are you trying to be a *better* applied statistician and implement causal inference methods like propensity scores but you‚Äôre overwhelmed and don‚Äôt know where to start?! Check out the following tutorial on TMLE aimed at applied scientists with statistics training.

***

# TMLE in two sentences

- The most basic form of Targeted Maximum Likelihood Estimation (TMLE) allows you to estimate treatment effects of interest using:

  1) the expected outcome of an individual, given the treatment they received and their baseline confounders
  
  2) the probability an individual received the treatment of interest, given their baseline confounders [propensity score]
  
- These estimates can come regressions we commonly classify as machine learning (ML), but the overall estimate of the treatment effect will still have **valid confidence intervals**, unlike the estimates you normally get from ML

***

# Is TMLE Causal Inference?

TMLE was *developed* to answer causal inference questions because it hones in on a treatment effect rather than

However, the estimates from TMLE do not have a causal interpretation themselves. TMLE is merely a tool we use to 





A recent past-self of mine would‚Äôve replied yes, yes, and yes. However, I have spent the past year learning and implementing causal inference methods as a Masters-level research biostatistician, and I‚Äôm excited to share my favorite method, Targeted Maximum Likelihood Estimation, or TMLE. I've tried to explain at the minimum level I think is necessary for an applied statistician to implement it in hopes that others will feel empowered to try it out.

*A warning:* There is no math in this post... üòá If you want to learn about the math behind TMLE, I highly recommend the paper ["Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies"](https://academic.oup.com/aje/article/185/1/65/2662306) by Megan S. Schuler and Sherri Rose, or for a deeper dive, the book ["Targeted Learning"](https://link.springer.com/book/10.1007/978-1-4419-9782-1) by Mark van der Laan and Sherri Rose. For broader introductions to causal inference, I like both Judea Pearl‚Äôs ["Causal Inference in Statistics: A Primer"](http://bayes.cs.ucla.edu/PRIMER/) and Miguel Hernan and James Robins‚Äô ["Causal Inference: What If"](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2019/11/ci_hernanrobins_10nov19.pdf) books.

**This article is solely meant to help someone understand TMLE on a conceptual or applied level, and then implement it in R for a statistical analysis.** Before I give a motivation for TMLE...

# A quick overview of causal inference and propensity scores

*Causal inference* refers to the many tools available to us to help answer questions like "what would have happened if we had done *this* instead of *that*?"  To truly answer these questions, we‚Äôd need to rewind time to see what would have happened if we‚Äôd done something differently. In a world without Time-Turners, our next best option is to set up an experiment where individuals randomly get *this* treatment, or *that* treatment, and then look at the differences in what happened to the two groups. In industry research, that‚Äôs called A/B testing, and in medical research, it‚Äôs a randomized control trial.

Due to practicality and ethics, this sort of experiment is not always possible. Sometimes our only option is to use the data we already have, which records what happened under various treatments that were not randomized. As statisticians, we know there‚Äôs a lot of bias in observational data like this, because individuals received (or even chose) their treatment for a myriad of reasons. We try to control for the factors that affected an individual's treatment and outcome by adjusting for confounders in our regression model.

The issue with an estimator like a generalized linear model is that its only purpose is to minimize the distance between a treatment, confounders, and an outcome. The interpretation of the estimated treatment coefficient can only ever be "the average difference in outcome for an individual who recieved *this* vs. *that* treatment, *holding all other covariates constant*," which is rarely an interpretation we can utilize to make future clinical, economic, etc. decisions.

**Propensity scores are one of the first ways statisticians came up with to help better answer the *this* vs. *that* question using observational data.** The idea of propensity scores is to use an estimator -- often as simple as a logistic regression model -- to get a probability estimate, or "score," which represents an individual's likelihood of receiving a treatment given their baseline covariates. We then use that score in some way to try to reduce bias in our calculation of the treatment effect.

**In medicine, the intuitive understanding of propensity scores is that we want to make the estimates from our observational data closer to what they would be if they came from data on a perfectly executed randomized clinical trial.**

A few common ways of using a propensity score, or probability estimate, that you may have heard of before:

  1) **Matching** is when you run your analysis on a subgroup of individuals in which every person who did receive the treatment has a ‚Äúmatching‚Äù person with a similar propensity score, but who did not receive the treatment.
  
  2) **Inverse probability of treatment weighting (IPTW):** uses the propensity score to down-weight patients who did receive the treatment, or treatment, and had a high probability of receiving the treatment, and up-weight patients who did not receive the treatment, but who had a high probability of receiving the treatment given their baseline covariates.

If we think a bit about the first option, matching, it should become obvious that by taking a subgroup of our population in this way, we are making our estimates of the treatment effect extremely prone to bias. IPTW is usually the better option of the two, and if you‚Äôd like to read a good blog post on understanding IPTW better with examples in R, I recommend Lucy D'Agostino McGowan‚Äôs ["Understanding Propensity Score Weighting."](https://livefreeordichotomize.com/2019/01/17/understanding-propensity-score-weighting/)

However, there‚Äôs actually another four letter acronym out there that far fewer statisticians understand or have even heard of... TMLE! Like propensity scores, TMLE aims to help solve the issue of imbalanced treatments and biased effect of treatment estimates. It is less intuitive, but it has statistical properties that give it an advantage over many easier-to-explain methods.

# The TMLE Algorithm

The TMLE algorithm uses two estimates:

  1) the estimated probability an individual receives a treatment of interest, predicted by their baseline covariates (their propensity score)

  2) the estimated outcome of an individual, predicted by their treatment and baseline covariates
  
and uses those prediction estimates to estimate the average difference in outcomes if everyone had received the treatment, compared to if everyone had not received the treatment.

The TMLE algorithm is complex, but it's often helpful for me to think of it as analagous to the iterative Expectation-Maximization (EM) algorithm in maximum likelihood estimation. Essentially, you start with the estimate of an individual's outcome given their treatment and baseline covariates, and you update that estimate using the probability an individual received the treatment given their baseline covariates. Every time you update that estimate, you remove more of the bias that naturally exists in your observational data.

What do I mean by "updating the estimate"? Basically, there's another parameter the TMLE algorithm uses, very similar to inverse probability weighting, which is determined by whether someone actually received the treatment, divided by their probability estimate for receiving the treatment. *That* parameter gets used to update (and debias) the outcome estimate in an iterative process. The update, behind the scenes, is to (re)predict the actual outcome using a regression model with the old estimates of the outcome as a fixed intercept. This updating process continues until the estimate of the outcome given the treatment and baseline confounders converges, or no longer changes.

**Through this iterative updating process, the TMLE algorithm *targets* the effect of the treatment on the outcome, and thus reduces bias and treatment imbalance that naturally exist in observational data.** The final estimate is for the "average treatment effect," which can be directly interpreted as the difference in outcomes if everyone had received the treatment compared to if everyone had not received the treatment.

**If you feel a bit lost, the wonderful news is that you don't need a detailed understanding of the TMLE algorithm or its properties as an applied statistician who wants to implement TMLE right now!** However, it's very interesting, and if you'd like to understand more, you should definitely read the aforementioned references.

# Why should I bother with TMLE?

Before I show an applied example of TMLE, I want to explain why I prefer TMLE to other propensity score estimation methods. In short, it is because because you don't need to fit any certain type of regression to get your final estimates. You can use any estimator you want -- a generalized linear model, splines, random forests, gradient boosting, neural nets, honestly anything -- to get the initial probability estimates. You can actually use *all* of those estimators to get the two probability estimates, if you want! But more on that later.

The benefit of expanding your toolbox of potential estimators is that most estimators are built with prediction in mind, and thus yield very good probability estimates to initiate our TMLE algorithm. When our goal as statisticians is to calculate effects on an outcome attributable to a treatment, it's easy to shy away from these prediction-focused types of estimators, because most of them do not have any statistical theory to allow us to calculate valid confidence intervals.

**A huge selling point of TMLE is that it allows you to utilize prediction-focused (often non-parametric) estimators, but still obtain valid confidence intervals on your final estimate of the treatment's effect.** TMLE utilizes concepts from semi-parametric influence function theory to determine valid standard errors, and therefore valid confidence intervals, on estimates of treatment effects. This is not important unless you plan to tackle the math of TMLE, but if you do decide to venture into technical explanations, know that you'll see references to influence functions *a lot*!

In other propensity score methods, like IPTW, we can only obtain valid confidence intervals if we obtain our propensity score using pre-specified parametric models. For example, we fit a logistic regression for our treatment predicted by five pre-specified baseline covariates. Using a logistic model like this to calculate the propensity score is not only placing a strong distributional assumption on our data, but it is limited in its ability to take in high-, or even medium-dimensional data. When we use estimators that are well equipped for very "wide" data and perform variable selection, we obtain better overall estimates for our treatment effect of interest.

There are a few other major benefits to TMLE. For one, it has very good bias-variance properties, and those properties are "robust," or "resistant to" model mispecification, i.e. having the wrong type of estimator or the wrong variables in your estimation of either the treatment or outcome. Another benefit is that if you are able to go through a causal identification process of the research question and available data (a concept I won't discuss further, since it's a separate realm of the problem), you'll have a causal interpretation of your average treatment effect estimate.

# A Real World Application of TMLE

```{r}
library(dplyr, warn.conflicts=F)
```

Here I'll be using a data set from the ???. EXPLAIN DATA aND goal

I chose this data set because it is the same data used in a really good resource if you want to do a deeper dive in Targeted Learning:

```{r}
washb_data <- data.table::fread("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv",
                    stringsAsFactors = TRUE)
```

Before we can run a TMLE analysis, we need to clean the data a bit. The `tmle` function requires a binary treatment, so I'll need to turn the treatment data from character strings to binary 0/1 variables. I'm only interested right now in women who recieved the treatment of Nutrition and Hand washing, and comparing that to the control women, so I'll filter out the appropriate subjects and make my data binary.

To use `tmle`, your data structure should be:

  - Wide (each row is one observation)
  
  - No factors (all categorical variables should be transformed to dummy/indicator columns using a function like `model.matrix()`
  
  - No missing data (I usually make a new column indicating whether the value was missing, and then impute at the mean or median for continuous variables)

```{r}
dat_clean <- 
  washb_data %>%
  filter(tr %in% c("Control", "Nutrition + WSH")) %>%
  mutate(tr = case_when(tr == "Control" ~ 0,
                        TRUE ~ 1)) %>%
  mutate_at(vars(one_of(c("momage","momheight"))), list(miss =~ ifelse(is.na(.), 1, 0))) %>%
  mutate_at(vars(one_of("momage","momheight")), list( ~ ifelse(is.na(.), mean(., na.rm=T), .))) %>%
  model.matrix(~ . + 0, data = .) %>%
  as.data.frame()
```

Once I've cleaned my data, I make vectors specifying my treatment, outcome, and baseline covariates. In the TMLE literature, and in this package, the notations are:

  - Outcome: `Y`

  - Treatment: `A`

  - Confounders: `W`
  
In this data set, our outcome is the variable `whz`, our treatment is the variable `tr`, and our baseline confounders are all the other variables in our data set.

```{r}
Y <- dat_clean$whz
A <- dat_clean$tr
W <- dat_clean %>% dplyr::select(-whz, -tr)
```

Once we've done that, we can get our TMLE estimate of the average treatment effect! Here, I'm only inputting the 

```{r, eval=F}
tmlefit_default <- tmle::tmle(Y, A, W)
tmlefit_default
```

Here, I'm going to specify the kinds of estimators I want to use to get my two important estimates: the expected outcome and the probability of the exposure.

I've chosen to call the functions `glm` for a generalized linear model, `glmnet` for penalized regression, `ranger` for random forests, and `xgboost` for extreme gradient boosting.

I'm going to use *all* of these estimators to estimate the 

```{r}
SL_library <- c("SL.glm",
                "SL.glmnet",
                "SL.ranger",
                "SL.xgboost")
```

We're going to specify those libraries in the arguments `Q.SL.library` and `g.SL.library`. These arguments sound a bit scary, but they are just what the notation in TMLE literature is -- Q refers to the estimation for the outcome given treatment and covariates, and g refers to the estimation of the probability of the treatment.

```{r, eval=F}
tmlefit <- tmle::tmle(Y, A, W,
                Q.SL.library = SL_library, g.SL.library = SL_library)
tmlefit
```


To understand how the various machine learning models are combining, you should look into a type of ensemble learning called stacking, or "superlearning." I have a slideshow that you could look at [here](https://github.com/hoffmakl/sl3-demo/blob/master/superlearning_slides_no_animation.pdf), but there are plenty of other resources online.


This is obviously not a super technical post, and it was not intended to be, but I hope that it may catch your interest to learn more about the complexities of TMLE and try it out in your next analysis. I plan to write a few similar posts on what to do if your outcomes are survival or longitudinal data. In the meantime, you may find the more technical tutorial on TMLE helpful:



or, "The Hitchiker's Guide to Targeted Learning" is an excellent, still-in-progress, resource for learning TMLE and many other targeted learning.

Learn about causal inference,

we should be estimating answers for questions of actual interest, rather than debiasing our results.

# References:
