---
title: "An Illustrated Guide to Targeted Minimum Loss-based Estimation"
author: "Katherine Hoffman"
draft: true
date: 2020-10-10T21:13:14-05:00
categories: ["statistics","R"]
math: true
tags: ["statistics","causal inference","R","lmtp","TMLE","Targeted Maximum Likelihood Estimation","introduction to TMLE","targeted learning"]
output: 
  html_document:
    toc: true
    toc_float: true
    smart: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> A Visual Guide and Introductory R Tutorial on TMLE aimed at applied scientists with statistics training.

***

# TMLE in two sentences

- The general form of Targeted Maximum Likelihood Estimation (TMLE) allows you to **estimate treatment effects of interest using**:

  1) the expected outcome of an individual, given the treatment they received and their baseline confounders
  
  2) the probability an individual received the treatment of interest, given their baseline confounders; also known as their propensity score
  
- These estimates can come from regressions we commonly classify as machine learning (ML), but the **overall estimate of the treatment effect will still have valid confidence intervals**, unlike estimates normally obtained from ML

***

# FAQ

TMLE is often used in causal inference because...

however, the estimates arising from TMLE do not automatically have a magical causal inference interpretation - that only comes from a process called identification. TMLE deals solely with estimation

**Is TMLE Causal Inference?**

Nope! TMLE is simply an *estimation method* that is often used for causal inference applications. That's because most statisticians who work on causal inference applications spend so. much. time. thinking about the research question of interest.

They're obsessed with drawing out Directed Acyclic Graphs (DAGs) and/or writing out structural equation models (SCMs), thinking about the precise research question and the data available to answer (or not answer) it, and just generally trying to "identify" the causal parameter of interest.

Some of them spend so much time getting the estimate of interest set up correctly that they don't want to "ruin" all that hard work by making strong assumptions about the underlying distribution of their data. They'd prefer to use flexible models for estimation, like the models we typically think of when we hear machine learning.

I won't get into identification in this post, but essentially it is getting to the point that you can say that a parameter of interest (for example, an Odds Ratio or an Average Treatment Effect) can be written in terms of the data you have available

The logic in using an estimation method like TMLE, where you don't have to

# What's the big deal about TMLE?

TMLE is super cool because it allows you to estimate an effect of interest with confidence intervals without having to fit standard parametric regressions (for example, linear, logistic, and poisson regressions). The issue with these types of models is that they make harsh distributions about the underlying distribution of your data.

Do you *really* ever -- like, ever -- believe your outcome has a linear, or log-linear, relationship with its explanatory variables? Let's say your outcome is blood pressure. Maybe for one predictor you can convince yourself that there is a linear relationship with ... but what about when you add five more variables. But ALL OF THEM? Absolutely bananas to think that would hold true.

Nobody whose job is to predict events accurately believes that to be true, that's for sure. I've got no experience working for Google but I'm pretty sure their statisticians, given a massively wide data set, do not think "I bet a plain old linear regression will predict this person's ... best if I feed it these 50 potential predictors!

They are going to use LASSO, or random forest, or neural net, or a wide range of other models!

Statisticians who deal with the need for treatment effect estimates have long been hesitant to adopt these 

These are not the most technical definitions, but hopefully they're enough to jog your memory of your formal training.

**Bias**: The difference between the true value of a parameter and the estimated value.

**Consistency**: The bias of an estimator decreases to 0 as sample size approaches infinity.

**Efficiency**: The variance of an estimator is as small as possible as sample size approaches infinity.

**Double robust**: If either of 

# A very brief background on semi-parametric

Semi-parametric models: models where at least one parameter is not defined. For example, 

# TMLE FAQ

I've been asking *a lot* of questions about TMLE over the past two years.

## I've only heard about TMLE in the context TMLE of causal inference, what's up with that?

TMLE was *developed* to answer causal inference questions, but the estimates from TMLE do not have a causal interpretation by themselves. TMLE is merely a tool, or an algorithm, used to estimate treatment effects.

When approaching a causal inference question (or really any statistical estimation application at all!), the majority of time and effort should often be focused on the underlying scientific model, research question of interest, and data available to answer it.

Once this process -- usually called **identification*** in the causal inference literature -- is completed and an analyst is ready to estimate an effect of interest, she/he has a choice in estimation methods.

You've likely heard of some of these estimation methods: propensity score matching, inverse probability weighting, g-computation (AKA substitution estimation), etc. TMLE is an estimation method just like these, however, it has some very appealing properties that I am convinced make it a better option than the aforementioned methods.

*I won't go into identification in this post, but if you are new to causal inference, my favorite book so far to understand the big picture of identification is Part I of Miguel Hernan and James Robinsâ€™ ["Causal Inference: What If"](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2019/11/ci_hernanrobins_10nov19.pdf) book.

# A comparison with G-Computation and IPTW

# TMLE, real intuitively

All causal inference methods try to use information about why patients were treated (non-randomly, unless it was an RCT) to 

Before I walk through the `R` code for the TMLE algorithm, I want to reinforce the big-picture idea of TMLE: we want to use information about why patients get treatment to update their outcomes 


# The TMLE Algorithm

Step 0

Load libraries, set seed, and simulate data

![](/img/tmle/1_data_structure.png){width=80%}

```{r}
library(tidyverse, quietly=T)
library(SuperLearner, quietly=T)
set.seed(7)

# Superlearner functions from Ivan
#source("sl.r")

# using data generating code from Miguel Angel Luque Fernandez' tutorial
generate_data <- function(n){
    w1 <- rbinom(n, size=1, prob=0.5)
    w2 <- rbinom(n, size=1, prob=0.65)
    w3 <- round(runif(n, min=0, max=4), digits=3)
    w4 <- round(runif(n, min=0, max=5), digits=3)
    A  <- rbinom(n, size=1, prob= plogis(-0.4 + 0.2*w2 + 0.15*w3 + 0.2*w4 + 0.15*w2*w4))
    # counterfactual
    Y_1 <- rbinom(n, size=1, prob= plogis(-1 + 1 -0.1*w1 + 0.3*w2 + 0.25*w3 + 0.2*w4 + 0.15*w2*w4))
    Y_0 <- rbinom(n, size=1, prob= plogis(-1 + 0 -0.1*w1 + 0.3*w2 + 0.25*w3 + 0.2*w4 + 0.15*w2*w4))
    # Observed outcome
    Y <- Y_1*A + Y_0*(1 - A)
    # return data.frame
    tibble(w1, w2, w3, w4, A, Y, Y_1, Y_0)
}

# observations N
n <- 10000

# full data set, including Y0 and Y0
dat_full <- generate_data(n)
```

Our estimate of interest here is going to be the mean difference in outcomes if everyone had recevied the treatment compared to if everyone had not received the treatment. If we were to go through the whole causal inference identification process (AKA the very careful thinking half of causal inference), this might be identifiable as the Average Treatment Effect, or ATE.

Since we have simulated data, we can generate what the outcome would actually look like if everyone were to receive treatment and then if everyone were to not receive treatment, and we can take the mean difference.

```{r}
# calculate the true psi if we saw both outcomes
true_psi <- mean(dat_full$Y_1 - dat_full$Y_0)
round(true_psi,3)
```

Cool, so the true ATE for our is XX. Since our outcome is binary, that translates to a 20% difference in outcomes if everyone were to receive the treatment of interest. That means if A huge effect! 


```{r}
# make a data set with observed data only
dat_obs <- dat_full %>%
  select(-Y_1,-Y_0)
```


# Step 1:

Estimate the expected value of the outcome using treatment and confounders as predictors.

$$Q(A,W) = \mathrm{E}[Y|A,W]$$

![](/img/tmle/2_outcome_fit.png){width=70%}

```{r}

```


Then predict the outcome for every observation under three scenarios:

**1. If every observation received the treatment they *actually* received.**

$$Q(A,W) = \mathrm{E}[Y|A,W]$$

![](/img/tmle/3_QA.png){width=50%}
Remember we're talking about a binary treatment for this tutorial.

**2. If every observation received the treatment.**

$$Q(1,W) = \mathrm{E}[Y|1,W]$$

![](/img/tmle/4_Q1.png){width=80%}

**3. If every observation received the control.**

$$Q(0,W) = \mathrm{E}[Y|0,W]$$


![](/img/tmle/5_Q1.png){width=80%}

> If you have heard of G-Computation, or the simple substitution estimator (same thing), we could compute that estimate now by doing:

...

> If you've never heard of G-computation or simple substitution, don't sweat it! I'll explain later.

#Step 2:

Estimate the probability of treatment, given confounders. Note that this quantity is often called a **propensity score**, related to the *propensity* that an observation will receive a treatment of interest.

![](/img/tmle/6_treatment_fit.png){width=60%}
![](/img/tmle/7_H1.png){width=60%}
![](/img/tmle/8_H0.png){width=70%}
![](/img/tmle/9_HA.png){width=50%}
![](/img/tmle/10_logistic_regression.png)
![](/img/tmle/11_epsilon.png){width=50%}
![](/img/tmle/12_update_Q1.png){width=70%}
![](/img/tmle/13_update_Q0.png){width=70%}
![](/img/tmle/14_compute_ATE.png){width=60%}
![](/img/tmle/15_ses.png){width=100%}



```{r,eval=F}


# set SL libraries
lib <- c('SL.speedglm', # faster glm
         'SL.glmnet', # lasso
         'SL.ranger', # random forest
         'SL.earth')  #


# estimate Q --------------------------------------------------------------

Y <- dat_obs$Y
X_Y <- dat_obs %>% select(-Y)

Q <- SuperLearner(Y = Y, X = X_Y,
                    family=binomial(),
                    SL.library=lib,
                    # metalearner = NNloglik for binary outcomes, NNLS for continuous
                    method = method.NNloglik,
                    # 5-fold cross validation
                    cvControl = list(V = 5))

# estimate g --------------------------------------------------------------

A <- dat_obs$A
X_A <- dat_obs %>% select(-Y, -A)

g <- SuperLearner(Y = A, X = X_A,
                    family=binomial(),
                    SL.library=lib,
                    # metalearner = NNloglik for binary outcomes, NNLS for continuous
                    method = method.NNloglik,
                    # 5-fold cross validation
                    cvControl = list(V = 5))


# predictions for Q -------------------------------------------------------

Q_A <- predict(Q)$pred

X_Y_A1 <- X_Y %>% mutate(A = 1) 
Q_1 <- predict(Q, newdata = as.data.frame(X_Y_A1))$pred

X_Y_A0 <- X_Y %>% mutate(A = 0)
Q_0 <- predict(Q, newdata = X_Y_A0)$pred


# create clever covariate -------------------------------------------------

H_1 <- 1/predict(g)$pred
H_0 <- -1/(1-predict(g)$pred)

# prep data to fit the clever covariate
dat_cc <-
  dat_obs %>%
  mutate(Q_A = as.vector(Q_A),
         H = case_when(A == 1 ~ H_1,
                       A == 0 ~ H_0)) %>%
  select(Y, A, Q_A, H)

# fit parametric working model
glm_fit <- glm(Y ~ -1 + offset(qlogis(Q_A)) + H, data=dat_cc, family=binomial)
# get epsilon
eps <- coef(glm_fit)
# also get H as a vector (for Q_A_update)
H <- dat_cc$H

# update expected outcome estimates (Q star) ------------------------------

# update
Q_1_update <- plogis(qlogis(Q_1) + eps*H_1)
Q_0_update <- plogis(qlogis(Q_0) + eps*H_0)
Q_A_update <- plogis(qlogis(Q_A) + eps*H)

# Estimate ATE ------------------------------------------------------------

# get the ATE TMLE
tmle_psi <- mean(Q_1_update - Q_0_update)

# compare to true parameter
tmle_psi
true_psi 


# Calculate SEs -----------------------------------------------------------

ic <- (dat_obs$Y - Q_A_update) * H + Q_1_update + Q_0_update
tmle_se <- sqrt(var(ic)/nrow(dat_obs))

ci_lo <- tmle_psi - 1.96*tmle_se
ci_hi <- tmle_psi + 1.96*tmle_se

pval <- 2 * (1 - pnorm(abs(tmle_psi / tmle_se)))


# Compare with TMLE package -----------------------------------------------------------

tmle_fit <- tmle::tmle(Y, A, X_A,
                       gbound = .000000001, # trying 
                       Q.SL.library = lib, g.SL.library = lib)
tmle_fit$epsilon # mine are different :(
tmle_fit$estimates$ATE
      
```


 Essentially, you start with the estimate of an individual's outcome given their treatment and baseline covariates, and you update that estimate using the probability an individual received the treatment given their baseline covariates. Every time you update that estimate, you remove more of the bias that naturally exists in your observational data.

# Why should I bother with TMLE?

Before I show an applied example of TMLE, I want to explain why I prefer TMLE to other propensity score estimation methods. In short, it is because because you don't need to fit any certain type of regression to get your final estimates. You can use any estimator you want -- a generalized linear model, splines, random forests, gradient boosting, neural nets, honestly anything -- to get the initial probability estimates. You can actually use *all* of those estimators to get the two probability estimates, if you want! But more on that later.

The benefit of expanding your toolbox of potential estimators is that most estimators are built with prediction in mind, and thus yield very good probability estimates to initiate our TMLE algorithm. When our goal as statisticians is to calculate effects on an outcome attributable to a treatment, it's easy to shy away from these prediction-focused types of estimators, because most of them do not have any statistical theory to allow us to calculate valid confidence intervals.

**A huge selling point of TMLE is that it allows you to utilize prediction-focused (often non-parametric) estimators, but still obtain valid confidence intervals on your final estimate of the treatment's effect.** TMLE utilizes concepts from semi-parametric influence function theory to determine valid standard errors, and therefore valid confidence intervals, on estimates of treatment effects. This is not important unless you plan to tackle the math of TMLE, but if you do decide to venture into technical explanations, know that you'll see references to influence functions *a lot*!

In other propensity score methods, like IPTW, we can only obtain valid confidence intervals if we obtain our propensity score using pre-specified parametric models. For example, we fit a logistic regression for our treatment predicted by five pre-specified baseline covariates. Using a logistic model like this to calculate the propensity score is not only placing a strong distributional assumption on our data, but it is limited in its ability to take in high-, or even medium-dimensional data. When we use estimators that are well equipped for very "wide" data and perform variable selection, we obtain better overall estimates for our treatment effect of interest.

There are a few other major benefits to TMLE. For one, it has very good bias-variance properties, and those properties are "robust," or "resistant to" model mispecification, i.e. having the wrong type of estimator or the wrong variables in your estimation of either the treatment or outcome. Another benefit is that if you are able to go through a causal identification process of the research question and available data (a concept I won't discuss further, since it's a separate realm of the problem), you'll have a causal interpretation of your average treatment effect estimate.

# A Real World Application of TMLE

```{r}
library(dplyr, warn.conflicts=F)
```

Here I'll be using a data set from the ???. EXPLAIN DATA aND goal

I chose this data set because it is the same data used in a really good resource if you want to do a deeper dive in Targeted Learning:

```{r}
washb_data <- data.table::fread("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv",
                    stringsAsFactors = TRUE)
```

Before we can run a TMLE analysis, we need to clean the data a bit. The `tmle` function requires a binary treatment, so I'll need to turn the treatment data from character strings to binary 0/1 variables. I'm only interested right now in women who recieved the treatment of Nutrition and Hand washing, and comparing that to the control women, so I'll filter out the appropriate subjects and make my data binary.

To use `tmle`, your data structure should be:

  - Wide (each row is one observation)
  
  - No factors (all categorical variables should be transformed to dummy/indicator columns using a function like `model.matrix()`
  
  - No missing data (I usually make a new column indicating whether the value was missing, and then impute at the mean or median for continuous variables)

```{r}
dat_clean <- 
  washb_data %>%
  filter(tr %in% c("Control", "Nutrition + WSH")) %>%
  mutate(tr = case_when(tr == "Control" ~ 0,
                        TRUE ~ 1)) %>%
  mutate_at(vars(one_of(c("momage","momheight"))), list(miss =~ ifelse(is.na(.), 1, 0))) %>%
  mutate_at(vars(one_of("momage","momheight")), list( ~ ifelse(is.na(.), mean(., na.rm=T), .))) %>%
  model.matrix(~ . + 0, data = .) %>%
  as.data.frame()
```

Once I've cleaned my data, I make vectors specifying my treatment, outcome, and baseline covariates. In the TMLE literature, and in this package, the notations are:

  - Outcome: `Y`

  - Treatment: `A`

  - Confounders: `W`
  
In this data set, our outcome is the variable `whz`, our treatment is the variable `tr`, and our baseline confounders are all the other variables in our data set.

```{r}
Y <- dat_clean$whz
A <- dat_clean$tr
W <- dat_clean %>% dplyr::select(-whz, -tr)
```

Once we've done that, we can get our TMLE estimate of the average treatment effect! Here, I'm only inputting the 

```{r, eval=F}
tmlefit_default <- tmle::tmle(Y, A, W)
tmlefit_default
```

Here, I'm going to specify the kinds of estimators I want to use to get my two important estimates: the expected outcome and the probability of the exposure.

I've chosen to call the functions `glm` for a generalized linear model, `glmnet` for penalized regression, `ranger` for random forests, and `xgboost` for extreme gradient boosting.

I'm going to use *all* of these estimators to estimate the 

```{r}
SL_library <- c("SL.glm",
                "SL.glmnet",
                "SL.ranger",
                "SL.xgboost")
```

We're going to specify those libraries in the arguments `Q.SL.library` and `g.SL.library`. These arguments sound a bit scary, but they are just what the notation in TMLE literature is -- Q refers to the estimation for the outcome given treatment and covariates, and g refers to the estimation of the probability of the treatment.

```{r, eval=F}
tmlefit <- tmle::tmle(Y, A, W,
                Q.SL.library = SL_library, g.SL.library = SL_library)
tmlefit
```


To understand how the various machine learning models are combining, you should look into a type of ensemble learning called stacking, or "superlearning." I have a slideshow that you could look at [here](https://github.com/hoffmakl/sl3-demo/blob/master/superlearning_slides_no_animation.pdf), but there are plenty of other resources online.


This is obviously not a super technical post, and it was not intended to be, but I hope that it may catch your interest to learn more about the complexities of TMLE and try it out in your next analysis. I plan to write a few similar posts on what to do if your outcomes are survival or longitudinal data. In the meantime, you may find the more technical tutorial on TMLE helpful:



or, "The Hitchiker's Guide to Targeted Learning" is an excellent, still-in-progress, resource for learning TMLE and many other targeted learning.

Learn about causal inference,

we should be estimating answers for questions of actual interest, rather than debiasing our results.



# References:
